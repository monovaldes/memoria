Running Hive from /opt/hive-0.12.0
Running Hadoop from 
Running Hive query: tpch/q1_pricing_summary_report.hive
13/11/28 23:33:57 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:33:57 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:33:57 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:33:57 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:33:57 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:33:57 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:33:57 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.591 seconds
OK
Time taken: 0.248 seconds
OK
Time taken: 0.227 seconds
OK
Time taken: 0.055 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0197, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0197/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0197
Hadoop job information for Stage-1: number of mappers: 29; number of reducers: 8
2013-11-28 23:35:02,591 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 350.17 sec
MapReduce Total cumulative CPU time: 5 minutes 50 seconds 170 msec
Ended Job = job_1385675857984_0197
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0198, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0198/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0198
Hadoop job information for Stage-2: number of mappers: 5; number of reducers: 1
2013-11-28 23:35:21,704 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.79 sec
MapReduce Total cumulative CPU time: 4 seconds 790 msec
Ended Job = job_1385675857984_0198
Loading data to table default.q1_pricing_summary_report
Table default.q1_pricing_summary_report stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 584, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 29  Reduce: 8   Cumulative CPU: 350.17 sec   HDFS Read: 7775969019 HDFS Write: 1097 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 4.79 sec   HDFS Read: 3598 HDFS Write: 584 SUCCESS
Total MapReduce CPU Time Spent: 5 minutes 54 seconds 960 msec
OK
Time taken: 76.925 seconds
Time:86.44
Running Hive query: tpch/q2_minimum_cost_supplier.hive
13/11/28 23:35:23 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:35:23 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:35:23 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:35:23 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:35:23 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:35:23 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:35:23 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.657 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.217 seconds
OK
Time taken: 0.121 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.204 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.052 seconds
Total MapReduce jobs = 10
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 23:35:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 23:35:38 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-35-32_761_6279028195522351037-1/-local-10020/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 23:35:38 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-35-32_761_6279028195522351037-1/-local-10020/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 23:35:39 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:35:39 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:35:39 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:35:39 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:35:39 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:35:39 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:35:39 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 11:35:39	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 11:35:40	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-35-32_761_6279028195522351037-1/-local-10017/HashTable-Stage-18/MapJoin-mapfile61--.hashtable
2013-11-28 11:35:40	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-35-32_761_6279028195522351037-1/-local-10017/HashTable-Stage-18/MapJoin-mapfile61--.hashtable
2013-11-28 11:35:40	End of local task; Time Taken: 0.933 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0199, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0199/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0199
Hadoop job information for Stage-18: number of mappers: 1; number of reducers: 0
2013-11-28 23:35:53,444 Stage-18 map = 100%,  reduce = 0%, Cumulative CPU 0.97 sec
MapReduce Total cumulative CPU time: 970 msec
Ended Job = job_1385675857984_0199
Stage-23 is filtered out by condition resolver.
Stage-24 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0200, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0200/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0200
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2013-11-28 23:36:19,605 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.48 sec
MapReduce Total cumulative CPU time: 6 seconds 480 msec
Ended Job = job_1385675857984_0200
Stage-21 is filtered out by condition resolver.
Stage-22 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 3 out of 10
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0201, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0201/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0201
Hadoop job information for Stage-2: number of mappers: 8; number of reducers: 2
2013-11-28 23:36:55,025 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 83.29 sec
MapReduce Total cumulative CPU time: 1 minutes 23 seconds 290 msec
Ended Job = job_1385675857984_0201
Stage-19 is filtered out by condition resolver.
Stage-20 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 4 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0202, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0202/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0202
Hadoop job information for Stage-3: number of mappers: 4; number of reducers: 1
2013-11-28 23:37:30,567 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 42.06 sec
MapReduce Total cumulative CPU time: 42 seconds 60 msec
Ended Job = job_1385675857984_0202
Loading data to table default.q2_minimum_cost_supplier_tmp1
Table default.q2_minimum_cost_supplier_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 1084086, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.97 sec   HDFS Read: 2424 HDFS Write: 231 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 6.48 sec   HDFS Read: 14177170 HDFS Write: 3270652 SUCCESS
Job 2: Map: 8  Reduce: 2   Cumulative CPU: 83.29 sec   HDFS Read: 1208156118 HDFS Write: 275598972 SUCCESS
Job 3: Map: 4  Reduce: 1   Cumulative CPU: 42.06 sec   HDFS Read: 518945064 HDFS Write: 1084086 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 12 seconds 800 msec
OK
Time taken: 118.333 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0203, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0203/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0203
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-28 23:37:50,501 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.22 sec
MapReduce Total cumulative CPU time: 3 seconds 220 msec
Ended Job = job_1385675857984_0203
Loading data to table default.q2_minimum_cost_supplier_tmp2
Table default.q2_minimum_cost_supplier_tmp2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 66266, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.22 sec   HDFS Read: 1084322 HDFS Write: 66266 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 220 msec
OK
Time taken: 19.89 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 23:37:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 23:37:53 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-37-50_986_3674612531771246448-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 23:37:53 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-37-50_986_3674612531771246448-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 23:37:53 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:37:53 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:37:53 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:37:53 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:37:53 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:37:53 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:37:53 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 11:37:54	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 11:37:54	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-37-50_986_3674612531771246448-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile71--.hashtable
2013-11-28 11:37:55	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-37-50_986_3674612531771246448-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile71--.hashtable
2013-11-28 11:37:55	End of local task; Time Taken: 0.855 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0204, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0204/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0204
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-28 23:38:14,546 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.65 sec
MapReduce Total cumulative CPU time: 3 seconds 650 msec
Ended Job = job_1385675857984_0204
Loading data to table default.q2_minimum_cost_supplier
Table default.q2_minimum_cost_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16120, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.65 sec   HDFS Read: 1084322 HDFS Write: 16120 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 650 msec
OK
Time taken: 24.031 seconds
Time:172.74
Running Hive query: tpch/q3_shipping_priority.hive
13/11/28 23:38:16 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:38:16 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:38:16 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:38:16 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:38:16 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:38:16 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:38:16 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.787 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.225 seconds
OK
Time taken: 0.231 seconds
OK
Time taken: 0.044 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.056 seconds
Total MapReduce jobs = 6
Stage-1 is selected by condition resolver.
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0205, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0205/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0205
Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 2
2013-11-28 23:39:02,677 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 100.62 sec
MapReduce Total cumulative CPU time: 1 minutes 40 seconds 620 msec
Ended Job = job_1385675857984_0205
Stage-14 is filtered out by condition resolver.
Stage-15 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0206, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0206/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0206
Hadoop job information for Stage-2: number of mappers: 30; number of reducers: 8
2013-11-28 23:40:01,920 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 415.51 sec
MapReduce Total cumulative CPU time: 6 minutes 55 seconds 510 msec
Ended Job = job_1385675857984_0206
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0207, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0207/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0207
Hadoop job information for Stage-3: number of mappers: 5; number of reducers: 1
2013-11-28 23:40:22,213 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 18.96 sec
MapReduce Total cumulative CPU time: 18 seconds 960 msec
Ended Job = job_1385675857984_0207
Launching Job 4 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0208, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0208/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0208
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 23:40:42,085 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 5.34 sec
MapReduce Total cumulative CPU time: 5 seconds 340 msec
Ended Job = job_1385675857984_0208
Loading data to table default.q3_shipping_priority
Table default.q3_shipping_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 355, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 11  Reduce: 2   Cumulative CPU: 100.62 sec   HDFS Read: 1994102540 HDFS Write: 49783581 SUCCESS
Job 1: Map: 30  Reduce: 8   Cumulative CPU: 415.51 sec   HDFS Read: 7825753189 HDFS Write: 4804309 SUCCESS
Job 2: Map: 5  Reduce: 1   Cumulative CPU: 18.96 sec   HDFS Read: 4806810 HDFS Write: 4803577 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 5.34 sec   HDFS Read: 4803944 HDFS Write: 355 SUCCESS
Total MapReduce CPU Time Spent: 9 minutes 0 seconds 430 msec
OK
Time taken: 137.54 seconds
Time:147.57
Running Hive query: tpch/q4_order_priority.hive
13/11/28 23:40:43 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:40:43 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:40:43 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:40:43 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:40:43 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:40:43 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:40:43 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.898 seconds
OK
Time taken: 0.11 seconds
OK
Time taken: 0.198 seconds
OK
Time taken: 0.131 seconds
OK
Time taken: 0.238 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.056 seconds
OK
Time taken: 0.042 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0209, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0209/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0209
Hadoop job information for Stage-1: number of mappers: 29; number of reducers: 8
2013-11-28 23:41:39,931 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 294.61 sec
MapReduce Total cumulative CPU time: 4 minutes 54 seconds 610 msec
Ended Job = job_1385675857984_0209
Loading data to table default.q4_order_priority_tmp
Table default.q4_order_priority_tmp stats: [num_partitions: 0, num_files: 8, num_rows: 0, total_size: 121234490, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 29  Reduce: 8   Cumulative CPU: 294.61 sec   HDFS Read: 7775969019 HDFS Write: 121234490 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 54 seconds 610 msec
OK
Time taken: 47.83 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0210, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0210/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0210
Hadoop job information for Stage-1: number of mappers: 13; number of reducers: 2
2013-11-28 23:42:33,914 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 142.95 sec
MapReduce Total cumulative CPU time: 2 minutes 22 seconds 950 msec
Ended Job = job_1385675857984_0210
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0211, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0211/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0211
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-28 23:42:54,304 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 1.9 sec
MapReduce Total cumulative CPU time: 1 seconds 900 msec
Ended Job = job_1385675857984_0211
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0212, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0212/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0212
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 23:43:13,643 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.83 sec
MapReduce Total cumulative CPU time: 1 seconds 830 msec
Ended Job = job_1385675857984_0212
Loading data to table default.q4_order_priority
Table default.q4_order_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 82, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 13  Reduce: 2   Cumulative CPU: 142.95 sec   HDFS Read: 1870486160 HDFS Write: 486 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 1.9 sec   HDFS Read: 1220 HDFS Write: 248 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.83 sec   HDFS Read: 615 HDFS Write: 82 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 26 seconds 680 msec
OK
Time taken: 93.687 seconds
Time:151.59
Running Hive query: tpch/q5_local_supplier_volume.hive
13/11/28 23:43:15 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:43:15 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:43:15 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:43:15 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:43:15 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:43:15 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:43:15 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.87 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.098 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.206 seconds
OK
Time taken: 0.047 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 23:43:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 23:43:32 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-43-24_693_4158688883104758905-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 23:43:32 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-43-24_693_4158688883104758905-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 23:43:32 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:43:32 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:43:32 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:43:32 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:43:32 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:43:32 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:43:32 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 11:43:33	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 11:43:34	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-43-24_693_4158688883104758905-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile81--.hashtable
2013-11-28 11:43:34	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-43-24_693_4158688883104758905-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile81--.hashtable
2013-11-28 11:43:34	End of local task; Time Taken: 0.964 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0213, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0213/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0213
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-11-28 23:43:46,973 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 1.0 sec
MapReduce Total cumulative CPU time: 1 seconds 0 msec
Ended Job = job_1385675857984_0213
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0214, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0214/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0214
Hadoop job information for Stage-7: number of mappers: 2; number of reducers: 1
2013-11-28 23:44:07,441 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 5.94 sec
MapReduce Total cumulative CPU time: 5 seconds 940 msec
Ended Job = job_1385675857984_0214
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0215, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0215/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0215
Hadoop job information for Stage-8: number of mappers: 30; number of reducers: 8
2013-11-28 23:45:36,057 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 594.4 sec
MapReduce Total cumulative CPU time: 9 minutes 54 seconds 400 msec
Ended Job = job_1385675857984_0215
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0216, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0216/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0216
Hadoop job information for Stage-1: number of mappers: 15; number of reducers: 3
2013-11-28 23:46:46,511 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 192.72 sec
MapReduce Total cumulative CPU time: 3 minutes 12 seconds 720 msec
Ended Job = job_1385675857984_0216
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0217, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0217/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0217
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 1
2013-11-28 23:47:21,199 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 37.14 sec
MapReduce Total cumulative CPU time: 37 seconds 140 msec
Ended Job = job_1385675857984_0217
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0218, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0218/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0218
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 23:47:39,341 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.4 sec
MapReduce Total cumulative CPU time: 1 seconds 400 msec
Ended Job = job_1385675857984_0218
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0219, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0219/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0219
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 23:47:56,998 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.48 sec
MapReduce Total cumulative CPU time: 1 seconds 480 msec
Ended Job = job_1385675857984_0219
Loading data to table default.q5_local_supplier_volume
Table default.q5_local_supplier_volume stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 134, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.0 sec   HDFS Read: 2424 HDFS Write: 222 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 5.94 sec   HDFS Read: 14177161 HDFS Write: 577426 SUCCESS
Job 2: Map: 30  Reduce: 8   Cumulative CPU: 594.4 sec   HDFS Read: 7776546812 HDFS Write: 557353424 SUCCESS
Job 3: Map: 15  Reduce: 3   Cumulative CPU: 192.72 sec   HDFS Read: 2306606464 HDFS Write: 83207169 SUCCESS
Job 4: Map: 4  Reduce: 1   Cumulative CPU: 37.14 sec   HDFS Read: 328060271 HDFS Write: 257 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 1.4 sec   HDFS Read: 624 HDFS Write: 257 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.48 sec   HDFS Read: 624 HDFS Write: 134 SUCCESS
Total MapReduce CPU Time Spent: 13 minutes 54 seconds 80 msec
OK
Time taken: 272.805 seconds
Time:283.30
Running Hive query: tpch/q6_forecast_revenue_change.hive
13/11/28 23:47:58 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:47:58 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:47:58 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:47:58 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:47:58 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:47:58 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:47:58 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.948 seconds
OK
Time taken: 0.209 seconds
OK
Time taken: 0.211 seconds
OK
Time taken: 0.051 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0220, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0220/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0220
Hadoop job information for Stage-1: number of mappers: 29; number of reducers: 1
2013-11-28 23:48:47,277 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 202.81 sec
MapReduce Total cumulative CPU time: 3 minutes 22 seconds 810 msec
Ended Job = job_1385675857984_0220
Loading data to table default.q6_forecast_revenue_change
Table default.q6_forecast_revenue_change stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 29  Reduce: 1   Cumulative CPU: 202.81 sec   HDFS Read: 7775969019 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 22 seconds 810 msec
OK
Time taken: 40.56 seconds
Time:50.33
Running Hive query: tpch/q7_volume_shipping.hive
13/11/28 23:48:49 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:48:49 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:48:49 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:48:49 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:48:49 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:48:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:48:49 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.735 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.135 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.23 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.081 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 3
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 23:49:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 23:49:01 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-48-58_303_4493090964156649031-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 23:49:01 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-48-58_303_4493090964156649031-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 23:49:01 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:49:01 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:49:01 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:49:01 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:49:01 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:49:01 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:49:01 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 11:49:02	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 11:49:03	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-48-58_303_4493090964156649031-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-11-28 11:49:03	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-48-58_303_4493090964156649031-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-11-28 11:49:03	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-48-58_303_4493090964156649031-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-11-28 11:49:03	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-48-58_303_4493090964156649031-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-11-28 11:49:03	End of local task; Time Taken: 0.949 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0221, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0221/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0221
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 0
2013-11-28 23:49:17,415 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.48 sec
MapReduce Total cumulative CPU time: 1 seconds 480 msec
Ended Job = job_1385675857984_0221
Stage-5 is selected by condition resolver.
Stage-4 is filtered out by condition resolver.
Stage-6 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-11-28_23-48-58_303_4493090964156649031-1/-ext-10000
Loading data to table default.q7_volume_shipping_tmp
Table default.q7_volume_shipping_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 38, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.48 sec   HDFS Read: 2424 HDFS Write: 38 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 480 msec
OK
Time taken: 19.634 seconds
Total MapReduce jobs = 9
Stage-6 is selected by condition resolver.
Launching Job 1 out of 9
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0222, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0222/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0222
Hadoop job information for Stage-6: number of mappers: 38; number of reducers: 8
2013-11-28 23:50:29,985 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 491.53 sec
MapReduce Total cumulative CPU time: 8 minutes 11 seconds 530 msec
Ended Job = job_1385675857984_0222
Stage-24 is filtered out by condition resolver.
Stage-25 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 9
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0223, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0223/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0223
Hadoop job information for Stage-7: number of mappers: 6; number of reducers: 1
2013-11-28 23:52:45,189 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 206.92 sec
MapReduce Total cumulative CPU time: 3 minutes 26 seconds 920 msec
Ended Job = job_1385675857984_0223
Stage-22 is filtered out by condition resolver.
Stage-23 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 9
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0224, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0224/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0224
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 1
2013-11-28 23:54:44,931 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 197.15 sec
MapReduce Total cumulative CPU time: 3 minutes 17 seconds 150 msec
Ended Job = job_1385675857984_0224
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 23:54:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 23:54:47 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-49-17_938_2584651679028337766-1/-local-10024/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 23:54:47 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-49-17_938_2584651679028337766-1/-local-10024/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 23:54:47 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:54:47 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:54:47 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:54:47 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:54:47 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:54:47 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:54:47 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 11:54:47	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 11:54:48	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-49-17_938_2584651679028337766-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-11-28 11:54:48	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-49-17_938_2584651679028337766-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-11-28 11:54:48	End of local task; Time Taken: 0.65 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 4 out of 9
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0225, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0225/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0225
Hadoop job information for Stage-3: number of mappers: 4; number of reducers: 1
2013-11-28 23:55:19,876 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 53.27 sec
MapReduce Total cumulative CPU time: 53 seconds 270 msec
Ended Job = job_1385675857984_0225
Launching Job 5 out of 9
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0226, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0226/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0226
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 23:55:38,984 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.4 sec
MapReduce Total cumulative CPU time: 1 seconds 400 msec
Ended Job = job_1385675857984_0226
Loading data to table default.q7_volume_shipping
Table default.q7_volume_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 162, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 38  Reduce: 8   Cumulative CPU: 491.53 sec   HDFS Read: 9525219413 HDFS Write: 944423875 SUCCESS
Job 1: Map: 6  Reduce: 1   Cumulative CPU: 206.92 sec   HDFS Read: 1189278377 HDFS Write: 889952492 SUCCESS
Job 2: Map: 7  Reduce: 1   Cumulative CPU: 197.15 sec   HDFS Read: 904141907 HDFS Write: 846881546 SUCCESS
Job 3: Map: 4  Reduce: 1   Cumulative CPU: 53.27 sec   HDFS Read: 846899940 HDFS Write: 268 SUCCESS
Job 4: Map: 1  Reduce: 1   Cumulative CPU: 1.4 sec   HDFS Read: 635 HDFS Write: 162 SUCCESS
Total MapReduce CPU Time Spent: 15 minutes 50 seconds 270 msec
OK
Time taken: 381.535 seconds
Time:411.64
Running Hive query: tpch/q8_national_market_share.hive
13/11/28 23:55:40 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:55:40 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:55:40 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:55:40 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:55:40 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:55:40 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:55:40 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.869 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.197 seconds
OK
Time taken: 0.212 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.047 seconds
OK
Time taken: 0.083 seconds
OK
Time taken: 0.024 seconds
OK
Time taken: 0.052 seconds
Total MapReduce jobs = 18
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 23:55:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 23:55:59 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-55-50_063_2261718030427256698-1/-local-10035/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 23:55:59 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-55-50_063_2261718030427256698-1/-local-10035/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 23:55:59 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:55:59 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:55:59 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:55:59 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:55:59 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:55:59 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:55:59 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 11:56:00	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 11:56:01	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-55-50_063_2261718030427256698-1/-local-10032/HashTable-Stage-32/MapJoin-mapfile111--.hashtable
2013-11-28 11:56:01	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-55-50_063_2261718030427256698-1/-local-10032/HashTable-Stage-32/MapJoin-mapfile111--.hashtable
2013-11-28 11:56:01	End of local task; Time Taken: 0.953 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 18
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0227, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0227/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0227
Hadoop job information for Stage-32: number of mappers: 1; number of reducers: 0
2013-11-28 23:56:19,018 Stage-32 map = 100%,  reduce = 0%, Cumulative CPU 1.02 sec
MapReduce Total cumulative CPU time: 1 seconds 20 msec
Ended Job = job_1385675857984_0227
Stage-42 is filtered out by condition resolver.
Stage-43 is filtered out by condition resolver.
Stage-9 is selected by condition resolver.
Launching Job 2 out of 18
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0228, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0228/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0228
Hadoop job information for Stage-9: number of mappers: 3; number of reducers: 1
2013-11-28 23:56:43,026 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 16.53 sec
MapReduce Total cumulative CPU time: 16 seconds 530 msec
Ended Job = job_1385675857984_0228
Stage-40 is filtered out by condition resolver.
Stage-41 is filtered out by condition resolver.
Stage-10 is selected by condition resolver.
Launching Job 3 out of 18
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0229, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0229/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0229
Hadoop job information for Stage-10: number of mappers: 10; number of reducers: 2
2013-11-28 23:57:14,438 Stage-10 map = 100%,  reduce = 100%, Cumulative CPU 82.25 sec
MapReduce Total cumulative CPU time: 1 minutes 22 seconds 250 msec
Ended Job = job_1385675857984_0229
Stage-38 is filtered out by condition resolver.
Stage-39 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 18
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0230, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0230/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0230
Hadoop job information for Stage-1: number of mappers: 31; number of reducers: 8
2013-11-28 23:58:38,555 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 616.78 sec
MapReduce Total cumulative CPU time: 10 minutes 16 seconds 780 msec
Ended Job = job_1385675857984_0230
Stage-36 is filtered out by condition resolver.
Stage-37 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 18
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0231, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0231/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0231
Hadoop job information for Stage-2: number of mappers: 5; number of reducers: 1
2013-11-28 23:59:18,849 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 52.3 sec
MapReduce Total cumulative CPU time: 52 seconds 300 msec
Ended Job = job_1385675857984_0231
Stage-34 is filtered out by condition resolver.
Stage-35 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 6 out of 18
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0232, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0232/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0232
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
2013-11-28 23:59:47,687 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 9.38 sec
MapReduce Total cumulative CPU time: 9 seconds 380 msec
Ended Job = job_1385675857984_0232
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 23:59:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 23:59:49 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-55-50_063_2261718030427256698-1/-local-10049/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 23:59:49 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-55-50_063_2261718030427256698-1/-local-10049/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 23:59:49 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:59:49 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:59:49 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:59:49 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:59:49 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:59:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:59:49 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 11:59:50	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 11:59:51	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-55-50_063_2261718030427256698-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-11-28 11:59:51	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-55-50_063_2261718030427256698-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-11-28 11:59:51	End of local task; Time Taken: 0.667 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 7 out of 18
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0233, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0233/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0233
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-29 00:00:13,588 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 5.82 sec
MapReduce Total cumulative CPU time: 5 seconds 820 msec
Ended Job = job_1385675857984_0233
Launching Job 8 out of 18
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0234, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0234/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0234
Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 1
2013-11-29 00:00:31,115 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 1.39 sec
MapReduce Total cumulative CPU time: 1 seconds 390 msec
Ended Job = job_1385675857984_0234
Loading data to table default.q8_national_market_share
Table default.q8_national_market_share stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 51, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.02 sec   HDFS Read: 2424 HDFS Write: 186 SUCCESS
Job 1: Map: 3  Reduce: 1   Cumulative CPU: 16.53 sec   HDFS Read: 244852699 HDFS Write: 6337396 SUCCESS
Job 2: Map: 10  Reduce: 2   Cumulative CPU: 82.25 sec   HDFS Read: 1755588157 HDFS Write: 30040072 SUCCESS
Job 3: Map: 31  Reduce: 8   Cumulative CPU: 616.78 sec   HDFS Read: 7806009825 HDFS Write: 188295372 SUCCESS
Job 4: Map: 5  Reduce: 1   Cumulative CPU: 52.3 sec   HDFS Read: 431638228 HDFS Write: 1157752 SUCCESS
Job 5: Map: 2  Reduce: 1   Cumulative CPU: 9.38 sec   HDFS Read: 15334691 HDFS Write: 1100621 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 5.82 sec   HDFS Read: 1100988 HDFS Write: 152 SUCCESS
Job 7: Map: 1  Reduce: 1   Cumulative CPU: 1.39 sec   HDFS Read: 519 HDFS Write: 51 SUCCESS
Total MapReduce CPU Time Spent: 13 minutes 5 seconds 470 msec
OK
Time taken: 281.528 seconds
Time:292.10
Running Hive query: tpch/q9_product_type_profit.hive
13/11/29 00:00:32 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:00:32 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:00:32 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:00:32 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:00:32 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:00:32 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:00:32 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.821 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.208 seconds
OK
Time taken: 0.229 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.082 seconds
OK
Time taken: 0.042 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 00:00:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 00:00:49 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-00-42_007_4116918384218740694-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 00:00:49 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-00-42_007_4116918384218740694-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 00:00:50 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:00:50 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:00:50 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:00:50 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:00:50 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:00:50 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:00:50 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:00:50	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:00:51	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_00-00-42_007_4116918384218740694-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-11-29 12:00:51	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_00-00-42_007_4116918384218740694-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-11-29 12:00:51	End of local task; Time Taken: 0.639 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0235, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0235/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0235
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-11-29 00:01:04,665 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 2.55 sec
MapReduce Total cumulative CPU time: 2 seconds 550 msec
Ended Job = job_1385675857984_0235
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0236, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0236/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0236
Hadoop job information for Stage-8: number of mappers: 30; number of reducers: 8
2013-11-29 00:03:17,009 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 807.83 sec
MapReduce Total cumulative CPU time: 13 minutes 27 seconds 830 msec
Ended Job = job_1385675857984_0236
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 5
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0237, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0237/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0237
Hadoop job information for Stage-1: number of mappers: 20; number of reducers: 5
2013-11-29 00:06:05,378 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 835.98 sec
MapReduce Total cumulative CPU time: 13 minutes 55 seconds 980 msec
Ended Job = job_1385675857984_0237
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 5
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0238, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0238/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0238
Hadoop job information for Stage-2: number of mappers: 18; number of reducers: 5
2013-11-29 00:08:37,848 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 645.02 sec
MapReduce Total cumulative CPU time: 10 minutes 45 seconds 20 msec
Ended Job = job_1385675857984_0238
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0239, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0239/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0239
Hadoop job information for Stage-3: number of mappers: 12; number of reducers: 2
2013-11-29 00:09:46,818 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 184.58 sec
MapReduce Total cumulative CPU time: 3 minutes 4 seconds 580 msec
Ended Job = job_1385675857984_0239
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0240, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0240/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0240
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-29 00:10:05,102 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.6 sec
MapReduce Total cumulative CPU time: 1 seconds 600 msec
Ended Job = job_1385675857984_0240
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0241, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0241/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0241
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-29 00:10:24,248 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 1.54 sec
MapReduce Total cumulative CPU time: 1 seconds 540 msec
Ended Job = job_1385675857984_0241
Loading data to table default.q9_product_type_profit
Table default.q9_product_type_profit stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 5842, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 2.55 sec   HDFS Read: 14176572 HDFS Write: 2869450 SUCCESS
Job 1: Map: 30  Reduce: 8   Cumulative CPU: 807.83 sec   HDFS Read: 7778838836 HDFS Write: 3701154936 SUCCESS
Job 2: Map: 20  Reduce: 5   Cumulative CPU: 835.98 sec   HDFS Read: 4906112840 HDFS Write: 3983392883 SUCCESS
Job 3: Map: 18  Reduce: 5   Cumulative CPU: 645.02 sec   HDFS Read: 4226802581 HDFS Write: 203521164 SUCCESS
Job 4: Map: 12  Reduce: 2   Cumulative CPU: 184.58 sec   HDFS Read: 1952773103 HDFS Write: 12940 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 1.6 sec   HDFS Read: 13529 HDFS Write: 6470 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.54 sec   HDFS Read: 6837 HDFS Write: 5842 SUCCESS
Total MapReduce CPU Time Spent: 41 minutes 19 seconds 100 msec
OK
Time taken: 582.735 seconds
Time:593.15
Running Hive query: tpch/q10_returned_item.hive
13/11/29 00:10:25 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:10:25 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:10:25 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:10:25 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:10:25 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:10:25 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:10:25 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.859 seconds
OK
Time taken: 0.119 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.199 seconds
OK
Time taken: 0.204 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.057 seconds
Total MapReduce jobs = 7
Stage-1 is selected by condition resolver.
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0242, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0242/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0242
Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 2
2013-11-29 00:11:15,495 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 87.71 sec
MapReduce Total cumulative CPU time: 1 minutes 27 seconds 710 msec
Ended Job = job_1385675857984_0242
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 00:11:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 00:11:18 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-10-34_917_495103170969675486-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 00:11:18 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-10-34_917_495103170969675486-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 00:11:18 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:11:18 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:11:18 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:11:18 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:11:18 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:11:18 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:11:18 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:11:18	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:11:19	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_00-10-34_917_495103170969675486-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-11-29 12:11:19	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_00-10-34_917_495103170969675486-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-11-29 12:11:19	End of local task; Time Taken: 0.641 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 7
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0243, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0243/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0243
Hadoop job information for Stage-13: number of mappers: 2; number of reducers: 0
2013-11-29 00:11:39,250 Stage-13 map = 100%,  reduce = 0%, Cumulative CPU 10.92 sec
MapReduce Total cumulative CPU time: 10 seconds 920 msec
Ended Job = job_1385675857984_0243
Stage-17 is filtered out by condition resolver.
Stage-18 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0244, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0244/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0244
Hadoop job information for Stage-3: number of mappers: 31; number of reducers: 8
2013-11-29 00:12:28,110 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 310.27 sec
MapReduce Total cumulative CPU time: 5 minutes 10 seconds 270 msec
Ended Job = job_1385675857984_0244
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0245, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0245/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0245
Hadoop job information for Stage-4: number of mappers: 6; number of reducers: 1
2013-11-29 00:12:56,345 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 29.01 sec
MapReduce Total cumulative CPU time: 29 seconds 10 msec
Ended Job = job_1385675857984_0245
Launching Job 5 out of 7
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0246, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0246/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0246
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-29 00:13:20,519 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 8.64 sec
MapReduce Total cumulative CPU time: 8 seconds 640 msec
Ended Job = job_1385675857984_0246
Loading data to table default.q10_returned_item
Table default.q10_returned_item stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3431, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 11  Reduce: 2   Cumulative CPU: 87.71 sec   HDFS Read: 1994102540 HDFS Write: 97877420 SUCCESS
Job 1: Map: 2   Cumulative CPU: 10.92 sec   HDFS Read: 97878152 HDFS Write: 101973906 SUCCESS
Job 2: Map: 31  Reduce: 8   Cumulative CPU: 310.27 sec   HDFS Read: 7877943657 HDFS Write: 86191919 SUCCESS
Job 3: Map: 6  Reduce: 1   Cumulative CPU: 29.01 sec   HDFS Read: 86194557 HDFS Write: 69050098 SUCCESS
Job 4: Map: 1  Reduce: 1   Cumulative CPU: 8.64 sec   HDFS Read: 69050464 HDFS Write: 3431 SUCCESS
Total MapReduce CPU Time Spent: 7 minutes 26 seconds 550 msec
OK
Time taken: 166.109 seconds
Time:176.29
Running Hive query: tpch/q11_important_stock.hive
13/11/29 00:13:22 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:13:22 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:13:22 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:13:22 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:13:22 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:13:22 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:13:22 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.622 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.215 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.166 seconds
OK
Time taken: 0.398 seconds
OK
Time taken: 0.038 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.076 seconds
OK
Time taken: 0.056 seconds
OK
Time taken: 0.084 seconds
Total MapReduce jobs = 5
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 00:13:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 00:13:35 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-13-31_520_9016205480923479766-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 00:13:35 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-13-31_520_9016205480923479766-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 00:13:35 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:13:35 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:13:35 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:13:35 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:13:35 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:13:35 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:13:35 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:13:36	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:13:37	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_00-13-31_520_9016205480923479766-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-11-29 12:13:37	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_00-13-31_520_9016205480923479766-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-11-29 12:13:37	End of local task; Time Taken: 0.943 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0247, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0247/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0247
Hadoop job information for Stage-10: number of mappers: 1; number of reducers: 0
2013-11-29 00:13:51,622 Stage-10 map = 100%,  reduce = 0%, Cumulative CPU 2.05 sec
MapReduce Total cumulative CPU time: 2 seconds 50 msec
Ended Job = job_1385675857984_0247
Stage-11 is filtered out by condition resolver.
Stage-12 is selected by condition resolver.
Stage-2 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 00:13:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 00:13:53 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-13-31_520_9016205480923479766-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 00:13:53 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-13-31_520_9016205480923479766-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 00:13:54 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:13:54 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:13:54 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:13:54 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:13:54 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:13:54 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:13:54 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:13:54	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:13:55	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_00-13-31_520_9016205480923479766-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile10--.hashtable
2013-11-29 12:13:55	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_00-13-31_520_9016205480923479766-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile10--.hashtable
2013-11-29 12:13:55	End of local task; Time Taken: 0.888 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 3 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0248, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0248/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0248
Hadoop job information for Stage-8: number of mappers: 7; number of reducers: 0
2013-11-29 00:14:17,022 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 41.05 sec
MapReduce Total cumulative CPU time: 41 seconds 50 msec
Ended Job = job_1385675857984_0248
Launching Job 4 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0249, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0249/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0249
Hadoop job information for Stage-3: number of mappers: 4; number of reducers: 1
2013-11-29 00:14:40,377 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 16.61 sec
MapReduce Total cumulative CPU time: 16 seconds 610 msec
Ended Job = job_1385675857984_0249
Loading data to table default.q11_part_tmp
Table default.q11_part_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6079705, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 2.05 sec   HDFS Read: 14176572 HDFS Write: 83300 SUCCESS
Job 1: Map: 7   Cumulative CPU: 41.05 sec   HDFS Read: 1204885099 HDFS Write: 8917282 SUCCESS
Job 2: Map: 4  Reduce: 1   Cumulative CPU: 16.61 sec   HDFS Read: 8919416 HDFS Write: 6079705 SUCCESS
Total MapReduce CPU Time Spent: 59 seconds 710 msec
OK
Time taken: 69.361 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0250, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0250/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0250
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-29 00:14:59,078 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.08 sec
MapReduce Total cumulative CPU time: 3 seconds 80 msec
Ended Job = job_1385675857984_0250
Loading data to table default.q11_sum_tmp
Table default.q11_sum_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.08 sec   HDFS Read: 6079924 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 80 msec
OK
Time taken: 18.699 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 00:15:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 00:15:01 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-14-59_582_2615008227679787591-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 00:15:01 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-14-59_582_2615008227679787591-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 00:15:02 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:15:02 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:15:02 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:15:02 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:15:02 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:15:02 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:15:02 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:15:02	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:15:03	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_00-14-59_582_2615008227679787591-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-11-29 12:15:03	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_00-14-59_582_2615008227679787591-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-11-29 12:15:03	End of local task; Time Taken: 0.709 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0251, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0251/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0251
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-29 00:15:24,482 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.84 sec
MapReduce Total cumulative CPU time: 3 seconds 840 msec
Ended Job = job_1385675857984_0251
Loading data to table default.q11_important_stock
Table default.q11_important_stock stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 0, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.84 sec   HDFS Read: 6079924 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 840 msec
OK
Time taken: 25.373 seconds
Time:123.91
Running Hive query: tpch/q12_shipping.hive
13/11/29 00:15:26 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:15:26 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:15:26 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:15:26 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:15:26 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:15:26 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:15:26 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.812 seconds
OK
Time taken: 0.111 seconds
OK
Time taken: 0.2 seconds
OK
Time taken: 0.208 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.051 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0252, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0252/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0252
Hadoop job information for Stage-1: number of mappers: 38; number of reducers: 8
2013-11-29 00:16:33,629 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 360.18 sec
MapReduce Total cumulative CPU time: 6 minutes 0 seconds 180 msec
Ended Job = job_1385675857984_0252
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0253, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0253/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0253
Hadoop job information for Stage-2: number of mappers: 5; number of reducers: 1
2013-11-29 00:16:52,914 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.68 sec
MapReduce Total cumulative CPU time: 3 seconds 680 msec
Ended Job = job_1385675857984_0253
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0254, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0254/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0254
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-29 00:17:10,761 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.86 sec
MapReduce Total cumulative CPU time: 1 seconds 860 msec
Ended Job = job_1385675857984_0254
Loading data to table default.q12_shipping
Table default.q12_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 42, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 38  Reduce: 8   Cumulative CPU: 360.18 sec   HDFS Read: 9525219413 HDFS Write: 1216 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 3.68 sec   HDFS Read: 3709 HDFS Write: 154 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.86 sec   HDFS Read: 520 HDFS Write: 42 SUCCESS
Total MapReduce CPU Time Spent: 6 minutes 5 seconds 720 msec
OK
Time taken: 96.576 seconds
Time:106.34
Running Hive query: tpch/q13_customer_distribution.hive
13/11/29 00:17:12 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:17:12 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:17:12 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:17:12 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:17:12 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:17:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:17:12 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.691 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.223 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.056 seconds
Total MapReduce jobs = 4
Stage-1 is selected by condition resolver.
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0255, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0255/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0255
Ended Job = job_1385675857984_0255
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0256, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0256/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0256
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-29 00:18:45,325 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 16.94 sec
MapReduce Total cumulative CPU time: 16 seconds 940 msec
Ended Job = job_1385675857984_0256
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0257, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0257/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0257
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-29 00:19:03,128 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.41 sec
MapReduce Total cumulative CPU time: 1 seconds 410 msec
Ended Job = job_1385675857984_0257
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0258, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0258/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0258
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-29 00:19:21,279 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.9 sec
MapReduce Total cumulative CPU time: 1 seconds 900 msec
Ended Job = job_1385675857984_0258
Loading data to table default.q13_customer_distribution
Table default.q13_customer_distribution stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 355, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0:  Cumulative CPU: 184.1 sec   HDFS Read: 1994102540 HDFS Write: 33263155 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 16.94 sec   HDFS Read: 33263889 HDFS Write: 1043 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.41 sec   HDFS Read: 1410 HDFS Write: 1043 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 1.9 sec   HDFS Read: 1410 HDFS Write: 355 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 24 seconds 350 msec
OK
Time taken: 120.686 seconds
Time:130.48
Running Hive query: tpch/q14_promotion_effect.hive
13/11/29 00:19:23 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:19:23 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:19:23 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:19:23 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:19:23 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:19:23 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:19:23 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.849 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.215 seconds
OK
Time taken: 0.225 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 2
Stage-1 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0259, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0259/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0259
Hadoop job information for Stage-1: number of mappers: 31; number of reducers: 8
2013-11-29 00:20:15,995 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 235.99 sec
MapReduce Total cumulative CPU time: 3 minutes 55 seconds 990 msec
Ended Job = job_1385675857984_0259
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0260, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0260/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0260
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 1
2013-11-29 00:20:35,274 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.52 sec
MapReduce Total cumulative CPU time: 3 seconds 520 msec
Ended Job = job_1385675857984_0260
Loading data to table default.q14_promotion_effect
Table default.q14_promotion_effect stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 19, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 31  Reduce: 8   Cumulative CPU: 235.99 sec   HDFS Read: 8019309664 HDFS Write: 1032 SUCCESS
Job 1: Map: 4  Reduce: 1   Cumulative CPU: 3.52 sec   HDFS Read: 3388 HDFS Write: 19 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 59 seconds 510 msec
OK
Time taken: 64.127 seconds
Time:73.98
Running Hive query: tpch/q15_top_supplier.hive
13/11/29 00:20:37 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:20:37 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:20:37 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:20:37 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:20:37 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:20:37 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:20:37 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.713 seconds
OK
Time taken: 0.118 seconds
OK
Time taken: 0.211 seconds
OK
Time taken: 0.129 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.224 seconds
OK
Time taken: 0.038 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0261, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0261/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0261
Hadoop job information for Stage-1: number of mappers: 29; number of reducers: 8
2013-11-29 00:21:23,976 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 232.09 sec
MapReduce Total cumulative CPU time: 3 minutes 52 seconds 90 msec
Ended Job = job_1385675857984_0261
Loading data to table default.revenue
Table default.revenue stats: [num_partitions: 0, num_files: 8, num_rows: 0, total_size: 2113812, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 29  Reduce: 8   Cumulative CPU: 232.09 sec   HDFS Read: 7775969019 HDFS Write: 2113812 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 52 seconds 90 msec
OK
Time taken: 38.644 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0262, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0262/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0262
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-29 00:21:44,977 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.45 sec
MapReduce Total cumulative CPU time: 7 seconds 450 msec
Ended Job = job_1385675857984_0262
Loading data to table default.max_revenue
Table default.max_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 13, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 7.45 sec   HDFS Read: 2114976 HDFS Write: 13 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 450 msec
OK
Time taken: 20.874 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 00:21:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 00:21:47 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-21-45_456_7963078842761955909-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 00:21:48 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-21-45_456_7963078842761955909-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 00:21:48 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:21:48 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:21:48 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:21:48 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:21:48 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:21:48 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:21:48 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:21:48	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:21:50	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_00-21-45_456_7963078842761955909-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile11--.hashtable
2013-11-29 12:21:50	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_00-21-45_456_7963078842761955909-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile11--.hashtable
2013-11-29 12:21:50	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_00-21-45_456_7963078842761955909-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-11-29 12:21:50	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_00-21-45_456_7963078842761955909-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-11-29 12:21:50	End of local task; Time Taken: 1.639 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0263, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0263/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0263
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-29 00:22:11,027 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 4.07 sec
MapReduce Total cumulative CPU time: 4 seconds 70 msec
Ended Job = job_1385675857984_0263
Loading data to table default.q15_top_supplier
Table default.q15_top_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 95, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 4.07 sec   HDFS Read: 14176572 HDFS Write: 95 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 70 msec
OK
Time taken: 26.075 seconds
Time:95.75
Running Hive query: tpch/q16_parts_supplier_relationship.hive
13/11/29 00:22:12 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:22:12 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:22:12 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:22:12 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:22:12 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:22:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:22:12 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.809 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.127 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.112 seconds
OK
Time taken: 0.125 seconds
OK
Time taken: 0.203 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.068 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0264, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0264/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0264
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2013-11-29 00:22:36,517 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.14 sec
MapReduce Total cumulative CPU time: 3 seconds 140 msec
Ended Job = job_1385675857984_0264
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-11-29_00-22-21_844_9210104426802149921-1/-ext-10000
Loading data to table default.supplier_tmp
Table default.supplier_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 588564, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 3.14 sec   HDFS Read: 14176572 HDFS Write: 588564 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 140 msec
OK
Time taken: 15.278 seconds
Total MapReduce jobs = 2
Stage-3 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0265, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0265/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0265
Hadoop job information for Stage-3: number of mappers: 9; number of reducers: 2
2013-11-29 00:23:17,800 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 101.64 sec
MapReduce Total cumulative CPU time: 1 minutes 41 seconds 640 msec
Ended Job = job_1385675857984_0265
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 00:23:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 00:23:20 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-22-37_123_6559773665261924268-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 00:23:20 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-22-37_123_6559773665261924268-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 00:23:20 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:23:20 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:23:20 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:23:20 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:23:20 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:23:20 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:23:20 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:23:20	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:23:21	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_00-22-37_123_6559773665261924268-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-29 12:23:22	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_00-22-37_123_6559773665261924268-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-29 12:23:22	End of local task; Time Taken: 1.074 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0266, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0266/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0266
Hadoop job information for Stage-5: number of mappers: 4; number of reducers: 0
2013-11-29 00:23:53,026 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 48.2 sec
MapReduce Total cumulative CPU time: 48 seconds 200 msec
Ended Job = job_1385675857984_0266
Loading data to table default.q16_tmp
Table default.q16_tmp stats: [num_partitions: 0, num_files: 4, num_rows: 0, total_size: 291513385, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 9  Reduce: 2   Cumulative CPU: 101.64 sec   HDFS Read: 1448225744 HDFS Write: 389243509 SUCCESS
Job 1: Map: 4   Cumulative CPU: 48.2 sec   HDFS Read: 389252150 HDFS Write: 291513385 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 29 seconds 840 msec
OK
Time taken: 76.356 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0267, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0267/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0267
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-11-29 00:24:28,995 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 36.77 sec
MapReduce Total cumulative CPU time: 36 seconds 770 msec
Ended Job = job_1385675857984_0267
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0268, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0268/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0268
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-29 00:24:49,789 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.52 sec
MapReduce Total cumulative CPU time: 5 seconds 520 msec
Ended Job = job_1385675857984_0268
Loading data to table default.q16_parts_supplier_relationship
Table default.q16_parts_supplier_relationship stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 1011572, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 3  Reduce: 1   Cumulative CPU: 36.77 sec   HDFS Read: 291514104 HDFS Write: 1394216 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 5.52 sec   HDFS Read: 1394582 HDFS Write: 1011572 SUCCESS
Total MapReduce CPU Time Spent: 42 seconds 290 msec
OK
Time taken: 56.781 seconds
Time:158.72
Running Hive query: tpch/q17_small_quantity_order_revenue.hive
13/11/29 00:24:51 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:24:51 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:24:51 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:24:51 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:24:51 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:24:51 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:24:51 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.617 seconds
OK
Time taken: 0.155 seconds
OK
Time taken: 0.236 seconds
OK
Time taken: 0.128 seconds
OK
Time taken: 0.238 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.047 seconds
OK
Time taken: 0.043 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0269, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0269/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0269
Hadoop job information for Stage-1: number of mappers: 29; number of reducers: 8
2013-11-29 00:26:17,903 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 588.36 sec
MapReduce Total cumulative CPU time: 9 minutes 48 seconds 360 msec
Ended Job = job_1385675857984_0269
Loading data to table default.lineitem_tmp
Table default.lineitem_tmp stats: [num_partitions: 0, num_files: 8, num_rows: 0, total_size: 47412103, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 29  Reduce: 8   Cumulative CPU: 588.36 sec   HDFS Read: 7775969019 HDFS Write: 47412103 SUCCESS
Total MapReduce CPU Time Spent: 9 minutes 48 seconds 360 msec
OK
Time taken: 78.326 seconds
Total MapReduce jobs = 5
Stage-1 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 9
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0270, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0270/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0270
Hadoop job information for Stage-1: number of mappers: 31; number of reducers: 9
2013-11-29 00:27:37,244 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 600.03 sec
MapReduce Total cumulative CPU time: 10 minutes 0 seconds 30 msec
Ended Job = job_1385675857984_0270
Stage-13 is filtered out by condition resolver.
Stage-14 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0271, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0271/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0271
Hadoop job information for Stage-2: number of mappers: 9; number of reducers: 1
2013-11-29 00:28:09,442 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 36.31 sec
MapReduce Total cumulative CPU time: 36 seconds 310 msec
Ended Job = job_1385675857984_0271
Launching Job 3 out of 5
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0272, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0272/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0272
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-29 00:28:28,616 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.8 sec
MapReduce Total cumulative CPU time: 1 seconds 800 msec
Ended Job = job_1385675857984_0272
Loading data to table default.q17_small_quantity_order_revenue
Table default.q17_small_quantity_order_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 19, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 31  Reduce: 9   Cumulative CPU: 600.03 sec   HDFS Read: 8019309664 HDFS Write: 2292088 SUCCESS
Job 1: Map: 9  Reduce: 1   Cumulative CPU: 36.31 sec   HDFS Read: 49708126 HDFS Write: 121 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.8 sec   HDFS Read: 488 HDFS Write: 19 SUCCESS
Total MapReduce CPU Time Spent: 10 minutes 38 seconds 140 msec
OK
Time taken: 130.622 seconds
Time:218.82
Running Hive query: tpch/q18_large_volume_customer.hive
13/11/29 00:28:30 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:28:30 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:28:30 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:28:30 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:28:30 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:28:30 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:28:30 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.728 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.138 seconds
OK
Time taken: 0.236 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.045 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0273, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0273/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0273
Hadoop job information for Stage-1: number of mappers: 29; number of reducers: 7
2013-11-29 00:29:32,154 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 349.94 sec
MapReduce Total cumulative CPU time: 5 minutes 49 seconds 940 msec
Ended Job = job_1385675857984_0273
Loading data to table default.q18_tmp
Table default.q18_tmp stats: [num_partitions: 0, num_files: 7, num_rows: 0, total_size: 214169325, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 29  Reduce: 7   Cumulative CPU: 349.94 sec   HDFS Read: 7775969019 HDFS Write: 214169325 SUCCESS
Total MapReduce CPU Time Spent: 5 minutes 49 seconds 940 msec
OK
Time taken: 53.577 seconds
Total MapReduce jobs = 5
Stage-5 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0274, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0274/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0274
Hadoop job information for Stage-5: number of mappers: 11; number of reducers: 2
2013-11-29 00:30:42,871 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 187.66 sec
MapReduce Total cumulative CPU time: 3 minutes 7 seconds 660 msec
Ended Job = job_1385675857984_0274
Stage-15 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0275, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0275/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0275
Hadoop job information for Stage-1: number of mappers: 38; number of reducers: 8
2013-11-29 00:32:06,938 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 670.53 sec
MapReduce Total cumulative CPU time: 11 minutes 10 seconds 530 msec
Ended Job = job_1385675857984_0275
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0276, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0276/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0276
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 1
2013-11-29 00:32:27,805 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.79 sec
MapReduce Total cumulative CPU time: 3 seconds 790 msec
Ended Job = job_1385675857984_0276
Launching Job 4 out of 5
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0277, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0277/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0277
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-29 00:32:50,684 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.71 sec
MapReduce Total cumulative CPU time: 1 seconds 710 msec
Ended Job = job_1385675857984_0277
Loading data to table default.q18_large_volume_customer
Table default.q18_large_volume_customer stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6202, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 11  Reduce: 2   Cumulative CPU: 187.66 sec   HDFS Read: 1994102540 HDFS Write: 964505568 SUCCESS
Job 1: Map: 38  Reduce: 8   Cumulative CPU: 670.53 sec   HDFS Read: 8954664928 HDFS Write: 45860 SUCCESS
Job 2: Map: 4  Reduce: 1   Cumulative CPU: 3.79 sec   HDFS Read: 48216 HDFS Write: 45268 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 1.71 sec   HDFS Read: 45635 HDFS Write: 6202 SUCCESS
Total MapReduce CPU Time Spent: 14 minutes 23 seconds 690 msec
OK
Time taken: 198.428 seconds
Time:262.07
Running Hive query: tpch/q19_discounted_revenue.hive
13/11/29 00:32:52 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:32:52 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:32:52 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:32:52 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:32:52 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:32:52 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:32:52 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 7.12 seconds
OK
Time taken: 0.103 seconds
OK
Time taken: 0.232 seconds
OK
Time taken: 0.21 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.057 seconds
Total MapReduce jobs = 2
Stage-1 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0278, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0278/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0278
Hadoop job information for Stage-1: number of mappers: 31; number of reducers: 8
2013-11-29 00:35:07,368 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 851.99 sec
MapReduce Total cumulative CPU time: 14 minutes 11 seconds 990 msec
Ended Job = job_1385675857984_0278
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0279, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0279/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0279
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 1
2013-11-29 00:35:32,743 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.18 sec
MapReduce Total cumulative CPU time: 3 seconds 180 msec
Ended Job = job_1385675857984_0279
Loading data to table default.q19_discounted_revenue
Table default.q19_discounted_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 31  Reduce: 8   Cumulative CPU: 851.99 sec   HDFS Read: 8019309664 HDFS Write: 968 SUCCESS
Job 1: Map: 4  Reduce: 1   Cumulative CPU: 3.18 sec   HDFS Read: 3324 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 14 minutes 15 seconds 170 msec
OK
Time taken: 151.97 seconds
Time:162.10
Running Hive query: tpch/q20_potential_part_promotion.hive
13/11/29 00:35:34 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:35:34 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:35:34 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:35:34 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:35:34 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:35:34 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:35:34 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.614 seconds
OK
Time taken: 0.129 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.217 seconds
OK
Time taken: 0.129 seconds
OK
Time taken: 0.125 seconds
OK
Time taken: 0.134 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.219 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0280, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0280/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0280
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2013-11-29 00:36:10,313 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.71 sec
MapReduce Total cumulative CPU time: 11 seconds 710 msec
Ended Job = job_1385675857984_0280
Loading data to table default.q20_tmp1
Table default.q20_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 160617, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 11.71 sec   HDFS Read: 243340645 HDFS Write: 160617 SUCCESS
Total MapReduce CPU Time Spent: 11 seconds 710 msec
OK
Time taken: 26.965 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0281, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0281/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0281
Hadoop job information for Stage-1: number of mappers: 29; number of reducers: 8
2013-11-29 00:37:08,107 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 289.56 sec
MapReduce Total cumulative CPU time: 4 minutes 49 seconds 560 msec
Ended Job = job_1385675857984_0281
Loading data to table default.q20_tmp2
Table default.q20_tmp2 stats: [num_partitions: 0, num_files: 8, num_rows: 0, total_size: 98539514, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 29  Reduce: 8   Cumulative CPU: 289.56 sec   HDFS Read: 7775969019 HDFS Write: 98539514 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 49 seconds 560 msec
OK
Time taken: 57.715 seconds
Total MapReduce jobs = 4
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 00:37:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 00:37:11 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-37-08_588_2763285810552456921-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 00:37:11 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-37-08_588_2763285810552456921-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 00:37:11 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:37:11 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:37:11 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:37:11 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:37:11 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:37:11 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:37:11 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:37:12	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:37:13	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_00-37-08_588_2763285810552456921-1/-local-10007/HashTable-Stage-8/MapJoin-mapfile21--.hashtable
2013-11-29 12:37:13	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_00-37-08_588_2763285810552456921-1/-local-10007/HashTable-Stage-8/MapJoin-mapfile21--.hashtable
2013-11-29 12:37:13	End of local task; Time Taken: 1.049 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 4
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0282, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0282/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0282
Hadoop job information for Stage-8: number of mappers: 7; number of reducers: 0
2013-11-29 00:37:37,427 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 31.35 sec
MapReduce Total cumulative CPU time: 31 seconds 350 msec
Ended Job = job_1385675857984_0282
Stage-9 is filtered out by condition resolver.
Stage-10 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0283, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0283/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0283
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 1
2013-11-29 00:38:19,396 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 60.72 sec
MapReduce Total cumulative CPU time: 1 minutes 0 seconds 720 msec
Ended Job = job_1385675857984_0283
Loading data to table default.q20_tmp3
Table default.q20_tmp3 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 914165, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 7   Cumulative CPU: 31.35 sec   HDFS Read: 1204885099 HDFS Write: 2374864 SUCCESS
Job 1: Map: 10  Reduce: 1   Cumulative CPU: 60.72 sec   HDFS Read: 100917966 HDFS Write: 914165 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 32 seconds 70 msec
OK
Time taken: 71.274 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0284, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0284/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0284
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-29 00:38:39,294 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.73 sec
MapReduce Total cumulative CPU time: 5 seconds 730 msec
Ended Job = job_1385675857984_0284
Loading data to table default.q20_tmp4
Table default.q20_tmp4 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 261982, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 5.73 sec   HDFS Read: 914380 HDFS Write: 261982 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 730 msec
OK
Time taken: 19.882 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 00:38:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 00:38:42 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-38-39_744_5509661877686544326-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 00:38:42 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-38-39_744_5509661877686544326-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 00:38:42 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:38:42 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:38:42 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:38:42 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:38:42 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:38:42 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:38:42 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:38:43	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:38:44	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_00-38-39_744_5509661877686544326-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile41--.hashtable
2013-11-29 12:38:44	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_00-38-39_744_5509661877686544326-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile41--.hashtable
2013-11-29 12:38:44	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_00-38-39_744_5509661877686544326-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-11-29 12:38:44	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_00-38-39_744_5509661877686544326-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-11-29 12:38:44	End of local task; Time Taken: 1.375 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0285, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0285/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0285
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-29 00:39:03,545 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 3.64 sec
MapReduce Total cumulative CPU time: 3 seconds 640 msec
Ended Job = job_1385675857984_0285
Loading data to table default.q20_potential_part_promotion
Table default.q20_potential_part_promotion stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 80878, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.64 sec   HDFS Read: 14176572 HDFS Write: 80878 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 640 msec
OK
Time taken: 24.258 seconds
Time:210.73
Running Hive query: tpch/q21_suppliers_who_kept_orders_waiting.hive
13/11/29 00:39:05 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:39:05 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:39:05 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:39:05 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:39:05 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:39:05 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:39:05 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.73 seconds
OK
Time taken: 0.121 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.097 seconds
OK
Time taken: 0.21 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.237 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.03 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.076 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0286, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0286/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0286
Hadoop job information for Stage-1: number of mappers: 29; number of reducers: 8
2013-11-29 00:40:39,990 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 632.62 sec
MapReduce Total cumulative CPU time: 10 minutes 32 seconds 620 msec
Ended Job = job_1385675857984_0286
Loading data to table default.q21_tmp1
Table default.q21_tmp1 stats: [num_partitions: 0, num_files: 8, num_rows: 0, total_size: 251959852, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 29  Reduce: 8   Cumulative CPU: 632.62 sec   HDFS Read: 7775969019 HDFS Write: 251959852 SUCCESS
Total MapReduce CPU Time Spent: 10 minutes 32 seconds 620 msec
OK
Time taken: 86.252 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0287, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0287/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0287
Hadoop job information for Stage-1: number of mappers: 29; number of reducers: 8
2013-11-29 00:41:40,257 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 468.4 sec
MapReduce Total cumulative CPU time: 7 minutes 48 seconds 400 msec
Ended Job = job_1385675857984_0287
Loading data to table default.q21_tmp2
Table default.q21_tmp2 stats: [num_partitions: 0, num_files: 8, num_rows: 0, total_size: 230852238, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 29  Reduce: 8   Cumulative CPU: 468.4 sec   HDFS Read: 7775969019 HDFS Write: 230852238 SUCCESS
Total MapReduce CPU Time Spent: 7 minutes 48 seconds 400 msec
OK
Time taken: 60.171 seconds
Total MapReduce jobs = 14
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 00:41:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 00:41:47 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-41-40_758_476515816496290654-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 00:41:47 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-41-40_758_476515816496290654-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 00:41:47 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:41:47 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:41:47 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:41:47 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:41:47 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:41:47 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:41:47 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:41:48	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:41:49	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_00-41-40_758_476515816496290654-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-11-29 12:41:49	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_00-41-40_758_476515816496290654-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-11-29 12:41:49	End of local task; Time Taken: 0.965 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 14
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0288, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0288/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0288
Hadoop job information for Stage-24: number of mappers: 1; number of reducers: 0
2013-11-29 00:42:01,607 Stage-24 map = 100%,  reduce = 0%, Cumulative CPU 1.98 sec
MapReduce Total cumulative CPU time: 1 seconds 980 msec
Ended Job = job_1385675857984_0288
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 14
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0289, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0289/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0289
Hadoop job information for Stage-8: number of mappers: 30; number of reducers: 8
2013-11-29 00:42:47,797 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 356.24 sec
MapReduce Total cumulative CPU time: 5 minutes 56 seconds 240 msec
Ended Job = job_1385675857984_0289
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 14
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0290, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0290/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0290
Hadoop job information for Stage-1: number of mappers: 14; number of reducers: 2
2013-11-29 00:43:19,590 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 104.38 sec
MapReduce Total cumulative CPU time: 1 minutes 44 seconds 380 msec
Ended Job = job_1385675857984_0290
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0291, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0291/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0291
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1
2013-11-29 00:44:41,213 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 126.18 sec
MapReduce Total cumulative CPU time: 2 minutes 6 seconds 180 msec
Ended Job = job_1385675857984_0291
Stage-25 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 5 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0292, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0292/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0292
Hadoop job information for Stage-3: number of mappers: 7; number of reducers: 1
2013-11-29 00:45:53,513 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 97.42 sec
MapReduce Total cumulative CPU time: 1 minutes 37 seconds 420 msec
Ended Job = job_1385675857984_0292
Launching Job 6 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0293, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0293/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0293
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-29 00:46:12,215 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 2.88 sec
MapReduce Total cumulative CPU time: 2 seconds 880 msec
Ended Job = job_1385675857984_0293
Launching Job 7 out of 14
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0294, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0294/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0294
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-29 00:46:31,661 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 2.91 sec
MapReduce Total cumulative CPU time: 2 seconds 910 msec
Ended Job = job_1385675857984_0294
Loading data to table default.q21_suppliers_who_kept_orders_waiting
Table default.q21_suppliers_who_kept_orders_waiting stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 2200, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.98 sec   HDFS Read: 14176572 HDFS Write: 159410 SUCCESS
Job 1: Map: 30  Reduce: 8   Cumulative CPU: 356.24 sec   HDFS Read: 7776128795 HDFS Write: 67739761 SUCCESS
Job 2: Map: 14  Reduce: 2   Cumulative CPU: 104.38 sec   HDFS Read: 1816992648 HDFS Write: 32680461 SUCCESS
Job 3: Map: 6  Reduce: 1   Cumulative CPU: 126.18 sec   HDFS Read: 284642217 HDFS Write: 31482111 SUCCESS
Job 4: Map: 7  Reduce: 1   Cumulative CPU: 97.42 sec   HDFS Read: 262336161 HDFS Write: 149869 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 2.88 sec   HDFS Read: 150235 HDFS Write: 149869 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 2.91 sec   HDFS Read: 150235 HDFS Write: 2200 SUCCESS
Total MapReduce CPU Time Spent: 11 minutes 31 seconds 990 msec
OK
Time taken: 291.376 seconds
Time:448.11
Running Hive query: tpch/q22_global_sales_opportunity.hive
13/11/29 00:46:33 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:46:33 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:46:33 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:46:33 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:46:33 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:46:33 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:46:33 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.766 seconds
OK
Time taken: 0.12 seconds
OK
Time taken: 0.206 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.203 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.084 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0295, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0295/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0295
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 0
2013-11-29 00:47:03,400 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 13.22 sec
MapReduce Total cumulative CPU time: 13 seconds 220 msec
Ended Job = job_1385675857984_0295
Stage-4 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Stage-5 is filtered out by condition resolver.
Launching Job 3 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0296, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0296/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0296
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
2013-11-29 00:47:17,107 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 2.63 sec
MapReduce Total cumulative CPU time: 2 seconds 630 msec
Ended Job = job_1385675857984_0296
Loading data to table default.q22_customer_tmp
Table default.q22_customer_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 7580355, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2   Cumulative CPU: 13.22 sec   HDFS Read: 244852146 HDFS Write: 7580355 SUCCESS
Job 1: Map: 1   Cumulative CPU: 2.63 sec   HDFS Read: 7580746 HDFS Write: 7580355 SUCCESS
Total MapReduce CPU Time Spent: 15 seconds 850 msec
OK
Time taken: 35.404 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0297, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0297/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0297
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-29 00:47:36,439 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.76 sec
MapReduce Total cumulative CPU time: 3 seconds 760 msec
Ended Job = job_1385675857984_0297
Loading data to table default.q22_customer_tmp1
Table default.q22_customer_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 18, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.76 sec   HDFS Read: 7580578 HDFS Write: 18 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 760 msec
OK
Time taken: 19.165 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0298, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0298/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0298
Hadoop job information for Stage-1: number of mappers: 9; number of reducers: 2
2013-11-29 00:48:13,697 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 107.69 sec
MapReduce Total cumulative CPU time: 1 minutes 47 seconds 690 msec
Ended Job = job_1385675857984_0298
Loading data to table default.q22_orders_tmp
Table default.q22_orders_tmp stats: [num_partitions: 0, num_files: 2, num_rows: 0, total_size: 7259132, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 9  Reduce: 2   Cumulative CPU: 107.69 sec   HDFS Read: 1749250394 HDFS Write: 7259132 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 47 seconds 690 msec
OK
Time taken: 37.256 seconds
Total MapReduce jobs = 2
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 00:48:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 00:48:16 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-48-14_183_7994295606957501971-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 00:48:16 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_00-48-14_183_7994295606957501971-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 00:48:16 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 00:48:16 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 00:48:16 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 00:48:16 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 00:48:16 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 00:48:16 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 00:48:16 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:48:17	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:48:19	Processing rows:	200000	Hashtable size:	199999	Memory usage:	57893952	percentage:	0.121
2013-11-29 12:48:19	Processing rows:	300000	Hashtable size:	299999	Memory usage:	79417880	percentage:	0.166
2013-11-29 12:48:19	Processing rows:	400000	Hashtable size:	399999	Memory usage:	107092816	percentage:	0.224
2013-11-29 12:48:19	Processing rows:	500000	Hashtable size:	499999	Memory usage:	131590848	percentage:	0.276
2013-11-29 12:48:19	Processing rows:	600000	Hashtable size:	599999	Memory usage:	152787200	percentage:	0.32
2013-11-29 12:48:20	Processing rows:	700000	Hashtable size:	699999	Memory usage:	165298392	percentage:	0.346
2013-11-29 12:48:20	Processing rows:	800000	Hashtable size:	799999	Memory usage:	188669128	percentage:	0.395
2013-11-29 12:48:21	Processing rows:	900000	Hashtable size:	899999	Memory usage:	219532896	percentage:	0.46
2013-11-29 12:48:21	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_00-48-14_183_7994295606957501971-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile10--.hashtable
2013-11-29 12:48:21	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_00-48-14_183_7994295606957501971-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile10--.hashtable
2013-11-29 12:48:21	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_00-48-14_183_7994295606957501971-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-11-29 12:48:21	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_00-48-14_183_7994295606957501971-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-11-29 12:48:21	End of local task; Time Taken: 4.26 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0299, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0299/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0299
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-29 00:48:49,056 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 13.09 sec
MapReduce Total cumulative CPU time: 13 seconds 90 msec
Ended Job = job_1385675857984_0299
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0300, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0300/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0300
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-29 00:49:06,484 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.81 sec
MapReduce Total cumulative CPU time: 1 seconds 810 msec
Ended Job = job_1385675857984_0300
Loading data to table default.q22_global_sales_opportunity
Table default.q22_global_sales_opportunity stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 196, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 13.09 sec   HDFS Read: 7580578 HDFS Write: 313 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 1.81 sec   HDFS Read: 680 HDFS Write: 196 SUCCESS
Total MapReduce CPU Time Spent: 14 seconds 900 msec
OK
Time taken: 52.775 seconds
Time:154.82
