Running Hive from /opt/hive-0.12.0
Running Hadoop from 
Running Hive query: tpch/q1_pricing_summary_report.hive
13/11/28 19:02:13 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:02:13 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:02:13 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:02:13 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:02:13 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:02:13 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:02:13 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.845 seconds
OK
Time taken: 0.233 seconds
OK
Time taken: 0.23 seconds
OK
Time taken: 0.066 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0001, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0001/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0001
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-28 19:02:58,184 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 44.67 sec
MapReduce Total cumulative CPU time: 44 seconds 670 msec
Ended Job = job_1385675857984_0001
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0002, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0002/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0002
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-28 19:03:18,502 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.16 sec
MapReduce Total cumulative CPU time: 2 seconds 160 msec
Ended Job = job_1385675857984_0002
Loading data to table default.q1_pricing_summary_report
Table default.q1_pricing_summary_report stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 570, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 44.67 sec   HDFS Read: 759884717 HDFS Write: 423 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 2.16 sec   HDFS Read: 790 HDFS Write: 570 SUCCESS
Total MapReduce CPU Time Spent: 46 seconds 830 msec
OK
Time taken: 56.81 seconds
Time:66.86
Running Hive query: tpch/q2_minimum_cost_supplier.hive
13/11/28 19:03:20 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:03:20 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:03:20 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:03:20 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:03:20 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:03:20 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:03:20 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.684 seconds
OK
Time taken: 0.095 seconds
OK
Time taken: 0.111 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.215 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.214 seconds
OK
Time taken: 0.038 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.035 seconds
OK
Time taken: 0.038 seconds
OK
Time taken: 0.092 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.042 seconds
Total MapReduce jobs = 7
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:03:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:03:35 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-03-29_522_685645314500515484-1/-local-10016/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:03:35 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-03-29_522_685645314500515484-1/-local-10016/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:03:35 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:03:35 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:03:35 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:03:35 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:03:35 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:03:35 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:03:35 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:03:36	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:03:38	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-03-29_522_685645314500515484-1/-local-10013/HashTable-Stage-15/MapJoin-mapfile41--.hashtable
2013-11-28 07:03:38	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-03-29_522_685645314500515484-1/-local-10013/HashTable-Stage-15/MapJoin-mapfile41--.hashtable
2013-11-28 07:03:38	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-03-29_522_685645314500515484-1/-local-10013/HashTable-Stage-15/MapJoin-mapfile51--.hashtable
2013-11-28 07:03:38	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-03-29_522_685645314500515484-1/-local-10013/HashTable-Stage-15/MapJoin-mapfile51--.hashtable
2013-11-28 07:03:38	End of local task; Time Taken: 1.946 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 7
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0003, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0003/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0003
Hadoop job information for Stage-15: number of mappers: 1; number of reducers: 0
2013-11-28 19:03:52,775 Stage-15 map = 100%,  reduce = 0%, Cumulative CPU 1.49 sec
MapReduce Total cumulative CPU time: 1 seconds 490 msec
Ended Job = job_1385675857984_0003
Stage-19 is filtered out by condition resolver.
Stage-20 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0004, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0004/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0004
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-28 19:04:18,122 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 12.25 sec
MapReduce Total cumulative CPU time: 12 seconds 250 msec
Ended Job = job_1385675857984_0004
Stage-17 is filtered out by condition resolver.
Stage-18 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0005, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0005/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0005
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
2013-11-28 19:04:44,430 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 10.43 sec
MapReduce Total cumulative CPU time: 10 seconds 430 msec
Ended Job = job_1385675857984_0005
Loading data to table default.q2_minimum_cost_supplier_tmp1
Table default.q2_minimum_cost_supplier_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 109507, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.49 sec   HDFS Read: 2424 HDFS Write: 324185 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 12.25 sec   HDFS Read: 119309371 HDFS Write: 27325124 SUCCESS
Job 2: Map: 2  Reduce: 1   Cumulative CPU: 10.43 sec   HDFS Read: 51460811 HDFS Write: 109507 SUCCESS
Total MapReduce CPU Time Spent: 24 seconds 170 msec
OK
Time taken: 75.469 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0006, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0006/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-28 19:05:04,186 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.93 sec
MapReduce Total cumulative CPU time: 1 seconds 930 msec
Ended Job = job_1385675857984_0006
Loading data to table default.q2_minimum_cost_supplier_tmp2
Table default.q2_minimum_cost_supplier_tmp2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6083, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 1.93 sec   HDFS Read: 109743 HDFS Write: 6083 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 930 msec
OK
Time taken: 19.699 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:05:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:05:07 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-05-04_692_2934796007542677001-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:05:07 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-05-04_692_2934796007542677001-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:05:07 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:05:07 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:05:07 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:05:07 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:05:07 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:05:07 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:05:07 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:05:07	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:05:08	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-05-04_692_2934796007542677001-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile61--.hashtable
2013-11-28 07:05:08	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-05-04_692_2934796007542677001-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile61--.hashtable
2013-11-28 07:05:08	End of local task; Time Taken: 0.692 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0007, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0007/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0007
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-28 19:05:27,388 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.15 sec
MapReduce Total cumulative CPU time: 2 seconds 150 msec
Ended Job = job_1385675857984_0007
Loading data to table default.q2_minimum_cost_supplier
Table default.q2_minimum_cost_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16329, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.15 sec   HDFS Read: 109743 HDFS Write: 16329 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 150 msec
OK
Time taken: 23.191 seconds
Time:128.80
Running Hive query: tpch/q3_shipping_priority.hive
13/11/28 19:05:29 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:05:29 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:05:29 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:05:29 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:05:29 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:05:29 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:05:29 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.684 seconds
OK
Time taken: 0.118 seconds
OK
Time taken: 0.112 seconds
OK
Time taken: 0.209 seconds
OK
Time taken: 0.233 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.047 seconds
Total MapReduce jobs = 7
Stage-16 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0008, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0008/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0008
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-11-28 19:06:06,032 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.67 sec
MapReduce Total cumulative CPU time: 17 seconds 670 msec
Ended Job = job_1385675857984_0008
Stage-14 is filtered out by condition resolver.
Stage-15 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0009, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0009/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0009
Hadoop job information for Stage-2: number of mappers: 5; number of reducers: 1
2013-11-28 19:06:39,828 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 46.03 sec
MapReduce Total cumulative CPU time: 46 seconds 30 msec
Ended Job = job_1385675857984_0009
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0010, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0010/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0010
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 19:06:59,145 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 4.96 sec
MapReduce Total cumulative CPU time: 4 seconds 960 msec
Ended Job = job_1385675857984_0010
Launching Job 4 out of 7
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0011, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0011/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0011
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 19:07:20,483 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.17 sec
MapReduce Total cumulative CPU time: 4 seconds 170 msec
Ended Job = job_1385675857984_0011
Loading data to table default.q3_shipping_priority
Table default.q3_shipping_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 340, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 3  Reduce: 1   Cumulative CPU: 17.67 sec   HDFS Read: 196303005 HDFS Write: 4901873 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 46.03 sec   HDFS Read: 764786957 HDFS Write: 481131 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 4.96 sec   HDFS Read: 481498 HDFS Write: 481131 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 4.17 sec   HDFS Read: 481498 HDFS Write: 340 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 12 seconds 830 msec
OK
Time taken: 103.243 seconds
Time:113.12
Running Hive query: tpch/q4_order_priority.hive
13/11/28 19:07:22 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:07:22 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:07:22 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:07:22 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:07:22 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:07:22 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:07:22 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.865 seconds
OK
Time taken: 0.119 seconds
OK
Time taken: 0.213 seconds
OK
Time taken: 0.165 seconds
OK
Time taken: 0.212 seconds
OK
Time taken: 0.068 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.058 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0012, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0012/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0012
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-28 19:08:00,011 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 32.42 sec
MapReduce Total cumulative CPU time: 32 seconds 420 msec
Ended Job = job_1385675857984_0012
Loading data to table default.q4_order_priority_tmp
Table default.q4_order_priority_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 10748076, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 32.42 sec   HDFS Read: 759884717 HDFS Write: 10748076 SUCCESS
Total MapReduce CPU Time Spent: 32 seconds 420 msec
OK
Time taken: 29.483 seconds
Total MapReduce jobs = 4
Stage-9 is selected by condition resolver.
Stage-1 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:08:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:08:04 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-08-00_601_4191305326869295858-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:08:04 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-08-00_601_4191305326869295858-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:08:04 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:08:04 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:08:04 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:08:04 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:08:04 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:08:04 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:08:04 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:08:05	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:08:06	Processing rows:	200000	Hashtable size:	199999	Memory usage:	38088320	percentage:	0.08
2013-11-28 07:08:06	Processing rows:	300000	Hashtable size:	299999	Memory usage:	49088560	percentage:	0.103
2013-11-28 07:08:06	Processing rows:	400000	Hashtable size:	399999	Memory usage:	62185952	percentage:	0.13
2013-11-28 07:08:06	Processing rows:	500000	Hashtable size:	499999	Memory usage:	71089048	percentage:	0.149
2013-11-28 07:08:06	Processing rows:	600000	Hashtable size:	599999	Memory usage:	79992112	percentage:	0.168
2013-11-28 07:08:06	Processing rows:	700000	Hashtable size:	699999	Memory usage:	88895216	percentage:	0.186
2013-11-28 07:08:06	Processing rows:	800000	Hashtable size:	799999	Memory usage:	106186936	percentage:	0.223
2013-11-28 07:08:07	Processing rows:	900000	Hashtable size:	899999	Memory usage:	93146408	percentage:	0.195
2013-11-28 07:08:07	Processing rows:	1000000	Hashtable size:	999999	Memory usage:	101905200	percentage:	0.214
2013-11-28 07:08:07	Processing rows:	1100000	Hashtable size:	1099999	Memory usage:	110663992	percentage:	0.232
2013-11-28 07:08:07	Processing rows:	1200000	Hashtable size:	1199999	Memory usage:	119422808	percentage:	0.25
2013-11-28 07:08:07	Processing rows:	1300000	Hashtable size:	1299999	Memory usage:	128181608	percentage:	0.269
2013-11-28 07:08:07	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-08-00_601_4191305326869295858-1/-local-10004/HashTable-Stage-6/MapJoin-mapfile01--.hashtable
2013-11-28 07:08:08	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-08-00_601_4191305326869295858-1/-local-10004/HashTable-Stage-6/MapJoin-mapfile01--.hashtable
2013-11-28 07:08:08	End of local task; Time Taken: 3.51 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 4
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0013, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0013/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0013
Hadoop job information for Stage-6: number of mappers: 2; number of reducers: 0
2013-11-28 19:08:34,264 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 20.43 sec
MapReduce Total cumulative CPU time: 20 seconds 430 msec
Ended Job = job_1385675857984_0013
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0014, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0014/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0014
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-28 19:08:55,019 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.04 sec
MapReduce Total cumulative CPU time: 2 seconds 40 msec
Ended Job = job_1385675857984_0014
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0015, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0015/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0015
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 19:09:12,951 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.9 sec
MapReduce Total cumulative CPU time: 1 seconds 900 msec
Ended Job = job_1385675857984_0015
Loading data to table default.q4_order_priority
Table default.q4_order_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 77, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2   Cumulative CPU: 20.43 sec   HDFS Read: 171956657 HDFS Write: 486 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 2.04 sec   HDFS Read: 1220 HDFS Write: 243 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.9 sec   HDFS Read: 610 HDFS Write: 77 SUCCESS
Total MapReduce CPU Time Spent: 24 seconds 370 msec
OK
Time taken: 72.84 seconds
Time:112.44
Running Hive query: tpch/q5_local_supplier_volume.hive
13/11/28 19:09:14 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:09:14 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:09:14 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:09:14 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:09:14 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:09:14 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:09:14 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.821 seconds
OK
Time taken: 0.104 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.098 seconds
OK
Time taken: 0.225 seconds
OK
Time taken: 0.246 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.074 seconds
OK
Time taken: 0.066 seconds
Total MapReduce jobs = 12
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:09:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:09:30 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-09-23_936_5584819693914917350-1/-local-10023/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:09:30 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-09-23_936_5584819693914917350-1/-local-10023/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:09:31 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:09:31 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:09:31 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:09:31 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:09:31 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:09:31 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:09:31 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:09:31	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:09:33	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-09-23_936_5584819693914917350-1/-local-10020/HashTable-Stage-22/MapJoin-mapfile60--.hashtable
2013-11-28 07:09:33	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-09-23_936_5584819693914917350-1/-local-10020/HashTable-Stage-22/MapJoin-mapfile60--.hashtable
2013-11-28 07:09:33	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-09-23_936_5584819693914917350-1/-local-10020/HashTable-Stage-22/MapJoin-mapfile71--.hashtable
2013-11-28 07:09:33	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-09-23_936_5584819693914917350-1/-local-10020/HashTable-Stage-22/MapJoin-mapfile71--.hashtable
2013-11-28 07:09:33	End of local task; Time Taken: 1.376 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 12
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0016, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0016/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0016
Hadoop job information for Stage-22: number of mappers: 1; number of reducers: 0
2013-11-28 19:09:45,922 Stage-22 map = 100%,  reduce = 0%, Cumulative CPU 1.38 sec
MapReduce Total cumulative CPU time: 1 seconds 380 msec
Ended Job = job_1385675857984_0016
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 12
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0017, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0017/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0017
Hadoop job information for Stage-8: number of mappers: 5; number of reducers: 1
2013-11-28 19:10:31,309 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 59.98 sec
MapReduce Total cumulative CPU time: 59 seconds 980 msec
Ended Job = job_1385675857984_0017
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 12
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0018, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0018/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0018
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-11-28 19:10:58,743 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 24.96 sec
MapReduce Total cumulative CPU time: 24 seconds 960 msec
Ended Job = job_1385675857984_0018
Stage-24 is filtered out by condition resolver.
Stage-25 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 12
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0019, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0019/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0019
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-28 19:11:26,577 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 11.17 sec
MapReduce Total cumulative CPU time: 11 seconds 170 msec
Ended Job = job_1385675857984_0019
Launching Job 5 out of 12
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0020, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0020/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0020
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 19:11:44,013 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.46 sec
MapReduce Total cumulative CPU time: 1 seconds 460 msec
Ended Job = job_1385675857984_0020
Launching Job 6 out of 12
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0021, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0021/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0021
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 19:12:02,416 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.4 sec
MapReduce Total cumulative CPU time: 1 seconds 400 msec
Ended Job = job_1385675857984_0021
Loading data to table default.q5_local_supplier_volume
Table default.q5_local_supplier_volume stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 137, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.38 sec   HDFS Read: 2424 HDFS Write: 57058 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 59.98 sec   HDFS Read: 759942142 HDFS Write: 54824341 SUCCESS
Job 2: Map: 3  Reduce: 1   Cumulative CPU: 24.96 sec   HDFS Read: 226781365 HDFS Write: 8237478 SUCCESS
Job 3: Map: 2  Reduce: 1   Cumulative CPU: 11.17 sec   HDFS Read: 32584193 HDFS Write: 257 SUCCESS
Job 4: Map: 1  Reduce: 1   Cumulative CPU: 1.46 sec   HDFS Read: 624 HDFS Write: 257 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 1.4 sec   HDFS Read: 624 HDFS Write: 137 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 40 seconds 350 msec
OK
Time taken: 159.007 seconds
Time:169.49
Running Hive query: tpch/q6_forecast_revenue_change.hive
13/11/28 19:12:04 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:12:04 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:12:04 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:12:04 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:12:04 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:12:04 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:12:04 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.924 seconds
OK
Time taken: 0.207 seconds
OK
Time taken: 0.203 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0022, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0022/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0022
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-28 19:12:37,534 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 25.32 sec
MapReduce Total cumulative CPU time: 25 seconds 320 msec
Ended Job = job_1385675857984_0022
Loading data to table default.q6_forecast_revenue_change
Table default.q6_forecast_revenue_change stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 20, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 25.32 sec   HDFS Read: 759884717 HDFS Write: 20 SUCCESS
Total MapReduce CPU Time Spent: 25 seconds 320 msec
OK
Time taken: 25.361 seconds
Time:35.11
Running Hive query: tpch/q7_volume_shipping.hive
13/11/28 19:12:39 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:12:39 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:12:39 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:12:39 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:12:39 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:12:39 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:12:39 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.663 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.134 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.098 seconds
OK
Time taken: 0.232 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.187 seconds
OK
Time taken: 0.06 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.048 seconds
Total MapReduce jobs = 3
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:12:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:12:51 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-12-48_431_8096203917906580018-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:12:51 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-12-48_431_8096203917906580018-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:12:51 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:12:51 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:12:51 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:12:51 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:12:51 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:12:51 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:12:51 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:12:52	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:12:53	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-12-48_431_8096203917906580018-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-11-28 07:12:53	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-12-48_431_8096203917906580018-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-11-28 07:12:53	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-12-48_431_8096203917906580018-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-11-28 07:12:53	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-12-48_431_8096203917906580018-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-11-28 07:12:53	End of local task; Time Taken: 1.096 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0023, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0023/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0023
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 0
2013-11-28 19:13:07,279 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.43 sec
MapReduce Total cumulative CPU time: 1 seconds 430 msec
Ended Job = job_1385675857984_0023
Stage-5 is selected by condition resolver.
Stage-4 is filtered out by condition resolver.
Stage-6 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-11-28_19-12-48_431_8096203917906580018-1/-ext-10000
Loading data to table default.q7_volume_shipping_tmp
Table default.q7_volume_shipping_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 38, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.43 sec   HDFS Read: 2424 HDFS Write: 38 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 430 msec
OK
Time taken: 19.401 seconds
Total MapReduce jobs = 6
Stage-6 is selected by condition resolver.
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0024, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0024/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0024
Hadoop job information for Stage-6: number of mappers: 6; number of reducers: 1
2013-11-28 19:13:47,335 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 53.64 sec
MapReduce Total cumulative CPU time: 53 seconds 640 msec
Ended Job = job_1385675857984_0024
Stage-20 is filtered out by condition resolver.
Stage-21 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0025, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0025/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0025
Hadoop job information for Stage-7: number of mappers: 2; number of reducers: 1
2013-11-28 19:14:22,008 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 27.23 sec
MapReduce Total cumulative CPU time: 27 seconds 230 msec
Ended Job = job_1385675857984_0025
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:14:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:14:24 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-13-07_838_6168563315767944240-1/-local-10018/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:14:24 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-13-07_838_6168563315767944240-1/-local-10018/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:14:24 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:14:24 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:14:24 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:14:24 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:14:24 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:14:24 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:14:24 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:14:25	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:14:26	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-13-07_838_6168563315767944240-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile30--.hashtable
2013-11-28 07:14:26	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-13-07_838_6168563315767944240-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile30--.hashtable
2013-11-28 07:14:26	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-13-07_838_6168563315767944240-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-11-28 07:14:26	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-13-07_838_6168563315767944240-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-11-28 07:14:26	End of local task; Time Taken: 1.172 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0026, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0026/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0026
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 19:14:53,719 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 11.74 sec
MapReduce Total cumulative CPU time: 11 seconds 740 msec
Ended Job = job_1385675857984_0026
Launching Job 4 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0027, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0027/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0027
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 19:15:11,184 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.52 sec
MapReduce Total cumulative CPU time: 1 seconds 520 msec
Ended Job = job_1385675857984_0027
Loading data to table default.q7_volume_shipping
Table default.q7_volume_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 160, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 6  Reduce: 1   Cumulative CPU: 53.64 sec   HDFS Read: 931841374 HDFS Write: 93291795 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 27.23 sec   HDFS Read: 117638510 HDFS Write: 88566860 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 11.74 sec   HDFS Read: 88567227 HDFS Write: 268 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 1.52 sec   HDFS Read: 635 HDFS Write: 160 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 34 seconds 130 msec
OK
Time taken: 123.859 seconds
Time:153.62
Running Hive query: tpch/q8_national_market_share.hive
13/11/28 19:15:12 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:15:12 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:15:12 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:15:12 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:15:12 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:15:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:15:12 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.834 seconds
OK
Time taken: 0.125 seconds
OK
Time taken: 0.142 seconds
OK
Time taken: 0.139 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.092 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.224 seconds
OK
Time taken: 0.194 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.056 seconds
OK
Time taken: 0.044 seconds
OK
Time taken: 0.047 seconds
OK
Time taken: 0.082 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:15:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:15:31 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-15-22_386_4024233662446545173-1/-local-10031/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:15:31 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-15-22_386_4024233662446545173-1/-local-10031/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:15:31 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:15:31 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:15:31 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:15:31 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:15:31 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:15:31 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:15:31 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:15:32	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:15:33	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-15-22_386_4024233662446545173-1/-local-10028/HashTable-Stage-30/MapJoin-mapfile101--.hashtable
2013-11-28 07:15:33	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-15-22_386_4024233662446545173-1/-local-10028/HashTable-Stage-30/MapJoin-mapfile101--.hashtable
2013-11-28 07:15:33	End of local task; Time Taken: 0.996 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0028, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0028/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0028
Hadoop job information for Stage-30: number of mappers: 1; number of reducers: 0
2013-11-28 19:15:45,843 Stage-30 map = 100%,  reduce = 0%, Cumulative CPU 0.98 sec
MapReduce Total cumulative CPU time: 980 msec
Ended Job = job_1385675857984_0028
Stage-38 is filtered out by condition resolver.
Stage-39 is filtered out by condition resolver.
Stage-9 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0029, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0029/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0029
Hadoop job information for Stage-9: number of mappers: 2; number of reducers: 1
2013-11-28 19:16:06,529 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 6.34 sec
MapReduce Total cumulative CPU time: 6 seconds 340 msec
Ended Job = job_1385675857984_0029
Stage-36 is filtered out by condition resolver.
Stage-37 is filtered out by condition resolver.
Stage-10 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0030, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0030/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0030
Hadoop job information for Stage-10: number of mappers: 3; number of reducers: 1
2013-11-28 19:16:27,901 Stage-10 map = 100%,  reduce = 100%, Cumulative CPU 16.31 sec
MapReduce Total cumulative CPU time: 16 seconds 310 msec
Ended Job = job_1385675857984_0030
Stage-34 is filtered out by condition resolver.
Stage-35 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0031, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0031/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0031
Hadoop job information for Stage-1: number of mappers: 5; number of reducers: 1
2013-11-28 19:17:09,087 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 60.52 sec
MapReduce Total cumulative CPU time: 1 minutes 0 seconds 520 msec
Ended Job = job_1385675857984_0031
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0032, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0032/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0032
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-28 19:17:31,900 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 12.07 sec
MapReduce Total cumulative CPU time: 12 seconds 70 msec
Ended Job = job_1385675857984_0032
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:17:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:17:34 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-15-22_386_4024233662446545173-1/-local-10043/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:17:34 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-15-22_386_4024233662446545173-1/-local-10043/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:17:34 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:17:34 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:17:34 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:17:34 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:17:34 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:17:34 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:17:34 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:17:35	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:17:35	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-15-22_386_4024233662446545173-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-11-28 07:17:35	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-15-22_386_4024233662446545173-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-11-28 07:17:36	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-15-22_386_4024233662446545173-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile10--.hashtable
2013-11-28 07:17:36	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-15-22_386_4024233662446545173-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile10--.hashtable
2013-11-28 07:17:36	End of local task; Time Taken: 1.246 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0033, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0033/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0033
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-28 19:17:55,908 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 4.08 sec
MapReduce Total cumulative CPU time: 4 seconds 80 msec
Ended Job = job_1385675857984_0033
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0034, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0034/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0034
Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 1
2013-11-28 19:18:13,473 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 1.39 sec
MapReduce Total cumulative CPU time: 1 seconds 390 msec
Ended Job = job_1385675857984_0034
Loading data to table default.q8_national_market_share
Table default.q8_national_market_share stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 50, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.98 sec   HDFS Read: 2424 HDFS Write: 186 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 6.34 sec   HDFS Read: 24346901 HDFS Write: 621992 SUCCESS
Job 2: Map: 3  Reduce: 1   Cumulative CPU: 16.31 sec   HDFS Read: 172579016 HDFS Write: 2941437 SUCCESS
Job 3: Map: 5  Reduce: 1   Cumulative CPU: 60.52 sec   HDFS Read: 762826521 HDFS Write: 18639423 SUCCESS
Job 4: Map: 2  Reduce: 1   Cumulative CPU: 12.07 sec   HDFS Read: 42775111 HDFS Write: 123284 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 4.08 sec   HDFS Read: 123651 HDFS Write: 152 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.39 sec   HDFS Read: 519 HDFS Write: 50 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 41 seconds 690 msec
OK
Time taken: 171.593 seconds
Time:182.28
Running Hive query: tpch/q9_product_type_profit.hive
13/11/28 19:18:15 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:18:15 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:18:15 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:18:15 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:18:15 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:18:15 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:18:15 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.758 seconds
OK
Time taken: 0.121 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.2 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.052 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.072 seconds
OK
Time taken: 0.059 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:18:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:18:32 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-18-24_372_8413154648248305457-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:18:32 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-18-24_372_8413154648248305457-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:18:32 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:18:32 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:18:32 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:18:32 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:18:32 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:18:32 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:18:32 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:18:33	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:18:33	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-18-24_372_8413154648248305457-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-11-28 07:18:33	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-18-24_372_8413154648248305457-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-11-28 07:18:33	End of local task; Time Taken: 0.613 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0035, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0035/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0035
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-11-28 19:18:46,625 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 1.88 sec
MapReduce Total cumulative CPU time: 1 seconds 880 msec
Ended Job = job_1385675857984_0035
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0036, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0036/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0036
Hadoop job information for Stage-8: number of mappers: 5; number of reducers: 1
2013-11-28 19:19:48,413 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 83.42 sec
MapReduce Total cumulative CPU time: 1 minutes 23 seconds 420 msec
Ended Job = job_1385675857984_0036
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0037, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0037/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0037
Ended Job = job_1385675857984_0037
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0038, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0038/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0038
Hadoop job information for Stage-2: number of mappers: 3; number of reducers: 1
2013-11-28 19:22:08,739 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 69.74 sec
MapReduce Total cumulative CPU time: 1 minutes 9 seconds 740 msec
Ended Job = job_1385675857984_0038
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0039, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0039/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0039
Hadoop job information for Stage-3: number of mappers: 3; number of reducers: 1
2013-11-28 19:22:36,042 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 23.71 sec
MapReduce Total cumulative CPU time: 23 seconds 710 msec
Ended Job = job_1385675857984_0039
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0040, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0040/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0040
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 19:22:58,490 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.55 sec
MapReduce Total cumulative CPU time: 1 seconds 550 msec
Ended Job = job_1385675857984_0040
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0041, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0041/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0041
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-28 19:23:16,067 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 1.49 sec
MapReduce Total cumulative CPU time: 1 seconds 490 msec
Ended Job = job_1385675857984_0041
Loading data to table default.q9_product_type_profit
Table default.q9_product_type_profit stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 5817, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.88 sec   HDFS Read: 1409388 HDFS Write: 283161 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 83.42 sec   HDFS Read: 760168245 HDFS Write: 361738889 SUCCESS
Job 2:  Cumulative CPU: 98.32 sec   HDFS Read: 480730560 HDFS Write: 392272628 SUCCESS
Job 3: Map: 3  Reduce: 1   Cumulative CPU: 69.74 sec   HDFS Read: 416415085 HDFS Write: 19699616 SUCCESS
Job 4: Map: 3  Reduce: 1   Cumulative CPU: 23.71 sec   HDFS Read: 191656640 HDFS Write: 6470 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 1.55 sec   HDFS Read: 6837 HDFS Write: 6470 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.49 sec   HDFS Read: 6837 HDFS Write: 5817 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 40 seconds 110 msec
OK
Time taken: 292.201 seconds
Time:302.59
Running Hive query: tpch/q10_returned_item.hive
13/11/28 19:23:17 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:23:17 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:23:17 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:23:17 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:23:17 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:23:17 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:23:17 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.682 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.265 seconds
OK
Time taken: 0.211 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.077 seconds
Total MapReduce jobs = 8
Stage-20 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 1 out of 8
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0042, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0042/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0042
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-11-28 19:23:52,787 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 16.44 sec
MapReduce Total cumulative CPU time: 16 seconds 440 msec
Ended Job = job_1385675857984_0042
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:23:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:23:55 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-23-26_623_8418922085824144880-1/-local-10017/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:23:55 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-23-26_623_8418922085824144880-1/-local-10017/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:23:55 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:23:55 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:23:55 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:23:55 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:23:55 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:23:55 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:23:55 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:23:56	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:23:56	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-23-26_623_8418922085824144880-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-11-28 07:23:56	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-23-26_623_8418922085824144880-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-11-28 07:23:56	End of local task; Time Taken: 0.68 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 8
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0043, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0043/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0043
Hadoop job information for Stage-13: number of mappers: 1; number of reducers: 0
2013-11-28 19:24:10,351 Stage-13 map = 100%,  reduce = 0%, Cumulative CPU 3.57 sec
MapReduce Total cumulative CPU time: 3 seconds 570 msec
Ended Job = job_1385675857984_0043
Stage-17 is filtered out by condition resolver.
Stage-18 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 3 out of 8
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0044, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0044/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0044
Hadoop job information for Stage-3: number of mappers: 5; number of reducers: 1
2013-11-28 19:24:38,877 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 38.6 sec
MapReduce Total cumulative CPU time: 38 seconds 600 msec
Ended Job = job_1385675857984_0044
Launching Job 4 out of 8
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0045, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0045/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0045
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 19:24:59,596 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 6.29 sec
MapReduce Total cumulative CPU time: 6 seconds 290 msec
Ended Job = job_1385675857984_0045
Launching Job 5 out of 8
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0046, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0046/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0046
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-28 19:25:18,432 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 4.62 sec
MapReduce Total cumulative CPU time: 4 seconds 620 msec
Ended Job = job_1385675857984_0046
Loading data to table default.q10_returned_item
Table default.q10_returned_item stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3315, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 3  Reduce: 1   Cumulative CPU: 16.44 sec   HDFS Read: 196303005 HDFS Write: 9679083 SUCCESS
Job 1: Map: 1   Cumulative CPU: 3.57 sec   HDFS Read: 9679450 HDFS Write: 10085522 SUCCESS
Job 2: Map: 5  Reduce: 1   Cumulative CPU: 38.6 sec   HDFS Read: 769970606 HDFS Write: 6865977 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 6.29 sec   HDFS Read: 6866344 HDFS Write: 6865957 SUCCESS
Job 4: Map: 1  Reduce: 1   Cumulative CPU: 4.62 sec   HDFS Read: 6866324 HDFS Write: 3315 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 9 seconds 520 msec
OK
Time taken: 112.355 seconds
Time:122.40
Running Hive query: tpch/q11_important_stock.hive
13/11/28 19:25:20 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:25:20 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:25:20 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:25:20 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:25:20 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:25:20 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:25:20 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.814 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.206 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.188 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.083 seconds
Total MapReduce jobs = 5
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:25:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:25:33 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-25-29_208_7529689329002829378-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:25:33 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-25-29_208_7529689329002829378-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:25:33 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:25:33 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:25:33 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:25:33 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:25:33 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:25:33 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:25:33 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:25:34	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:25:35	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-25-29_208_7529689329002829378-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-11-28 07:25:35	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-25-29_208_7529689329002829378-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-11-28 07:25:35	End of local task; Time Taken: 0.973 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0047, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0047/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0047
Hadoop job information for Stage-10: number of mappers: 1; number of reducers: 0
2013-11-28 19:25:48,390 Stage-10 map = 100%,  reduce = 0%, Cumulative CPU 1.47 sec
MapReduce Total cumulative CPU time: 1 seconds 470 msec
Ended Job = job_1385675857984_0047
Stage-11 is filtered out by condition resolver.
Stage-12 is selected by condition resolver.
Stage-2 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:25:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:25:50 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-25-29_208_7529689329002829378-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:25:50 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-25-29_208_7529689329002829378-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:25:50 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:25:50 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:25:50 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:25:50 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:25:50 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:25:50 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:25:50 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:25:51	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:25:52	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-25-29_208_7529689329002829378-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile10--.hashtable
2013-11-28 07:25:52	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-25-29_208_7529689329002829378-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile10--.hashtable
2013-11-28 07:25:52	End of local task; Time Taken: 0.694 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 3 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0048, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0048/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0048
Hadoop job information for Stage-8: number of mappers: 1; number of reducers: 0
2013-11-28 19:26:07,730 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 4.63 sec
MapReduce Total cumulative CPU time: 4 seconds 630 msec
Ended Job = job_1385675857984_0048
Launching Job 4 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0049, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0049/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0049
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 19:26:26,955 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 5.48 sec
MapReduce Total cumulative CPU time: 5 seconds 480 msec
Ended Job = job_1385675857984_0049
Loading data to table default.q11_part_tmp
Table default.q11_part_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 563609, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.47 sec   HDFS Read: 1409388 HDFS Write: 8056 SUCCESS
Job 1: Map: 1   Cumulative CPU: 4.63 sec   HDFS Read: 118984820 HDFS Write: 863493 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 5.48 sec   HDFS Read: 863860 HDFS Write: 563609 SUCCESS
Total MapReduce CPU Time Spent: 11 seconds 580 msec
OK
Time taken: 58.235 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0050, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0050/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0050
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-28 19:26:45,469 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.53 sec
MapReduce Total cumulative CPU time: 2 seconds 530 msec
Ended Job = job_1385675857984_0050
Loading data to table default.q11_sum_tmp
Table default.q11_sum_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.53 sec   HDFS Read: 563828 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 530 msec
OK
Time taken: 18.691 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:26:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:26:48 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-26-46_136_5525041421803041863-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:26:48 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-26-46_136_5525041421803041863-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:26:48 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:26:48 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:26:48 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:26:48 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:26:48 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:26:48 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:26:48 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:26:49	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:26:49	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-26-46_136_5525041421803041863-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-11-28 07:26:50	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-26-46_136_5525041421803041863-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-11-28 07:26:50	End of local task; Time Taken: 0.647 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0051, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0051/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0051
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-28 19:27:09,246 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.64 sec
MapReduce Total cumulative CPU time: 3 seconds 640 msec
Ended Job = job_1385675857984_0051
Loading data to table default.q11_important_stock
Table default.q11_important_stock stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 20102, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.64 sec   HDFS Read: 563828 HDFS Write: 20102 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 640 msec
OK
Time taken: 23.623 seconds
Time:110.78
Running Hive query: tpch/q12_shipping.hive
13/11/28 19:27:11 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:27:11 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:27:11 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:27:11 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:27:11 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:27:11 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:27:11 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.834 seconds
OK
Time taken: 0.102 seconds
OK
Time taken: 0.226 seconds
OK
Time taken: 0.232 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0052, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0052/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0052
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1
2013-11-28 19:27:50,893 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 41.71 sec
MapReduce Total cumulative CPU time: 41 seconds 710 msec
Ended Job = job_1385675857984_0052
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0053, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0053/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0053
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-28 19:28:08,244 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 1.44 sec
MapReduce Total cumulative CPU time: 1 seconds 440 msec
Ended Job = job_1385675857984_0053
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0054, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0054/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0054
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 19:28:25,926 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.85 sec
MapReduce Total cumulative CPU time: 1 seconds 850 msec
Ended Job = job_1385675857984_0054
Loading data to table default.q12_shipping
Table default.q12_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 38, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 6  Reduce: 1   Cumulative CPU: 41.71 sec   HDFS Read: 931841374 HDFS Write: 152 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 1.44 sec   HDFS Read: 519 HDFS Write: 152 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.85 sec   HDFS Read: 519 HDFS Write: 38 SUCCESS
Total MapReduce CPU Time Spent: 45 seconds 0 msec
OK
Time taken: 66.749 seconds
Time:76.68
Running Hive query: tpch/q13_customer_distribution.hive
13/11/28 19:28:27 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:28:27 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:28:27 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:28:27 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:28:27 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:28:27 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:28:27 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.799 seconds
OK
Time taken: 0.111 seconds
OK
Time taken: 0.209 seconds
OK
Time taken: 0.183 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.059 seconds
Total MapReduce jobs = 4
Stage-1 is selected by condition resolver.
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0055, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0055/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0055
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-11-28 19:29:06,160 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 25.7 sec
MapReduce Total cumulative CPU time: 25 seconds 700 msec
Ended Job = job_1385675857984_0055
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0056, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0056/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0056
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-28 19:29:26,787 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.08 sec
MapReduce Total cumulative CPU time: 6 seconds 80 msec
Ended Job = job_1385675857984_0056
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0057, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0057/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0057
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 19:29:43,914 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.36 sec
MapReduce Total cumulative CPU time: 1 seconds 360 msec
Ended Job = job_1385675857984_0057
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0058, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0058/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0058
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 19:30:01,695 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.89 sec
MapReduce Total cumulative CPU time: 1 seconds 890 msec
Ended Job = job_1385675857984_0058
Loading data to table default.q13_customer_distribution
Table default.q13_customer_distribution stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 295, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 3  Reduce: 1   Cumulative CPU: 25.7 sec   HDFS Read: 196303005 HDFS Write: 3266379 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 6.08 sec   HDFS Read: 3266746 HDFS Write: 955 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.36 sec   HDFS Read: 1322 HDFS Write: 955 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 1.89 sec   HDFS Read: 1322 HDFS Write: 295 SUCCESS
Total MapReduce CPU Time Spent: 35 seconds 30 msec
OK
Time taken: 86.036 seconds
Time:95.75
Running Hive query: tpch/q14_promotion_effect.hive
13/11/28 19:30:03 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:30:03 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:30:03 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:30:03 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:30:03 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:30:03 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:30:03 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.741 seconds
OK
Time taken: 0.104 seconds
OK
Time taken: 0.208 seconds
OK
Time taken: 0.211 seconds
OK
Time taken: 0.046 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 3
Stage-8 is selected by condition resolver.
Stage-1 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:30:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:30:16 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-30-11_963_3400438408208833013-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:30:16 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-30-11_963_3400438408208833013-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:30:16 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:30:16 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:30:16 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:30:16 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:30:16 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:30:16 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:30:16 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:30:17	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:30:18	Processing rows:	200000	Hashtable size:	199999	Memory usage:	85424704	percentage:	0.179
2013-11-28 07:30:18	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-30-11_963_3400438408208833013-1/-local-10003/HashTable-Stage-6/MapJoin-mapfile10--.hashtable
2013-11-28 07:30:18	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-30-11_963_3400438408208833013-1/-local-10003/HashTable-Stage-6/MapJoin-mapfile10--.hashtable
2013-11-28 07:30:18	End of local task; Time Taken: 1.731 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0059, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0059/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0059
Hadoop job information for Stage-6: number of mappers: 4; number of reducers: 0
2013-11-28 19:30:41,583 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 27.88 sec
MapReduce Total cumulative CPU time: 27 seconds 880 msec
Ended Job = job_1385675857984_0059
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0060, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0060/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0060
Hadoop job information for Stage-2: number of mappers: 3; number of reducers: 1
2013-11-28 19:30:59,206 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.15 sec
MapReduce Total cumulative CPU time: 3 seconds 150 msec
Ended Job = job_1385675857984_0060
Loading data to table default.q14_promotion_effect
Table default.q14_promotion_effect stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 19, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4   Cumulative CPU: 27.88 sec   HDFS Read: 759884717 HDFS Write: 516 SUCCESS
Job 1: Map: 3  Reduce: 1   Cumulative CPU: 3.15 sec   HDFS Read: 1839 HDFS Write: 19 SUCCESS
Total MapReduce CPU Time Spent: 31 seconds 30 msec
OK
Time taken: 47.742 seconds
Time:57.48
Running Hive query: tpch/q15_top_supplier.hive
13/11/28 19:31:01 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:31:01 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:31:01 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:31:01 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:31:01 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:31:01 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:31:01 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.957 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.132 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.244 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0061, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0061/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0061
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-28 19:31:36,214 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 26.96 sec
MapReduce Total cumulative CPU time: 26 seconds 960 msec
Ended Job = job_1385675857984_0061
Loading data to table default.revenue
Table default.revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 193813, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 26.96 sec   HDFS Read: 759884717 HDFS Write: 193813 SUCCESS
Total MapReduce CPU Time Spent: 26 seconds 960 msec
OK
Time taken: 26.75 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0062, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0062/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0062
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-28 19:31:54,960 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.3 sec
MapReduce Total cumulative CPU time: 2 seconds 300 msec
Ended Job = job_1385675857984_0062
Loading data to table default.max_revenue
Table default.max_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 19, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.3 sec   HDFS Read: 194027 HDFS Write: 19 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 300 msec
OK
Time taken: 18.665 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:31:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:31:58 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-31-55_463_3310105027715697767-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:31:58 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-31-55_463_3310105027715697767-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:31:58 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:31:58 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:31:58 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:31:58 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:31:58 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:31:58 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:31:58 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:31:58	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:31:59	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-31-55_463_3310105027715697767-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile11--.hashtable
2013-11-28 07:32:00	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-31-55_463_3310105027715697767-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile11--.hashtable
2013-11-28 07:32:00	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-31-55_463_3310105027715697767-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-11-28 07:32:00	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-31-55_463_3310105027715697767-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-11-28 07:32:00	End of local task; Time Taken: 1.231 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0063, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0063/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0063
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 19:32:19,314 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 3.04 sec
MapReduce Total cumulative CPU time: 3 seconds 40 msec
Ended Job = job_1385675857984_0063
Loading data to table default.q15_top_supplier
Table default.q15_top_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 77, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.04 sec   HDFS Read: 1409388 HDFS Write: 77 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 40 msec
OK
Time taken: 24.349 seconds
Time:80.10
Running Hive query: tpch/q16_parts_supplier_relationship.hive
13/11/28 19:32:21 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:32:21 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:32:21 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:32:21 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:32:21 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:32:21 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:32:21 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.574 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.224 seconds
OK
Time taken: 0.121 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.211 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.06 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.082 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0064, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0064/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0064
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2013-11-28 19:32:43,616 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.18 sec
MapReduce Total cumulative CPU time: 2 seconds 180 msec
Ended Job = job_1385675857984_0064
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-11-28_19-32-29_849_8224891623978285458-1/-ext-10000
Loading data to table default.supplier_tmp
Table default.supplier_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 48875, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 2.18 sec   HDFS Read: 1409388 HDFS Write: 48875 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 180 msec
OK
Time taken: 14.355 seconds
Total MapReduce jobs = 3
Stage-10 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:32:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:32:47 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-32-44_207_517731155538144395-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:32:47 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-32-44_207_517731155538144395-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:32:47 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:32:47 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:32:47 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:32:47 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:32:47 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:32:47 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:32:47 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:32:48	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:32:50	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-32-44_207_517731155538144395-1/-local-10005/HashTable-Stage-6/MapJoin-mapfile11--.hashtable
2013-11-28 07:32:50	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-32-44_207_517731155538144395-1/-local-10005/HashTable-Stage-6/MapJoin-mapfile11--.hashtable
2013-11-28 07:32:50	End of local task; Time Taken: 2.208 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0065, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0065/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0065
Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 0
2013-11-28 19:33:08,832 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 8.12 sec
MapReduce Total cumulative CPU time: 8 seconds 120 msec
Ended Job = job_1385675857984_0065
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:33:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:33:11 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-32-44_207_517731155538144395-1/-local-10012/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:33:11 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-32-44_207_517731155538144395-1/-local-10012/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:33:11 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:33:11 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:33:11 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:33:11 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:33:11 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:33:11 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:33:11 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:33:12	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:33:12	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-32-44_207_517731155538144395-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-28 07:33:12	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-32-44_207_517731155538144395-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-28 07:33:12	End of local task; Time Taken: 0.857 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 3 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0066, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0066/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0066
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 0
2013-11-28 19:33:29,231 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 6.51 sec
MapReduce Total cumulative CPU time: 6 seconds 510 msec
Ended Job = job_1385675857984_0066
Loading data to table default.q16_tmp
Table default.q16_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 28413398, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 8.12 sec   HDFS Read: 118984820 HDFS Write: 38640426 SUCCESS
Job 1: Map: 1   Cumulative CPU: 6.51 sec   HDFS Read: 38640792 HDFS Write: 28413398 SUCCESS
Total MapReduce CPU Time Spent: 14 seconds 630 msec
OK
Time taken: 45.528 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0067, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0067/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0067
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-28 19:33:51,444 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.97 sec
MapReduce Total cumulative CPU time: 7 seconds 970 msec
Ended Job = job_1385675857984_0067
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0068, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0068/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0068
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-28 19:34:11,210 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.49 sec
MapReduce Total cumulative CPU time: 5 seconds 490 msec
Ended Job = job_1385675857984_0068
Loading data to table default.q16_parts_supplier_relationship
Table default.q16_parts_supplier_relationship stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 649661, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 7.97 sec   HDFS Read: 28413612 HDFS Write: 917247 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 5.49 sec   HDFS Read: 917614 HDFS Write: 649661 SUCCESS
Total MapReduce CPU Time Spent: 13 seconds 460 msec
OK
Time taken: 41.965 seconds
Time:111.88
Running Hive query: tpch/q17_small_quantity_order_revenue.hive
13/11/28 19:34:13 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:34:13 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:34:13 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:34:13 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:34:13 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:34:13 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:34:13 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.885 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.199 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.23 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.056 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0069, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0069/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0069
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-28 19:35:02,818 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 59.43 sec
MapReduce Total cumulative CPU time: 59 seconds 430 msec
Ended Job = job_1385675857984_0069
Loading data to table default.lineitem_tmp
Table default.lineitem_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 4543028, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 59.43 sec   HDFS Read: 759884717 HDFS Write: 4543028 SUCCESS
Total MapReduce CPU Time Spent: 59 seconds 430 msec
OK
Time taken: 41.57 seconds
Total MapReduce jobs = 3
Stage-12 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0070, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0070/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0070
Hadoop job information for Stage-1: number of mappers: 5; number of reducers: 1
2013-11-28 19:35:47,968 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 62.24 sec
MapReduce Total cumulative CPU time: 1 minutes 2 seconds 240 msec
Ended Job = job_1385675857984_0070
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:35:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:35:50 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-35-03_403_5516266140414976676-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:35:50 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-35-03_403_5516266140414976676-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:35:50 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:35:50 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:35:50 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:35:50 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:35:50 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:35:50 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:35:50 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:35:51	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:35:52	Processing rows:	200000	Hashtable size:	199999	Memory usage:	127349872	percentage:	0.267
2013-11-28 07:35:52	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-35-03_403_5516266140414976676-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-11-28 07:35:52	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-35-03_403_5516266140414976676-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-11-28 07:35:52	End of local task; Time Taken: 1.746 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0071, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0071/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0071
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 19:36:14,090 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 4.49 sec
MapReduce Total cumulative CPU time: 4 seconds 490 msec
Ended Job = job_1385675857984_0071
Loading data to table default.q17_small_quantity_order_revenue
Table default.q17_small_quantity_order_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 18, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 5  Reduce: 1   Cumulative CPU: 62.24 sec   HDFS Read: 784020038 HDFS Write: 225252 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 4.49 sec   HDFS Read: 225619 HDFS Write: 18 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 6 seconds 730 msec
OK
Time taken: 71.147 seconds
Time:122.84
Running Hive query: tpch/q18_large_volume_customer.hive
13/11/28 19:36:15 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:36:15 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:36:15 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:36:15 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:36:15 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:36:15 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:36:15 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.653 seconds
OK
Time taken: 0.11 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.209 seconds
OK
Time taken: 0.138 seconds
OK
Time taken: 0.219 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.058 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0072, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0072/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0072
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-28 19:36:57,559 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 41.0 sec
MapReduce Total cumulative CPU time: 41 seconds 0 msec
Ended Job = job_1385675857984_0072
Loading data to table default.q18_tmp
Table default.q18_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 19917655, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 41.0 sec   HDFS Read: 759884717 HDFS Write: 19917655 SUCCESS
Total MapReduce CPU Time Spent: 41 seconds 0 msec
OK
Time taken: 33.543 seconds
Total MapReduce jobs = 7
Stage-17 is filtered out by condition resolver.
Stage-5 is selected by condition resolver.
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0073, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0073/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0073
Hadoop job information for Stage-5: number of mappers: 3; number of reducers: 1
2013-11-28 19:37:29,316 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 24.18 sec
MapReduce Total cumulative CPU time: 24 seconds 180 msec
Ended Job = job_1385675857984_0073
Stage-15 is filtered out by condition resolver.
Stage-16 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0074, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0074/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0074
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1
2013-11-28 19:38:10,640 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 67.73 sec
MapReduce Total cumulative CPU time: 1 minutes 7 seconds 730 msec
Ended Job = job_1385675857984_0074
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0075, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0075/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0075
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-28 19:38:27,977 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 1.5 sec
MapReduce Total cumulative CPU time: 1 seconds 500 msec
Ended Job = job_1385675857984_0075
Launching Job 4 out of 7
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0076, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0076/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0076
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 19:38:45,502 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.57 sec
MapReduce Total cumulative CPU time: 1 seconds 570 msec
Ended Job = job_1385675857984_0076
Loading data to table default.q18_large_volume_customer
Table default.q18_large_volume_customer stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3427, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 3  Reduce: 1   Cumulative CPU: 24.18 sec   HDFS Read: 196303005 HDFS Write: 94749328 SUCCESS
Job 1: Map: 6  Reduce: 1   Cumulative CPU: 67.73 sec   HDFS Read: 874552281 HDFS Write: 4159 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.5 sec   HDFS Read: 4526 HDFS Write: 4139 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 1.57 sec   HDFS Read: 4506 HDFS Write: 3427 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 34 seconds 980 msec
OK
Time taken: 107.861 seconds
Time:151.43
Running Hive query: tpch/q19_discounted_revenue.hive
13/11/28 19:38:47 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:38:47 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:38:47 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:38:47 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:38:47 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:38:47 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:38:47 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 7.006 seconds
OK
Time taken: 0.11 seconds
OK
Time taken: 0.229 seconds
OK
Time taken: 0.238 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 3
Stage-8 is selected by condition resolver.
Stage-1 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:39:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:39:00 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-38-56_047_6701105457874246172-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:39:00 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-38-56_047_6701105457874246172-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:39:00 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:39:00 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:39:00 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:39:00 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:39:00 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:39:00 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:39:00 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:39:01	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:39:03	Processing rows:	200000	Hashtable size:	199999	Memory usage:	87333536	percentage:	0.183
2013-11-28 07:39:03	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-38-56_047_6701105457874246172-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-28 07:39:03	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-38-56_047_6701105457874246172-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-28 07:39:03	End of local task; Time Taken: 2.311 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0077, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0077/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0077
Hadoop job information for Stage-5: number of mappers: 4; number of reducers: 0
2013-11-28 19:39:39,033 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 62.3 sec
MapReduce Total cumulative CPU time: 1 minutes 2 seconds 300 msec
Ended Job = job_1385675857984_0077
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0078, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0078/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0078
Hadoop job information for Stage-2: number of mappers: 3; number of reducers: 1
2013-11-28 19:40:00,589 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.54 sec
MapReduce Total cumulative CPU time: 2 seconds 540 msec
Ended Job = job_1385675857984_0078
Loading data to table default.q19_discounted_revenue
Table default.q19_discounted_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4   Cumulative CPU: 62.3 sec   HDFS Read: 759884717 HDFS Write: 484 SUCCESS
Job 1: Map: 3  Reduce: 1   Cumulative CPU: 2.54 sec   HDFS Read: 1807 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 4 seconds 840 msec
OK
Time taken: 65.054 seconds
Time:75.09
Running Hive query: tpch/q20_potential_part_promotion.hive
13/11/28 19:40:02 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:40:02 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:40:02 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:40:02 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:40:02 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:40:02 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:40:02 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.71 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.196 seconds
OK
Time taken: 0.13 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.242 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.052 seconds
OK
Time taken: 0.082 seconds
OK
Time taken: 0.06 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.059 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0079, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0079/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0079
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-28 19:40:32,088 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.65 sec
MapReduce Total cumulative CPU time: 3 seconds 650 msec
Ended Job = job_1385675857984_0079
Loading data to table default.q20_tmp1
Table default.q20_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 13746, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.65 sec   HDFS Read: 24135321 HDFS Write: 13746 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 650 msec
OK
Time taken: 20.828 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0080, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0080/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0080
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-28 19:41:00,622 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 34.7 sec
MapReduce Total cumulative CPU time: 34 seconds 700 msec
Ended Job = job_1385675857984_0080
Loading data to table default.q20_tmp2
Table default.q20_tmp2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 8750143, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 34.7 sec   HDFS Read: 759884717 HDFS Write: 8750143 SUCCESS
Total MapReduce CPU Time Spent: 34 seconds 700 msec
OK
Time taken: 28.473 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:41:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:41:03 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-41-01_149_6700196627925717608-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:41:03 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-41-01_149_6700196627925717608-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:41:03 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:41:03 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:41:03 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:41:03 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:41:03 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:41:03 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:41:03 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:41:04	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:41:06	Processing rows:	200000	Hashtable size:	199999	Memory usage:	68361240	percentage:	0.143
2013-11-28 07:41:06	Processing rows:	300000	Hashtable size:	299999	Memory usage:	122843472	percentage:	0.257
2013-11-28 07:41:07	Processing rows:	400000	Hashtable size:	399999	Memory usage:	121211800	percentage:	0.254
2013-11-28 07:41:07	Processing rows:	500000	Hashtable size:	499999	Memory usage:	175694216	percentage:	0.368
2013-11-28 07:41:07	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-41-01_149_6700196627925717608-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-28 07:41:08	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-41-01_149_6700196627925717608-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-28 07:41:08	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-41-01_149_6700196627925717608-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile11--.hashtable
2013-11-28 07:41:08	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-41-01_149_6700196627925717608-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile11--.hashtable
2013-11-28 07:41:08	End of local task; Time Taken: 3.827 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0081, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0081/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0081
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 0
2013-11-28 19:41:26,328 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 7.57 sec
MapReduce Total cumulative CPU time: 7 seconds 570 msec
Ended Job = job_1385675857984_0081
Loading data to table default.q20_tmp3
Table default.q20_tmp3 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 85072, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 7.57 sec   HDFS Read: 118984820 HDFS Write: 85072 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 570 msec
OK
Time taken: 25.642 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0082, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0082/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0082
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-28 19:41:46,217 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.51 sec
MapReduce Total cumulative CPU time: 3 seconds 510 msec
Ended Job = job_1385675857984_0082
Loading data to table default.q20_tmp4
Table default.q20_tmp4 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21481, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.51 sec   HDFS Read: 85287 HDFS Write: 21481 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 510 msec
OK
Time taken: 19.924 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:41:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:41:49 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-41-46_715_163384363945404220-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:41:49 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-41-46_715_163384363945404220-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:41:49 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:41:49 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:41:49 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:41:49 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:41:49 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:41:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:41:49 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:41:50	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:41:51	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-41-46_715_163384363945404220-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-11-28 07:41:51	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-41-46_715_163384363945404220-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-11-28 07:41:51	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-41-46_715_163384363945404220-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile21--.hashtable
2013-11-28 07:41:51	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-41-46_715_163384363945404220-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile21--.hashtable
2013-11-28 07:41:51	End of local task; Time Taken: 1.154 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0083, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0083/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0083
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 19:42:10,288 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.63 sec
MapReduce Total cumulative CPU time: 2 seconds 630 msec
Ended Job = job_1385675857984_0083
Loading data to table default.q20_potential_part_promotion
Table default.q20_potential_part_promotion stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 8381, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.63 sec   HDFS Read: 1409388 HDFS Write: 8381 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 630 msec
OK
Time taken: 24.042 seconds
Time:129.66
Running Hive query: tpch/q21_suppliers_who_kept_orders_waiting.hive
13/11/28 19:42:12 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:42:12 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:42:12 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:42:12 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:42:12 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:42:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:42:12 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.814 seconds
OK
Time taken: 0.13 seconds
OK
Time taken: 0.122 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.209 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.193 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.094 seconds
OK
Time taken: 0.078 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0084, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0084/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0084
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-28 19:43:09,587 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 63.16 sec
MapReduce Total cumulative CPU time: 1 minutes 3 seconds 160 msec
Ended Job = job_1385675857984_0084
Loading data to table default.q21_tmp1
Table default.q21_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 22196331, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 63.16 sec   HDFS Read: 759884717 HDFS Write: 22196331 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 3 seconds 160 msec
OK
Time taken: 48.882 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0085, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0085/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0085
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-28 19:43:47,859 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 50.6 sec
MapReduce Total cumulative CPU time: 50 seconds 600 msec
Ended Job = job_1385675857984_0085
Loading data to table default.q21_tmp2
Table default.q21_tmp2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 20334949, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 50.6 sec   HDFS Read: 759884717 HDFS Write: 20334949 SUCCESS
Total MapReduce CPU Time Spent: 50 seconds 600 msec
OK
Time taken: 38.215 seconds
Total MapReduce jobs = 14
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:43:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:43:55 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-43-48_372_3871418531802555484-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:43:55 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-43-48_372_3871418531802555484-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:43:55 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:43:55 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:43:55 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:43:55 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:43:55 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:43:55 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:43:55 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:43:55	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:43:56	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-43-48_372_3871418531802555484-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-11-28 07:43:56	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-43-48_372_3871418531802555484-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-11-28 07:43:56	End of local task; Time Taken: 1.029 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 14
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0086, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0086/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0086
Hadoop job information for Stage-24: number of mappers: 1; number of reducers: 0
2013-11-28 19:44:09,223 Stage-24 map = 100%,  reduce = 0%, Cumulative CPU 1.47 sec
MapReduce Total cumulative CPU time: 1 seconds 470 msec
Ended Job = job_1385675857984_0086
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0087, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0087/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0087
Hadoop job information for Stage-8: number of mappers: 5; number of reducers: 1
2013-11-28 19:44:38,977 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 40.65 sec
MapReduce Total cumulative CPU time: 40 seconds 650 msec
Ended Job = job_1385675857984_0087
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0088, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0088/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0088
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-11-28 19:45:01,627 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.59 sec
MapReduce Total cumulative CPU time: 17 seconds 590 msec
Ended Job = job_1385675857984_0088
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0089, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0089/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0089
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-28 19:45:27,760 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 14.91 sec
MapReduce Total cumulative CPU time: 14 seconds 910 msec
Ended Job = job_1385675857984_0089
Stage-25 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 5 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0090, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0090/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0090
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
2013-11-28 19:45:54,348 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 15.97 sec
MapReduce Total cumulative CPU time: 15 seconds 970 msec
Ended Job = job_1385675857984_0090
Launching Job 6 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0091, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0091/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0091
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 19:46:11,694 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.52 sec
MapReduce Total cumulative CPU time: 1 seconds 520 msec
Ended Job = job_1385675857984_0091
Launching Job 7 out of 14
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0092, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0092/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0092
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-28 19:46:29,517 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 1.92 sec
MapReduce Total cumulative CPU time: 1 seconds 920 msec
Ended Job = job_1385675857984_0092
Loading data to table default.q21_suppliers_who_kept_orders_waiting
Table default.q21_suppliers_who_kept_orders_waiting stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 2200, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.47 sec   HDFS Read: 1409388 HDFS Write: 16258 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 40.65 sec   HDFS Read: 759901342 HDFS Write: 6802110 SUCCESS
Job 2: Map: 3  Reduce: 1   Cumulative CPU: 17.59 sec   HDFS Read: 178759134 HDFS Write: 3292611 SUCCESS
Job 3: Map: 2  Reduce: 1   Cumulative CPU: 14.91 sec   HDFS Read: 25489524 HDFS Write: 3171898 SUCCESS
Job 4: Map: 2  Reduce: 1   Cumulative CPU: 15.97 sec   HDFS Read: 23507429 HDFS Write: 15443 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 1.52 sec   HDFS Read: 15810 HDFS Write: 15443 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.92 sec   HDFS Read: 15810 HDFS Write: 2200 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 34 seconds 30 msec
OK
Time taken: 161.58 seconds
Time:259.19
Running Hive query: tpch/q22_global_sales_opportunity.hive
13/11/28 19:46:31 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:46:31 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:46:31 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:46:31 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:46:31 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:46:31 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:46:31 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.662 seconds
OK
Time taken: 0.097 seconds
OK
Time taken: 0.206 seconds
OK
Time taken: 0.12 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.202 seconds
OK
Time taken: 0.038 seconds
OK
Time taken: 0.061 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.076 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0093, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0093/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0093
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2013-11-28 19:46:54,715 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.25 sec
MapReduce Total cumulative CPU time: 3 seconds 250 msec
Ended Job = job_1385675857984_0093
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-11-28_19-46-40_127_1856492703212848069-1/-ext-10000
Loading data to table default.q22_customer_tmp
Table default.q22_customer_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 716626, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 3.25 sec   HDFS Read: 24346348 HDFS Write: 716626 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 250 msec
OK
Time taken: 15.19 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0094, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0094/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0094
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-28 19:47:13,996 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.28 sec
MapReduce Total cumulative CPU time: 3 seconds 280 msec
Ended Job = job_1385675857984_0094
Loading data to table default.q22_customer_tmp1
Table default.q22_customer_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 19, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.28 sec   HDFS Read: 716849 HDFS Write: 19 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 280 msec
OK
Time taken: 19.224 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0095, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0095/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0095
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2013-11-28 19:47:39,850 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 14.79 sec
MapReduce Total cumulative CPU time: 14 seconds 790 msec
Ended Job = job_1385675857984_0095
Loading data to table default.q22_orders_tmp
Table default.q22_orders_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 625906, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 14.79 sec   HDFS Read: 171956657 HDFS Write: 625906 SUCCESS
Total MapReduce CPU Time Spent: 14 seconds 790 msec
OK
Time taken: 25.825 seconds
Total MapReduce jobs = 2
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 19:47:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 19:47:42 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-47-40_368_3424086347050842258-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 19:47:42 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_19-47-40_368_3424086347050842258-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 19:47:42 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 19:47:42 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 19:47:42 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 19:47:42 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 19:47:42 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 19:47:42 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 19:47:42 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 07:47:43	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 07:47:44	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-47-40_368_3424086347050842258-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile10--.hashtable
2013-11-28 07:47:45	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-47-40_368_3424086347050842258-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile10--.hashtable
2013-11-28 07:47:45	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_19-47-40_368_3424086347050842258-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-11-28 07:47:45	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_19-47-40_368_3424086347050842258-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-11-28 07:47:45	End of local task; Time Taken: 1.306 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0096, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0096/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0096
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 19:48:05,495 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 4.71 sec
MapReduce Total cumulative CPU time: 4 seconds 710 msec
Ended Job = job_1385675857984_0096
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0097, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0097/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0097
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 19:48:23,141 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.92 sec
MapReduce Total cumulative CPU time: 1 seconds 920 msec
Ended Job = job_1385675857984_0097
Loading data to table default.q22_global_sales_opportunity
Table default.q22_global_sales_opportunity stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 168, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 4.71 sec   HDFS Read: 716849 HDFS Write: 313 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 1.92 sec   HDFS Read: 680 HDFS Write: 168 SUCCESS
Total MapReduce CPU Time Spent: 6 seconds 630 msec
OK
Time taken: 43.423 seconds
Time:113.82
