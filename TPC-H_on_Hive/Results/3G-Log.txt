Running Hive from /opt/hive-0.12.0
Running Hadoop from 
Running Hive query: tpch/q1_pricing_summary_report.hive
13/11/28 22:20:26 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:20:26 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:20:26 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:20:26 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:20:26 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:20:26 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:20:26 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 7.372 seconds
OK
Time taken: 0.222 seconds
OK
Time taken: 0.225 seconds
OK
Time taken: 0.067 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0098, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0098/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0098
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 3
2013-11-28 22:21:06,843 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 117.41 sec
MapReduce Total cumulative CPU time: 1 minutes 57 seconds 410 msec
Ended Job = job_1385675857984_0098
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0099, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0099/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0099
Hadoop job information for Stage-2: number of mappers: 3; number of reducers: 1
2013-11-28 22:21:24,732 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.5 sec
MapReduce Total cumulative CPU time: 3 seconds 500 msec
Ended Job = job_1385675857984_0099
Loading data to table default.q1_pricing_summary_report
Table default.q1_pricing_summary_report stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 581, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10  Reduce: 3   Cumulative CPU: 117.41 sec   HDFS Read: 2309130054 HDFS Write: 616 SUCCESS
Job 1: Map: 3  Reduce: 1   Cumulative CPU: 3.5 sec   HDFS Read: 1717 HDFS Write: 581 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 0 seconds 910 msec
OK
Time taken: 49.77 seconds
Time:60.05
Running Hive query: tpch/q2_minimum_cost_supplier.hive
13/11/28 22:21:26 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:21:26 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:21:26 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:21:26 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:21:26 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:21:26 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:21:26 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.711 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.151 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.172 seconds
OK
Time taken: 0.112 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.242 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 7
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 22:21:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 22:21:41 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-21-35_893_2598999680233051580-1/-local-10016/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 22:21:41 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-21-35_893_2598999680233051580-1/-local-10016/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 22:21:41 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:21:41 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:21:41 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:21:41 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:21:41 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:21:41 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:21:41 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 10:21:42	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 10:21:43	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-21-35_893_2598999680233051580-1/-local-10013/HashTable-Stage-15/MapJoin-mapfile41--.hashtable
2013-11-28 10:21:44	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-21-35_893_2598999680233051580-1/-local-10013/HashTable-Stage-15/MapJoin-mapfile41--.hashtable
2013-11-28 10:21:44	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-21-35_893_2598999680233051580-1/-local-10013/HashTable-Stage-15/MapJoin-mapfile51--.hashtable
2013-11-28 10:21:44	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-21-35_893_2598999680233051580-1/-local-10013/HashTable-Stage-15/MapJoin-mapfile51--.hashtable
2013-11-28 10:21:44	End of local task; Time Taken: 1.674 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 7
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0100, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0100/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0100
Hadoop job information for Stage-15: number of mappers: 1; number of reducers: 0
2013-11-28 22:21:57,643 Stage-15 map = 100%,  reduce = 0%, Cumulative CPU 2.17 sec
MapReduce Total cumulative CPU time: 2 seconds 170 msec
Ended Job = job_1385675857984_0100
Stage-19 is filtered out by condition resolver.
Stage-20 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0101, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0101/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0101
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 1
2013-11-28 22:22:26,353 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 29.74 sec
MapReduce Total cumulative CPU time: 29 seconds 740 msec
Ended Job = job_1385675857984_0101
Stage-17 is filtered out by condition resolver.
Stage-18 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0102, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0102/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0102
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
2013-11-28 22:22:52,713 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 16.46 sec
MapReduce Total cumulative CPU time: 16 seconds 460 msec
Ended Job = job_1385675857984_0102
Loading data to table default.q2_minimum_cost_supplier_tmp1
Table default.q2_minimum_cost_supplier_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 333386, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 2.17 sec   HDFS Read: 2424 HDFS Write: 975180 SUCCESS
Job 1: Map: 4  Reduce: 1   Cumulative CPU: 29.74 sec   HDFS Read: 360404592 HDFS Write: 82306564 SUCCESS
Job 2: Map: 2  Reduce: 1   Cumulative CPU: 16.46 sec   HDFS Read: 154934242 HDFS Write: 333386 SUCCESS
Total MapReduce CPU Time Spent: 48 seconds 370 msec
OK
Time taken: 77.381 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0103, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0103/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0103
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-28 22:23:10,971 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.37 sec
MapReduce Total cumulative CPU time: 2 seconds 370 msec
Ended Job = job_1385675857984_0103
Loading data to table default.q2_minimum_cost_supplier_tmp2
Table default.q2_minimum_cost_supplier_tmp2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 19290, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.37 sec   HDFS Read: 333622 HDFS Write: 19290 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 370 msec
OK
Time taken: 18.19 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 22:23:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 22:23:13 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-23-11_466_4696147738351807262-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 22:23:13 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-23-11_466_4696147738351807262-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 22:23:13 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:23:13 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:23:13 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:23:13 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:23:13 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:23:13 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:23:13 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 10:23:14	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 10:23:15	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-23-11_466_4696147738351807262-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile61--.hashtable
2013-11-28 10:23:15	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-23-11_466_4696147738351807262-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile61--.hashtable
2013-11-28 10:23:15	End of local task; Time Taken: 0.711 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0104, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0104/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0104
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-28 22:23:34,247 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.83 sec
MapReduce Total cumulative CPU time: 2 seconds 830 msec
Ended Job = job_1385675857984_0104
Loading data to table default.q2_minimum_cost_supplier
Table default.q2_minimum_cost_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16540, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.83 sec   HDFS Read: 333622 HDFS Write: 16540 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 830 msec
OK
Time taken: 23.265 seconds
Time:129.38
Running Hive query: tpch/q3_shipping_priority.hive
13/11/28 22:23:35 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:23:35 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:23:35 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:23:35 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:23:35 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:23:35 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:23:35 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.917 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.122 seconds
OK
Time taken: 0.226 seconds
OK
Time taken: 0.257 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.066 seconds
Total MapReduce jobs = 6
Stage-1 is selected by condition resolver.
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0105, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0105/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0105
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-28 22:24:18,024 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 34.5 sec
MapReduce Total cumulative CPU time: 34 seconds 500 msec
Ended Job = job_1385675857984_0105
Stage-14 is filtered out by condition resolver.
Stage-15 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0106, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0106/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0106
Hadoop job information for Stage-2: number of mappers: 11; number of reducers: 3
2013-11-28 22:24:52,736 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 131.94 sec
MapReduce Total cumulative CPU time: 2 minutes 11 seconds 940 msec
Ended Job = job_1385675857984_0106
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0107, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0107/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0107
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
2013-11-28 22:25:16,141 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 9.1 sec
MapReduce Total cumulative CPU time: 9 seconds 100 msec
Ended Job = job_1385675857984_0107
Launching Job 4 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0108, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0108/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0108
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 22:25:35,252 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.24 sec
MapReduce Total cumulative CPU time: 4 seconds 240 msec
Ended Job = job_1385675857984_0108
Loading data to table default.q3_shipping_priority
Table default.q3_shipping_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 369, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 34.5 sec   HDFS Read: 593821728 HDFS Write: 14715172 SUCCESS
Job 1: Map: 11  Reduce: 3   Cumulative CPU: 131.94 sec   HDFS Read: 2323845592 HDFS Write: 1418893 SUCCESS
Job 2: Map: 2  Reduce: 1   Cumulative CPU: 9.1 sec   HDFS Read: 1419846 HDFS Write: 1418761 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 4.24 sec   HDFS Read: 1419127 HDFS Write: 369 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 59 seconds 780 msec
OK
Time taken: 110.836 seconds
Time:121.02
Running Hive query: tpch/q4_order_priority.hive
13/11/28 22:25:37 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:25:37 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:25:37 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:25:37 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:25:37 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:25:37 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:25:37 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.87 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.213 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.213 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.217 seconds
OK
Time taken: 0.056 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0109, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0109/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0109
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 3
2013-11-28 22:26:15,220 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 97.48 sec
MapReduce Total cumulative CPU time: 1 minutes 37 seconds 480 msec
Ended Job = job_1385675857984_0109
Loading data to table default.q4_order_priority_tmp
Table default.q4_order_priority_tmp stats: [num_partitions: 0, num_files: 3, num_rows: 0, total_size: 34586594, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10  Reduce: 3   Cumulative CPU: 97.48 sec   HDFS Read: 2309130054 HDFS Write: 34586594 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 37 seconds 480 msec
OK
Time taken: 29.863 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0110, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0110/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0110
Hadoop job information for Stage-1: number of mappers: 5; number of reducers: 1
2013-11-28 22:26:50,064 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 42.8 sec
MapReduce Total cumulative CPU time: 42 seconds 800 msec
Ended Job = job_1385675857984_0110
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0111, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0111/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0111
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-28 22:27:06,580 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 1.35 sec
MapReduce Total cumulative CPU time: 1 seconds 350 msec
Ended Job = job_1385675857984_0111
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0112, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0112/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0112
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 22:27:24,605 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.82 sec
MapReduce Total cumulative CPU time: 1 seconds 820 msec
Ended Job = job_1385675857984_0112
Loading data to table default.q4_order_priority
Table default.q4_order_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 77, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 5  Reduce: 1   Cumulative CPU: 42.8 sec   HDFS Read: 555174356 HDFS Write: 243 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 1.35 sec   HDFS Read: 610 HDFS Write: 243 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.82 sec   HDFS Read: 610 HDFS Write: 77 SUCCESS
Total MapReduce CPU Time Spent: 45 seconds 970 msec
OK
Time taken: 69.272 seconds
Time:109.34
Running Hive query: tpch/q5_local_supplier_volume.hive
13/11/28 22:27:26 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:27:26 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:27:26 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:27:26 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:27:26 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:27:26 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:27:26 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.828 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.119 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.192 seconds
OK
Time taken: 0.225 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 12
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 22:27:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 22:27:42 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-27-35_546_6164010866352148118-1/-local-10023/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 22:27:42 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-27-35_546_6164010866352148118-1/-local-10023/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 22:27:42 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:27:42 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:27:42 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:27:42 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:27:42 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:27:42 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:27:42 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 10:27:43	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 10:27:44	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-27-35_546_6164010866352148118-1/-local-10020/HashTable-Stage-22/MapJoin-mapfile60--.hashtable
2013-11-28 10:27:44	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-27-35_546_6164010866352148118-1/-local-10020/HashTable-Stage-22/MapJoin-mapfile60--.hashtable
2013-11-28 10:27:44	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-27-35_546_6164010866352148118-1/-local-10020/HashTable-Stage-22/MapJoin-mapfile71--.hashtable
2013-11-28 10:27:44	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-27-35_546_6164010866352148118-1/-local-10020/HashTable-Stage-22/MapJoin-mapfile71--.hashtable
2013-11-28 10:27:44	End of local task; Time Taken: 1.464 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 12
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0113, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0113/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0113
Hadoop job information for Stage-22: number of mappers: 1; number of reducers: 0
2013-11-28 22:27:58,192 Stage-22 map = 100%,  reduce = 0%, Cumulative CPU 1.6 sec
MapReduce Total cumulative CPU time: 1 seconds 600 msec
Ended Job = job_1385675857984_0113
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 12
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0114, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0114/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0114
Hadoop job information for Stage-8: number of mappers: 11; number of reducers: 3
2013-11-28 22:28:45,235 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 182.07 sec
MapReduce Total cumulative CPU time: 3 minutes 2 seconds 70 msec
Ended Job = job_1385675857984_0114
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 12
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0115, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0115/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0115
Hadoop job information for Stage-1: number of mappers: 5; number of reducers: 1
2013-11-28 22:29:27,402 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 58.65 sec
MapReduce Total cumulative CPU time: 58 seconds 650 msec
Ended Job = job_1385675857984_0115
Stage-24 is filtered out by condition resolver.
Stage-25 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 12
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0116, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0116/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0116
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-28 22:29:53,457 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 15.72 sec
MapReduce Total cumulative CPU time: 15 seconds 720 msec
Ended Job = job_1385675857984_0116
Launching Job 5 out of 12
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0117, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0117/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0117
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 22:30:10,765 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.38 sec
MapReduce Total cumulative CPU time: 1 seconds 380 msec
Ended Job = job_1385675857984_0117
Launching Job 6 out of 12
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0118, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0118/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0118
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 22:30:33,020 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.42 sec
MapReduce Total cumulative CPU time: 1 seconds 420 msec
Ended Job = job_1385675857984_0118
Loading data to table default.q5_local_supplier_volume
Table default.q5_local_supplier_volume stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 139, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.6 sec   HDFS Read: 2424 HDFS Write: 172115 SUCCESS
Job 1: Map: 11  Reduce: 3   Cumulative CPU: 182.07 sec   HDFS Read: 2309302536 HDFS Write: 165626400 SUCCESS
Job 2: Map: 5  Reduce: 1   Cumulative CPU: 58.65 sec   HDFS Read: 686214571 HDFS Write: 25063844 SUCCESS
Job 3: Map: 2  Reduce: 1   Cumulative CPU: 15.72 sec   HDFS Read: 98298724 HDFS Write: 257 SUCCESS
Job 4: Map: 1  Reduce: 1   Cumulative CPU: 1.38 sec   HDFS Read: 624 HDFS Write: 257 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 1.42 sec   HDFS Read: 624 HDFS Write: 139 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 20 seconds 840 msec
OK
Time taken: 177.989 seconds
Time:188.42
Running Hive query: tpch/q6_forecast_revenue_change.hive
13/11/28 22:30:34 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:30:34 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:30:34 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:30:34 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:30:34 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:30:34 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:30:34 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.822 seconds
OK
Time taken: 0.179 seconds
OK
Time taken: 0.236 seconds
OK
Time taken: 0.057 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0119, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0119/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0119
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 1
2013-11-28 22:31:08,253 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 69.21 sec
MapReduce Total cumulative CPU time: 1 minutes 9 seconds 210 msec
Ended Job = job_1385675857984_0119
Loading data to table default.q6_forecast_revenue_change
Table default.q6_forecast_revenue_change stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 20, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10  Reduce: 1   Cumulative CPU: 69.21 sec   HDFS Read: 2309130054 HDFS Write: 20 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 9 seconds 210 msec
OK
Time taken: 25.643 seconds
Time:35.32
Running Hive query: tpch/q7_volume_shipping.hive
13/11/28 22:31:10 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:31:10 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:31:10 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:31:10 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:31:10 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:31:10 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:31:10 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.685 seconds
OK
Time taken: 0.111 seconds
OK
Time taken: 0.119 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.179 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.246 seconds
OK
Time taken: 0.052 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.035 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 3
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 22:31:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 22:31:22 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-31-19_250_9185504113346842600-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 22:31:22 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-31-19_250_9185504113346842600-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 22:31:22 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:31:22 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:31:22 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:31:22 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:31:22 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:31:22 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:31:22 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 10:31:23	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 10:31:24	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-31-19_250_9185504113346842600-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-11-28 10:31:24	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-31-19_250_9185504113346842600-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-11-28 10:31:24	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-31-19_250_9185504113346842600-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-11-28 10:31:24	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-31-19_250_9185504113346842600-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-11-28 10:31:24	End of local task; Time Taken: 0.988 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0120, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0120/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0120
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 0
2013-11-28 22:31:37,260 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.43 sec
MapReduce Total cumulative CPU time: 1 seconds 430 msec
Ended Job = job_1385675857984_0120
Stage-5 is selected by condition resolver.
Stage-4 is filtered out by condition resolver.
Stage-6 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-11-28_22-31-19_250_9185504113346842600-1/-ext-10000
Loading data to table default.q7_volume_shipping_tmp
Table default.q7_volume_shipping_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 38, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.43 sec   HDFS Read: 2424 HDFS Write: 38 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 430 msec
OK
Time taken: 18.541 seconds
Total MapReduce jobs = 6
Stage-6 is selected by condition resolver.
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0121, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0121/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0121
Hadoop job information for Stage-6: number of mappers: 13; number of reducers: 3
2013-11-28 22:32:19,759 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 158.43 sec
MapReduce Total cumulative CPU time: 2 minutes 38 seconds 430 msec
Ended Job = job_1385675857984_0121
Stage-20 is filtered out by condition resolver.
Stage-21 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0122, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0122/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0122
Hadoop job information for Stage-7: number of mappers: 4; number of reducers: 1
2013-11-28 22:33:11,801 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 71.58 sec
MapReduce Total cumulative CPU time: 1 minutes 11 seconds 580 msec
Ended Job = job_1385675857984_0122
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 22:33:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 22:33:14 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-31-37_794_3226487132138943944-1/-local-10018/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 22:33:14 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-31-37_794_3226487132138943944-1/-local-10018/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 22:33:14 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:33:14 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:33:14 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:33:14 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:33:14 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:33:14 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:33:14 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 10:33:15	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 10:33:16	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-31-37_794_3226487132138943944-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile30--.hashtable
2013-11-28 10:33:16	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-31-37_794_3226487132138943944-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile30--.hashtable
2013-11-28 10:33:16	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-31-37_794_3226487132138943944-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-11-28 10:33:16	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-31-37_794_3226487132138943944-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-11-28 10:33:16	End of local task; Time Taken: 1.25 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0123, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0123/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0123
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
2013-11-28 22:33:51,056 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 30.41 sec
MapReduce Total cumulative CPU time: 30 seconds 410 msec
Ended Job = job_1385675857984_0123
Launching Job 4 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0124, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0124/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0124
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 22:34:08,650 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.42 sec
MapReduce Total cumulative CPU time: 1 seconds 420 msec
Ended Job = job_1385675857984_0124
Loading data to table default.q7_volume_shipping
Table default.q7_volume_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 163, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 13  Reduce: 3   Cumulative CPU: 158.43 sec   HDFS Read: 2829717269 HDFS Write: 280533795 SUCCESS
Job 1: Map: 4  Reduce: 1   Cumulative CPU: 71.58 sec   HDFS Read: 353769409 HDFS Write: 264813603 SUCCESS
Job 2: Map: 2  Reduce: 1   Cumulative CPU: 30.41 sec   HDFS Read: 264816141 HDFS Write: 268 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 1.42 sec   HDFS Read: 635 HDFS Write: 163 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 21 seconds 840 msec
OK
Time taken: 151.354 seconds
Time:180.28
Running Hive query: tpch/q8_national_market_share.hive
13/11/28 22:34:10 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:34:10 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:34:10 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:34:10 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:34:10 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:34:10 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:34:10 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.845 seconds
OK
Time taken: 0.119 seconds
OK
Time taken: 0.132 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.098 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.201 seconds
OK
Time taken: 0.209 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.044 seconds
OK
Time taken: 0.047 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 22:34:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 22:34:28 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-34-19_812_7921501789438080697-1/-local-10031/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 22:34:28 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-34-19_812_7921501789438080697-1/-local-10031/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 22:34:28 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:34:28 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:34:28 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:34:28 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:34:28 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:34:28 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:34:28 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 10:34:29	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 10:34:30	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-34-19_812_7921501789438080697-1/-local-10028/HashTable-Stage-30/MapJoin-mapfile101--.hashtable
2013-11-28 10:34:30	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-34-19_812_7921501789438080697-1/-local-10028/HashTable-Stage-30/MapJoin-mapfile101--.hashtable
2013-11-28 10:34:30	End of local task; Time Taken: 0.95 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0125, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0125/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0125
Hadoop job information for Stage-30: number of mappers: 1; number of reducers: 0
2013-11-28 22:34:47,881 Stage-30 map = 100%,  reduce = 0%, Cumulative CPU 1.13 sec
MapReduce Total cumulative CPU time: 1 seconds 130 msec
Ended Job = job_1385675857984_0125
Stage-38 is filtered out by condition resolver.
Stage-39 is filtered out by condition resolver.
Stage-9 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0126, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0126/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0126
Hadoop job information for Stage-9: number of mappers: 2; number of reducers: 1
2013-11-28 22:35:08,845 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 8.77 sec
MapReduce Total cumulative CPU time: 8 seconds 770 msec
Ended Job = job_1385675857984_0126
Stage-36 is filtered out by condition resolver.
Stage-37 is filtered out by condition resolver.
Stage-10 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0127, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0127/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0127
Hadoop job information for Stage-10: number of mappers: 4; number of reducers: 1
2013-11-28 22:46:44,561 Stage-10 map = 100%,  reduce = 100%, Cumulative CPU 28.98 sec
MapReduce Total cumulative CPU time: 28 seconds 980 msec
Ended Job = job_1385675857984_0127
Stage-34 is filtered out by condition resolver.
Stage-35 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0128, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0128/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0128
Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 3
2013-11-28 22:47:37,572 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 198.36 sec
MapReduce Total cumulative CPU time: 3 minutes 18 seconds 360 msec
Ended Job = job_1385675857984_0128
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0129, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0129/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0129
Hadoop job information for Stage-2: number of mappers: 3; number of reducers: 1
2013-11-28 22:48:06,778 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 22.89 sec
MapReduce Total cumulative CPU time: 22 seconds 890 msec
Ended Job = job_1385675857984_0129
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 22:48:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 22:48:09 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-34-19_812_7921501789438080697-1/-local-10043/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 22:48:09 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-34-19_812_7921501789438080697-1/-local-10043/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 22:48:09 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:48:09 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:48:09 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:48:09 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:48:09 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:48:09 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:48:09 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 10:48:10	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 10:48:10	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-34-19_812_7921501789438080697-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-11-28 10:48:10	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-34-19_812_7921501789438080697-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-11-28 10:48:11	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-34-19_812_7921501789438080697-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile10--.hashtable
2013-11-28 10:48:11	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-34-19_812_7921501789438080697-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile10--.hashtable
2013-11-28 10:48:11	End of local task; Time Taken: 1.268 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0130, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0130/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0130
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-28 22:48:31,384 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 4.86 sec
MapReduce Total cumulative CPU time: 4 seconds 860 msec
Ended Job = job_1385675857984_0130
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0131, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0131/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0131
Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 1
2013-11-28 22:48:48,964 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 1.51 sec
MapReduce Total cumulative CPU time: 1 seconds 510 msec
Ended Job = job_1385675857984_0131
Loading data to table default.q8_national_market_share
Table default.q8_national_market_share stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 50, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.13 sec   HDFS Read: 2424 HDFS Write: 186 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 8.77 sec   HDFS Read: 73235066 HDFS Write: 1889200 SUCCESS
Job 2: Map: 4  Reduce: 1   Cumulative CPU: 28.98 sec   HDFS Read: 522476782 HDFS Write: 8768843 SUCCESS
Job 3: Map: 11  Reduce: 3   Cumulative CPU: 198.36 sec   HDFS Read: 2317899264 HDFS Write: 55652107 SUCCESS
Job 4: Map: 3  Reduce: 1   Cumulative CPU: 22.89 sec   HDFS Read: 128280374 HDFS Write: 362354 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 4.86 sec   HDFS Read: 362721 HDFS Write: 152 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.51 sec   HDFS Read: 519 HDFS Write: 50 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 26 seconds 500 msec
OK
Time taken: 869.675 seconds
Time:880.32
Running Hive query: tpch/q9_product_type_profit.hive
13/11/28 22:48:50 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:48:50 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:48:50 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:48:50 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:48:50 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:48:50 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:48:50 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.876 seconds
OK
Time taken: 0.111 seconds
OK
Time taken: 0.118 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.183 seconds
OK
Time taken: 0.225 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.035 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.059 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 22:49:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 22:49:07 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-49-00_001_2075003171151422534-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 22:49:07 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-49-00_001_2075003171151422534-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 22:49:07 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:49:07 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:49:07 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:49:07 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:49:07 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:49:07 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:49:07 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 10:49:08	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 10:49:09	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-49-00_001_2075003171151422534-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-11-28 10:49:09	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-49-00_001_2075003171151422534-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-11-28 10:49:09	End of local task; Time Taken: 0.62 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0132, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0132/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0132
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-11-28 22:49:22,271 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 2.12 sec
MapReduce Total cumulative CPU time: 2 seconds 120 msec
Ended Job = job_1385675857984_0132
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0133, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0133/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0133
Hadoop job information for Stage-8: number of mappers: 11; number of reducers: 3
2013-11-28 22:50:37,020 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 244.42 sec
MapReduce Total cumulative CPU time: 4 minutes 4 seconds 420 msec
Ended Job = job_1385675857984_0133
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0134, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0134/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0134
Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 2
2013-11-28 22:52:38,472 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 255.39 sec
MapReduce Total cumulative CPU time: 4 minutes 15 seconds 390 msec
Ended Job = job_1385675857984_0134
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0135, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0135/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0135
Hadoop job information for Stage-2: number of mappers: 7; number of reducers: 2
2013-11-28 22:54:12,487 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 207.78 sec
MapReduce Total cumulative CPU time: 3 minutes 27 seconds 780 msec
Ended Job = job_1385675857984_0135
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0136, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0136/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0136
Hadoop job information for Stage-3: number of mappers: 5; number of reducers: 1
2013-11-28 22:54:54,342 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 53.21 sec
MapReduce Total cumulative CPU time: 53 seconds 210 msec
Ended Job = job_1385675857984_0136
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0137, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0137/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0137
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 22:55:11,844 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.47 sec
MapReduce Total cumulative CPU time: 1 seconds 470 msec
Ended Job = job_1385675857984_0137
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0138, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0138/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0138
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-28 22:55:35,113 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 1.5 sec
MapReduce Total cumulative CPU time: 1 seconds 500 msec
Ended Job = job_1385675857984_0138
Loading data to table default.q9_product_type_profit
Table default.q9_product_type_profit stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 5865, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 2.12 sec   HDFS Read: 4249168 HDFS Write: 850579 SUCCESS
Job 1: Map: 11  Reduce: 3   Cumulative CPU: 244.42 sec   HDFS Read: 2309981000 HDFS Write: 1090887555 SUCCESS
Job 2: Map: 8  Reduce: 2   Cumulative CPU: 255.39 sec   HDFS Read: 1450332695 HDFS Write: 1181964755 SUCCESS
Job 3: Map: 7  Reduce: 2   Cumulative CPU: 207.78 sec   HDFS Read: 1254613997 HDFS Write: 60529986 SUCCESS
Job 4: Map: 5  Reduce: 1   Cumulative CPU: 53.21 sec   HDFS Read: 581117935 HDFS Write: 6470 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 1.47 sec   HDFS Read: 6837 HDFS Write: 6470 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.5 sec   HDFS Read: 6837 HDFS Write: 5865 SUCCESS
Total MapReduce CPU Time Spent: 12 minutes 45 seconds 890 msec
OK
Time taken: 395.62 seconds
Time:406.12
Running Hive query: tpch/q10_returned_item.hive
13/11/28 22:55:36 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:55:36 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:55:36 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:55:36 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:55:36 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:55:36 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:55:36 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.701 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.126 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.188 seconds
OK
Time taken: 0.214 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.075 seconds
Total MapReduce jobs = 7
Stage-1 is selected by condition resolver.
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0139, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0139/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0139
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-28 22:56:19,569 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 28.68 sec
MapReduce Total cumulative CPU time: 28 seconds 680 msec
Ended Job = job_1385675857984_0139
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 22:56:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 22:56:21 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-55-45_714_766316732439959112-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 22:56:21 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-55-45_714_766316732439959112-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 22:56:22 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:56:22 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:56:22 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:56:22 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:56:22 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:56:22 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:56:22 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 10:56:22	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 10:56:23	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-55-45_714_766316732439959112-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-11-28 10:56:23	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-55-45_714_766316732439959112-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-11-28 10:56:23	End of local task; Time Taken: 0.623 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 7
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0140, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0140/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0140
Hadoop job information for Stage-13: number of mappers: 1; number of reducers: 0
2013-11-28 22:56:37,811 Stage-13 map = 100%,  reduce = 0%, Cumulative CPU 4.48 sec
MapReduce Total cumulative CPU time: 4 seconds 480 msec
Ended Job = job_1385675857984_0140
Stage-17 is filtered out by condition resolver.
Stage-18 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0141, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0141/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0141
Hadoop job information for Stage-3: number of mappers: 11; number of reducers: 3
2013-11-28 22:57:07,842 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 102.68 sec
MapReduce Total cumulative CPU time: 1 minutes 42 seconds 680 msec
Ended Job = job_1385675857984_0141
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0142, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0142/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0142
Hadoop job information for Stage-4: number of mappers: 2; number of reducers: 1
2013-11-28 22:57:31,219 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 10.61 sec
MapReduce Total cumulative CPU time: 10 seconds 610 msec
Ended Job = job_1385675857984_0142
Launching Job 5 out of 7
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0143, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0143/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0143
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-28 22:57:51,287 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 5.54 sec
MapReduce Total cumulative CPU time: 5 seconds 540 msec
Ended Job = job_1385675857984_0143
Loading data to table default.q10_returned_item
Table default.q10_returned_item stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3588, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 28.68 sec   HDFS Read: 593821728 HDFS Write: 29250181 SUCCESS
Job 1: Map: 1   Cumulative CPU: 4.48 sec   HDFS Read: 29250547 HDFS Write: 30480684 SUCCESS
Job 2: Map: 11  Reduce: 3   Cumulative CPU: 102.68 sec   HDFS Read: 2339611104 HDFS Write: 24462983 SUCCESS
Job 3: Map: 2  Reduce: 1   Cumulative CPU: 10.61 sec   HDFS Read: 24463936 HDFS Write: 20700374 SUCCESS
Job 4: Map: 1  Reduce: 1   Cumulative CPU: 5.54 sec   HDFS Read: 20700740 HDFS Write: 3588 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 31 seconds 990 msec
OK
Time taken: 126.078 seconds
Time:136.17
Running Hive query: tpch/q11_important_stock.hive
13/11/28 22:57:53 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:57:53 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:57:53 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:57:53 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:57:53 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:57:53 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:57:53 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.646 seconds
OK
Time taken: 0.103 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.178 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.112 seconds
OK
Time taken: 0.215 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.06 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.082 seconds
Total MapReduce jobs = 5
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 22:58:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 22:58:06 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-58-01_997_5230509011584360254-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 22:58:06 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-58-01_997_5230509011584360254-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 22:58:06 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:58:06 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:58:06 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:58:06 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:58:06 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:58:06 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:58:06 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 10:58:07	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 10:58:08	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-58-01_997_5230509011584360254-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-11-28 10:58:08	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-58-01_997_5230509011584360254-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-11-28 10:58:08	End of local task; Time Taken: 0.936 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0144, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0144/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0144
Hadoop job information for Stage-10: number of mappers: 1; number of reducers: 0
2013-11-28 22:58:21,018 Stage-10 map = 100%,  reduce = 0%, Cumulative CPU 1.63 sec
MapReduce Total cumulative CPU time: 1 seconds 630 msec
Ended Job = job_1385675857984_0144
Stage-11 is filtered out by condition resolver.
Stage-12 is selected by condition resolver.
Stage-2 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 22:58:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 22:58:23 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-58-01_997_5230509011584360254-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 22:58:23 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-58-01_997_5230509011584360254-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 22:58:23 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:58:23 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:58:23 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:58:23 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:58:23 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:58:23 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:58:23 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 10:58:24	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 10:58:25	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-58-01_997_5230509011584360254-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile10--.hashtable
2013-11-28 10:58:25	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-58-01_997_5230509011584360254-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile10--.hashtable
2013-11-28 10:58:25	End of local task; Time Taken: 0.769 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 3 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0145, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0145/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0145
Hadoop job information for Stage-8: number of mappers: 2; number of reducers: 0
2013-11-28 22:58:44,421 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 11.74 sec
MapReduce Total cumulative CPU time: 11 seconds 740 msec
Ended Job = job_1385675857984_0145
Launching Job 4 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0146, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0146/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0146
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 22:59:04,969 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 6.26 sec
MapReduce Total cumulative CPU time: 6 seconds 260 msec
Ended Job = job_1385675857984_0146
Loading data to table default.q11_part_tmp
Table default.q11_part_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 1739530, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.63 sec   HDFS Read: 4249168 HDFS Write: 24216 SUCCESS
Job 1: Map: 2   Cumulative CPU: 11.74 sec   HDFS Read: 359428908 HDFS Write: 2627475 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 6.26 sec   HDFS Read: 2628064 HDFS Write: 1739530 SUCCESS
Total MapReduce CPU Time Spent: 19 seconds 630 msec
OK
Time taken: 63.458 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0147, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0147/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0147
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-28 22:59:23,802 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.73 sec
MapReduce Total cumulative CPU time: 2 seconds 730 msec
Ended Job = job_1385675857984_0147
Loading data to table default.q11_sum_tmp
Table default.q11_sum_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.73 sec   HDFS Read: 1739749 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 730 msec
OK
Time taken: 18.849 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 22:59:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 22:59:26 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-59-24_306_581384600646876555-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 22:59:26 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_22-59-24_306_581384600646876555-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 22:59:26 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:59:26 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:59:26 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:59:26 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:59:26 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:59:26 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:59:26 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 10:59:27	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 10:59:28	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_22-59-24_306_581384600646876555-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-11-28 10:59:28	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_22-59-24_306_581384600646876555-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-11-28 10:59:28	End of local task; Time Taken: 0.624 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0148, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0148/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0148
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-28 22:59:48,497 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.65 sec
MapReduce Total cumulative CPU time: 3 seconds 650 msec
Ended Job = job_1385675857984_0148
Loading data to table default.q11_important_stock
Table default.q11_important_stock stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 0, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.65 sec   HDFS Read: 1739749 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 650 msec
OK
Time taken: 24.757 seconds
Time:117.26
Running Hive query: tpch/q12_shipping.hive
13/11/28 22:59:50 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 22:59:50 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 22:59:50 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 22:59:50 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 22:59:50 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 22:59:50 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 22:59:50 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.744 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.175 seconds
OK
Time taken: 0.237 seconds
OK
Time taken: 0.038 seconds
OK
Time taken: 0.051 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0149, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0149/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0149
Hadoop job information for Stage-1: number of mappers: 13; number of reducers: 3
2013-11-28 23:00:34,931 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 112.83 sec
MapReduce Total cumulative CPU time: 1 minutes 52 seconds 830 msec
Ended Job = job_1385675857984_0149
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0150, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0150/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0150
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-28 23:00:54,133 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.07 sec
MapReduce Total cumulative CPU time: 2 seconds 70 msec
Ended Job = job_1385675857984_0150
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0151, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0151/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0151
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 23:01:11,698 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.83 sec
MapReduce Total cumulative CPU time: 1 seconds 830 msec
Ended Job = job_1385675857984_0151
Loading data to table default.q12_shipping
Table default.q12_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 42, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 13  Reduce: 3   Cumulative CPU: 112.83 sec   HDFS Read: 2829717269 HDFS Write: 456 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 2.07 sec   HDFS Read: 1412 HDFS Write: 152 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.83 sec   HDFS Read: 519 HDFS Write: 42 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 56 seconds 730 msec
OK
Time taken: 73.404 seconds
Time:83.16
Running Hive query: tpch/q13_customer_distribution.hive
13/11/28 23:01:13 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:01:13 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:01:13 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:01:13 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:01:13 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:01:13 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:01:13 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.806 seconds
OK
Time taken: 0.103 seconds
OK
Time taken: 0.201 seconds
OK
Time taken: 0.232 seconds
OK
Time taken: 0.052 seconds
OK
Time taken: 0.064 seconds
Total MapReduce jobs = 4
Stage-1 is selected by condition resolver.
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0152, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0152/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0152
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-28 23:02:07,414 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 59.36 sec
MapReduce Total cumulative CPU time: 59 seconds 360 msec
Ended Job = job_1385675857984_0152
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0153, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0153/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0153
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-28 23:02:29,395 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.07 sec
MapReduce Total cumulative CPU time: 8 seconds 70 msec
Ended Job = job_1385675857984_0153
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0154, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0154/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0154
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 23:02:46,364 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.44 sec
MapReduce Total cumulative CPU time: 1 seconds 440 msec
Ended Job = job_1385675857984_0154
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0155, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0155/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0155
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 23:03:04,707 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.92 sec
MapReduce Total cumulative CPU time: 1 seconds 920 msec
Ended Job = job_1385675857984_0155
Loading data to table default.q13_customer_distribution
Table default.q13_customer_distribution stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 325, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 59.36 sec   HDFS Read: 593821728 HDFS Write: 9932319 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 8.07 sec   HDFS Read: 9932686 HDFS Write: 981 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.44 sec   HDFS Read: 1348 HDFS Write: 981 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 1.92 sec   HDFS Read: 1348 HDFS Write: 325 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 10 seconds 790 msec
OK
Time taken: 103.108 seconds
Time:113.01
Running Hive query: tpch/q14_promotion_effect.hive
13/11/28 23:03:06 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:03:06 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:03:06 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:03:06 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:03:06 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:03:06 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:03:06 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.832 seconds
OK
Time taken: 0.11 seconds
OK
Time taken: 0.178 seconds
OK
Time taken: 0.206 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.09 seconds
Total MapReduce jobs = 2
Stage-1 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0156, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0156/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0156
Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 3
2013-11-28 23:03:42,586 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 81.06 sec
MapReduce Total cumulative CPU time: 1 minutes 21 seconds 60 msec
Ended Job = job_1385675857984_0156
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0157, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0157/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0157
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-28 23:04:02,121 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.58 sec
MapReduce Total cumulative CPU time: 2 seconds 580 msec
Ended Job = job_1385675857984_0157
Loading data to table default.q14_promotion_effect
Table default.q14_promotion_effect stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 18, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 11  Reduce: 3   Cumulative CPU: 81.06 sec   HDFS Read: 2381757365 HDFS Write: 387 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 2.58 sec   HDFS Read: 1343 HDFS Write: 18 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 23 seconds 640 msec
OK
Time taken: 47.524 seconds
Time:57.36
Running Hive query: tpch/q15_top_supplier.hive
13/11/28 23:04:03 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:04:03 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:04:03 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:04:03 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:04:03 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:04:03 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:04:03 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.871 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.168 seconds
OK
Time taken: 0.098 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.228 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0158, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0158/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0158
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 3
2013-11-28 23:04:38,870 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 78.2 sec
MapReduce Total cumulative CPU time: 1 minutes 18 seconds 200 msec
Ended Job = job_1385675857984_0158
Loading data to table default.revenue
Table default.revenue stats: [num_partitions: 0, num_files: 3, num_rows: 0, total_size: 612397, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10  Reduce: 3   Cumulative CPU: 78.2 sec   HDFS Read: 2309130054 HDFS Write: 612397 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 18 seconds 200 msec
OK
Time taken: 26.721 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0159, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0159/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0159
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-11-28 23:04:57,253 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.93 sec
MapReduce Total cumulative CPU time: 4 seconds 930 msec
Ended Job = job_1385675857984_0159
Loading data to table default.max_revenue
Table default.max_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 13, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 3  Reduce: 1   Cumulative CPU: 4.93 sec   HDFS Read: 613039 HDFS Write: 13 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 930 msec
OK
Time taken: 18.282 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 23:05:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 23:05:00 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-04-57_757_11209470393720699-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 23:05:00 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-04-57_757_11209470393720699-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 23:05:00 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:05:00 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:05:00 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:05:00 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:05:00 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:05:00 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:05:00 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 11:05:01	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 11:05:02	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-04-57_757_11209470393720699-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile11--.hashtable
2013-11-28 11:05:02	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-04-57_757_11209470393720699-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile11--.hashtable
2013-11-28 11:05:02	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-04-57_757_11209470393720699-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-11-28 11:05:02	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-04-57_757_11209470393720699-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-11-28 11:05:02	End of local task; Time Taken: 1.416 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0160, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0160/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0160
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 23:05:22,083 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 3.45 sec
MapReduce Total cumulative CPU time: 3 seconds 450 msec
Ended Job = job_1385675857984_0160
Loading data to table default.q15_top_supplier
Table default.q15_top_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 88, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.45 sec   HDFS Read: 4249168 HDFS Write: 88 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 450 msec
OK
Time taken: 24.825 seconds
Time:79.96
Running Hive query: tpch/q16_parts_supplier_relationship.hive
13/11/28 23:05:23 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:05:23 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:05:23 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:05:23 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:05:23 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:05:23 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:05:23 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.838 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.161 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.221 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.035 seconds
OK
Time taken: 0.056 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.091 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0161, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0161/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0161
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2013-11-28 23:05:46,649 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.72 sec
MapReduce Total cumulative CPU time: 2 seconds 720 msec
Ended Job = job_1385675857984_0161
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-11-28_23-05-32_847_2595761534122472452-1/-ext-10000
Loading data to table default.supplier_tmp
Table default.supplier_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 168809, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 2.72 sec   HDFS Read: 4249168 HDFS Write: 168809 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 720 msec
OK
Time taken: 14.492 seconds
Total MapReduce jobs = 2
Stage-3 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0162, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0162/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0162
Hadoop job information for Stage-3: number of mappers: 3; number of reducers: 1
2013-11-28 23:06:18,196 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 30.03 sec
MapReduce Total cumulative CPU time: 30 seconds 30 msec
Ended Job = job_1385675857984_0162
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 23:06:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 23:06:20 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-05-47_341_1089664867815559732-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 23:06:20 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-05-47_341_1089664867815559732-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 23:06:20 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:06:20 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:06:20 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:06:20 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:06:20 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:06:20 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:06:20 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 11:06:21	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 11:06:22	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-05-47_341_1089664867815559732-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-28 11:06:22	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-05-47_341_1089664867815559732-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-28 11:06:22	End of local task; Time Taken: 0.994 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0163, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0163/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0163
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 0
2013-11-28 23:06:46,084 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 14.19 sec
MapReduce Total cumulative CPU time: 14 seconds 190 msec
Ended Job = job_1385675857984_0163
Loading data to table default.q16_tmp
Table default.q16_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 86930646, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 3  Reduce: 1   Cumulative CPU: 30.03 sec   HDFS Read: 432056219 HDFS Write: 116044887 SUCCESS
Job 1: Map: 1   Cumulative CPU: 14.19 sec   HDFS Read: 116045254 HDFS Write: 86930646 SUCCESS
Total MapReduce CPU Time Spent: 44 seconds 220 msec
OK
Time taken: 59.28 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0164, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0164/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0164
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-28 23:07:13,624 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 13.34 sec
MapReduce Total cumulative CPU time: 13 seconds 340 msec
Ended Job = job_1385675857984_0164
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0165, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0165/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0165
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-28 23:07:32,933 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.7 sec
MapReduce Total cumulative CPU time: 5 seconds 700 msec
Ended Job = job_1385675857984_0165
Loading data to table default.q16_parts_supplier_relationship
Table default.q16_parts_supplier_relationship stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 963959, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 13.34 sec   HDFS Read: 86930860 HDFS Write: 1341895 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 5.7 sec   HDFS Read: 1342262 HDFS Write: 963959 SUCCESS
Total MapReduce CPU Time Spent: 19 seconds 40 msec
OK
Time taken: 46.79 seconds
Time:130.81
Running Hive query: tpch/q17_small_quantity_order_revenue.hive
13/11/28 23:07:34 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:07:34 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:07:34 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:07:34 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:07:34 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:07:34 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:07:34 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.738 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.171 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0166, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0166/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0166
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 3
2013-11-28 23:08:28,140 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 180.86 sec
MapReduce Total cumulative CPU time: 3 minutes 0 seconds 860 msec
Ended Job = job_1385675857984_0166
Loading data to table default.lineitem_tmp
Table default.lineitem_tmp stats: [num_partitions: 0, num_files: 3, num_rows: 0, total_size: 13844302, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10  Reduce: 3   Cumulative CPU: 180.86 sec   HDFS Read: 2309130054 HDFS Write: 13844302 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 0 seconds 860 msec
OK
Time taken: 45.433 seconds
Total MapReduce jobs = 5
Stage-1 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0167, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0167/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0167
Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 3
2013-11-28 23:09:16,345 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 181.91 sec
MapReduce Total cumulative CPU time: 3 minutes 1 seconds 910 msec
Ended Job = job_1385675857984_0167
Stage-13 is filtered out by condition resolver.
Stage-14 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0168, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0168/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0168
Hadoop job information for Stage-2: number of mappers: 3; number of reducers: 1
2013-11-28 23:09:39,235 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 14.18 sec
MapReduce Total cumulative CPU time: 14 seconds 180 msec
Ended Job = job_1385675857984_0168
Launching Job 3 out of 5
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0169, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0169/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0169
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 23:09:57,348 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.86 sec
MapReduce Total cumulative CPU time: 1 seconds 860 msec
Ended Job = job_1385675857984_0169
Loading data to table default.q17_small_quantity_order_revenue
Table default.q17_small_quantity_order_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 18, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 11  Reduce: 3   Cumulative CPU: 181.91 sec   HDFS Read: 2381757365 HDFS Write: 669989 SUCCESS
Job 1: Map: 3  Reduce: 1   Cumulative CPU: 14.18 sec   HDFS Read: 14515630 HDFS Write: 121 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.86 sec   HDFS Read: 488 HDFS Write: 18 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 17 seconds 950 msec
OK
Time taken: 89.097 seconds
Time:144.41
Running Hive query: tpch/q18_large_volume_customer.hive
13/11/28 23:09:59 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:09:59 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:09:59 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:09:59 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:09:59 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:09:59 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:09:59 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.823 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.167 seconds
OK
Time taken: 0.102 seconds
OK
Time taken: 0.233 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.052 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0170, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0170/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0170
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 2
2013-11-28 23:10:43,080 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 112.62 sec
MapReduce Total cumulative CPU time: 1 minutes 52 seconds 620 msec
Ended Job = job_1385675857984_0170
Loading data to table default.q18_tmp
Table default.q18_tmp stats: [num_partitions: 0, num_files: 2, num_rows: 0, total_size: 62306757, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10  Reduce: 2   Cumulative CPU: 112.62 sec   HDFS Read: 2309130054 HDFS Write: 62306757 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 52 seconds 620 msec
OK
Time taken: 35.743 seconds
Total MapReduce jobs = 5
Stage-5 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0171, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0171/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0171
Hadoop job information for Stage-5: number of mappers: 4; number of reducers: 1
2013-11-28 23:11:34,896 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 57.75 sec
MapReduce Total cumulative CPU time: 57 seconds 750 msec
Ended Job = job_1385675857984_0171
Stage-15 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0172, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0172/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0172
Hadoop job information for Stage-1: number of mappers: 14; number of reducers: 3
2013-11-28 23:12:20,435 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 205.78 sec
MapReduce Total cumulative CPU time: 3 minutes 25 seconds 780 msec
Ended Job = job_1385675857984_0172
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0173, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0173/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0173
Hadoop job information for Stage-2: number of mappers: 3; number of reducers: 1
2013-11-28 23:12:37,866 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.86 sec
MapReduce Total cumulative CPU time: 2 seconds 860 msec
Ended Job = job_1385675857984_0173
Launching Job 4 out of 5
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0174, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0174/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0174
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 23:12:55,069 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.44 sec
MapReduce Total cumulative CPU time: 1 seconds 440 msec
Ended Job = job_1385675857984_0174
Loading data to table default.q18_large_volume_customer
Table default.q18_large_volume_customer stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6095, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 57.75 sec   HDFS Read: 593821728 HDFS Write: 285929869 SUCCESS
Job 1: Map: 14  Reduce: 3   Cumulative CPU: 205.78 sec   HDFS Read: 2657373864 HDFS Write: 13072 SUCCESS
Job 2: Map: 3  Reduce: 1   Cumulative CPU: 2.86 sec   HDFS Read: 14173 HDFS Write: 12900 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 1.44 sec   HDFS Read: 13267 HDFS Write: 6095 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 27 seconds 830 msec
OK
Time taken: 131.88 seconds
Time:177.73
Running Hive query: tpch/q19_discounted_revenue.hive
13/11/28 23:12:56 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:12:56 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:12:56 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:12:56 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:12:56 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:12:56 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:12:56 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.776 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.181 seconds
OK
Time taken: 0.234 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.058 seconds
Total MapReduce jobs = 2
Stage-1 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0175, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0175/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0175
Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 3
2013-11-28 23:14:12,636 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 260.98 sec
MapReduce Total cumulative CPU time: 4 minutes 20 seconds 980 msec
Ended Job = job_1385675857984_0175
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0176, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0176/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0176
Ended Job = job_1385675857984_0176
Loading data to table default.q19_discounted_revenue
Table default.q19_discounted_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 20, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 11  Reduce: 3   Cumulative CPU: 260.98 sec   HDFS Read: 2381757365 HDFS Write: 363 SUCCESS
Job 1:  Cumulative CPU: 2.51 sec   HDFS Read: 1464 HDFS Write: 20 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 23 seconds 490 msec
OK
Time taken: 85.638 seconds
Time:95.52
Running Hive query: tpch/q20_potential_part_promotion.hive
13/11/28 23:14:32 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:14:32 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:14:32 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:14:32 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:14:32 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:14:32 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:14:32 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.96 seconds
OK
Time taken: 0.139 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.166 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.118 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.215 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.083 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.058 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0177, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0177/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0177
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-28 23:15:03,885 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.5 sec
MapReduce Total cumulative CPU time: 5 seconds 500 msec
Ended Job = job_1385675857984_0177
Loading data to table default.q20_tmp1
Table default.q20_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 44294, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 5.5 sec   HDFS Read: 72627311 HDFS Write: 44294 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 500 msec
OK
Time taken: 22.443 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0178, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0178/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0178
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 3
2013-11-28 23:15:35,282 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 95.9 sec
MapReduce Total cumulative CPU time: 1 minutes 35 seconds 900 msec
Ended Job = job_1385675857984_0178
Loading data to table default.q20_tmp2
Table default.q20_tmp2 stats: [num_partitions: 0, num_files: 3, num_rows: 0, total_size: 28102876, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10  Reduce: 3   Cumulative CPU: 95.9 sec   HDFS Read: 2309130054 HDFS Write: 28102876 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 35 seconds 900 msec
OK
Time taken: 31.52 seconds
Total MapReduce jobs = 4
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 23:15:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 23:15:39 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-15-35_987_6179180787569763559-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 23:15:39 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-15-35_987_6179180787569763559-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 23:15:39 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:15:39 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:15:39 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:15:39 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:15:39 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:15:39 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:15:39 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 11:15:40	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 11:15:40	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-15-35_987_6179180787569763559-1/-local-10007/HashTable-Stage-8/MapJoin-mapfile21--.hashtable
2013-11-28 11:15:40	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-15-35_987_6179180787569763559-1/-local-10007/HashTable-Stage-8/MapJoin-mapfile21--.hashtable
2013-11-28 11:15:40	End of local task; Time Taken: 0.837 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 4
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0179, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0179/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0179
Hadoop job information for Stage-8: number of mappers: 2; number of reducers: 0
2013-11-28 23:16:00,074 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 10.03 sec
MapReduce Total cumulative CPU time: 10 seconds 30 msec
Ended Job = job_1385675857984_0179
Stage-9 is filtered out by condition resolver.
Stage-10 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0180, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0180/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0180
Hadoop job information for Stage-1: number of mappers: 5; number of reducers: 1
2013-11-28 23:16:23,617 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 23.24 sec
MapReduce Total cumulative CPU time: 23 seconds 240 msec
Ended Job = job_1385675857984_0180
Loading data to table default.q20_tmp3
Table default.q20_tmp3 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 268634, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2   Cumulative CPU: 10.03 sec   HDFS Read: 359428908 HDFS Write: 704014 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 23.24 sec   HDFS Read: 28808269 HDFS Write: 268634 SUCCESS
Total MapReduce CPU Time Spent: 33 seconds 270 msec
OK
Time taken: 48.141 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0181, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0181/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0181
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-28 23:16:43,290 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.07 sec
MapReduce Total cumulative CPU time: 5 seconds 70 msec
Ended Job = job_1385675857984_0181
Loading data to table default.q20_tmp4
Table default.q20_tmp4 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 74758, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 5.07 sec   HDFS Read: 268849 HDFS Write: 74758 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 70 msec
OK
Time taken: 19.607 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 23:16:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 23:16:46 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-16-43_737_1837219715175066238-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 23:16:46 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-16-43_737_1837219715175066238-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 23:16:46 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:16:46 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:16:46 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:16:46 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:16:46 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:16:46 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:16:46 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 11:16:47	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 11:16:47	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-16-43_737_1837219715175066238-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile41--.hashtable
2013-11-28 11:16:47	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-16-43_737_1837219715175066238-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile41--.hashtable
2013-11-28 11:16:48	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-16-43_737_1837219715175066238-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-11-28 11:16:48	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-16-43_737_1837219715175066238-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-11-28 11:16:48	End of local task; Time Taken: 1.329 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0182, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0182/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0182
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 23:17:07,478 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.92 sec
MapReduce Total cumulative CPU time: 2 seconds 920 msec
Ended Job = job_1385675857984_0182
Loading data to table default.q20_potential_part_promotion
Table default.q20_potential_part_promotion stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 25073, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.92 sec   HDFS Read: 4249168 HDFS Write: 25073 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 920 msec
OK
Time taken: 24.208 seconds
Time:156.85
Running Hive query: tpch/q21_suppliers_who_kept_orders_waiting.hive
13/11/28 23:17:09 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:17:09 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:17:09 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:17:09 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:17:09 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:17:09 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:17:09 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.884 seconds
OK
Time taken: 0.131 seconds
OK
Time taken: 0.121 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.18 seconds
OK
Time taken: 0.111 seconds
OK
Time taken: 0.112 seconds
OK
Time taken: 0.224 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0183, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0183/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0183
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 3
2013-11-28 23:18:09,281 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 206.07 sec
MapReduce Total cumulative CPU time: 3 minutes 26 seconds 70 msec
Ended Job = job_1385675857984_0183
Loading data to table default.q21_tmp1
Table default.q21_tmp1 stats: [num_partitions: 0, num_files: 3, num_rows: 0, total_size: 73376067, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10  Reduce: 3   Cumulative CPU: 206.07 sec   HDFS Read: 2309130054 HDFS Write: 73376067 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 26 seconds 70 msec
OK
Time taken: 51.277 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0184, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0184/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0184
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 3
2013-11-28 23:18:49,811 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 157.44 sec
MapReduce Total cumulative CPU time: 2 minutes 37 seconds 440 msec
Ended Job = job_1385675857984_0184
Loading data to table default.q21_tmp2
Table default.q21_tmp2 stats: [num_partitions: 0, num_files: 3, num_rows: 0, total_size: 67065808, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10  Reduce: 3   Cumulative CPU: 157.44 sec   HDFS Read: 2309130054 HDFS Write: 67065808 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 37 seconds 440 msec
OK
Time taken: 40.49 seconds
Total MapReduce jobs = 14
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 23:18:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 23:18:56 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-18-50_302_8874958636641121636-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 23:18:56 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-18-50_302_8874958636641121636-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 23:18:56 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:18:56 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:18:56 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:18:56 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:18:56 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:18:56 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:18:56 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 11:18:57	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 11:18:58	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-18-50_302_8874958636641121636-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-11-28 11:18:58	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-18-50_302_8874958636641121636-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-11-28 11:18:58	End of local task; Time Taken: 0.965 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 14
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0185, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0185/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0185
Hadoop job information for Stage-24: number of mappers: 1; number of reducers: 0
2013-11-28 23:19:10,873 Stage-24 map = 100%,  reduce = 0%, Cumulative CPU 1.7 sec
MapReduce Total cumulative CPU time: 1 seconds 700 msec
Ended Job = job_1385675857984_0185
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 14
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0186, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0186/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0186
Hadoop job information for Stage-8: number of mappers: 11; number of reducers: 3
2013-11-28 23:19:42,549 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 115.23 sec
MapReduce Total cumulative CPU time: 1 minutes 55 seconds 230 msec
Ended Job = job_1385675857984_0186
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0187, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0187/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0187
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1
2013-11-28 23:20:10,562 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 38.28 sec
MapReduce Total cumulative CPU time: 38 seconds 280 msec
Ended Job = job_1385675857984_0187
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0188, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0188/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0188
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 1
2013-11-28 23:20:44,582 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 34.5 sec
MapReduce Total cumulative CPU time: 34 seconds 500 msec
Ended Job = job_1385675857984_0188
Stage-25 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 5 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0189, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0189/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0189
Hadoop job information for Stage-3: number of mappers: 4; number of reducers: 1
2013-11-28 23:21:17,952 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 31.98 sec
MapReduce Total cumulative CPU time: 31 seconds 980 msec
Ended Job = job_1385675857984_0189
Launching Job 6 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0190, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0190/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0190
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 23:21:35,303 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 2.0 sec
MapReduce Total cumulative CPU time: 2 seconds 0 msec
Ended Job = job_1385675857984_0190
Launching Job 7 out of 14
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0191, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0191/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0191
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-28 23:21:52,873 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 2.36 sec
MapReduce Total cumulative CPU time: 2 seconds 360 msec
Ended Job = job_1385675857984_0191
Loading data to table default.q21_suppliers_who_kept_orders_waiting
Table default.q21_suppliers_who_kept_orders_waiting stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 2200, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.7 sec   HDFS Read: 4249168 HDFS Write: 48675 SUCCESS
Job 1: Map: 11  Reduce: 3   Cumulative CPU: 115.23 sec   HDFS Read: 2309179096 HDFS Write: 20385810 SUCCESS
Job 2: Map: 6  Reduce: 1   Cumulative CPU: 38.28 sec   HDFS Read: 540974126 HDFS Write: 9846316 SUCCESS
Job 3: Map: 4  Reduce: 1   Cumulative CPU: 34.5 sec   HDFS Read: 83223395 HDFS Write: 9485407 SUCCESS
Job 4: Map: 4  Reduce: 1   Cumulative CPU: 31.98 sec   HDFS Read: 76552227 HDFS Write: 46194 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 2.0 sec   HDFS Read: 46561 HDFS Write: 46194 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 2.36 sec   HDFS Read: 46561 HDFS Write: 2200 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 46 seconds 50 msec
OK
Time taken: 183.046 seconds
Time:285.39
Running Hive query: tpch/q22_global_sales_opportunity.hive
13/11/28 23:21:54 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:21:54 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:21:54 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:21:54 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:21:54 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:21:54 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:21:54 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.71 seconds
OK
Time taken: 0.111 seconds
OK
Time taken: 0.184 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.231 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.084 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0192, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0192/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0192
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2013-11-28 23:22:19,510 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.79 sec
MapReduce Total cumulative CPU time: 4 seconds 790 msec
Ended Job = job_1385675857984_0192
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-11-28_23-22-03_536_6062207337651935602-1/-ext-10000
Loading data to table default.q22_customer_tmp
Table default.q22_customer_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 2205310, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 4.79 sec   HDFS Read: 73234513 HDFS Write: 2205310 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 790 msec
OK
Time taken: 16.585 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0193, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0193/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0193
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-28 23:22:38,916 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.09 sec
MapReduce Total cumulative CPU time: 3 seconds 90 msec
Ended Job = job_1385675857984_0193
Loading data to table default.q22_customer_tmp1
Table default.q22_customer_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 18, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.09 sec   HDFS Read: 2205533 HDFS Write: 18 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 90 msec
OK
Time taken: 19.274 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0194, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0194/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0194
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-11-28 23:23:09,472 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 32.55 sec
MapReduce Total cumulative CPU time: 32 seconds 550 msec
Ended Job = job_1385675857984_0194
Loading data to table default.q22_orders_tmp
Table default.q22_orders_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 2025890, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 3  Reduce: 1   Cumulative CPU: 32.55 sec   HDFS Read: 520587215 HDFS Write: 2025890 SUCCESS
Total MapReduce CPU Time Spent: 32 seconds 550 msec
OK
Time taken: 30.557 seconds
Total MapReduce jobs = 2
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/28 23:23:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/28 23:23:12 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-23-09_955_2208774273584081411-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/28 23:23:12 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-28_23-23-09_955_2208774273584081411-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/28 23:23:12 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/28 23:23:12 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/28 23:23:12 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/28 23:23:12 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/28 23:23:12 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/28 23:23:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/28 23:23:12 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-28 11:23:13	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-28 11:23:14	Processing rows:	200000	Hashtable size:	199999	Memory usage:	67879768	percentage:	0.142
2013-11-28 11:23:14	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-23-09_955_2208774273584081411-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile10--.hashtable
2013-11-28 11:23:14	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-23-09_955_2208774273584081411-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile10--.hashtable
2013-11-28 11:23:14	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-28_23-23-09_955_2208774273584081411-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-11-28 11:23:14	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-28_23-23-09_955_2208774273584081411-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-11-28 11:23:14	End of local task; Time Taken: 1.492 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0195, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0195/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0195
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-28 23:23:36,457 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 6.55 sec
MapReduce Total cumulative CPU time: 6 seconds 550 msec
Ended Job = job_1385675857984_0195
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0196, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0196/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0196
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-28 23:23:53,912 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.96 sec
MapReduce Total cumulative CPU time: 1 seconds 960 msec
Ended Job = job_1385675857984_0196
Loading data to table default.q22_global_sales_opportunity
Table default.q22_global_sales_opportunity stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 200, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 6.55 sec   HDFS Read: 2205533 HDFS Write: 313 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 1.96 sec   HDFS Read: 680 HDFS Write: 200 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 510 msec
OK
Time taken: 44.432 seconds
Time:121.03
