Running Hive from /opt/hive-0.12.0
Running Hadoop from 
Running Hive query: tpch/q1_pricing_summary_report.hive
13/12/09 23:46:45 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 23:46:45 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 23:46:45 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 23:46:45 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 23:46:45 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 23:46:45 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 23:46:45 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.978 seconds
OK
Time taken: 0.187 seconds
OK
Time taken: 0.212 seconds
OK
Time taken: 0.051 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0103, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0103/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0103
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-09 23:54:51,647 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3653.43 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 0 minutes 53 seconds 430 msec
Ended Job = job_1386606905013_0103
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0104, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0104/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0104
Hadoop job information for Stage-2: number of mappers: 8; number of reducers: 1
2013-12-09 23:55:11,573 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.49 sec
MapReduce Total cumulative CPU time: 7 seconds 490 msec
Ended Job = job_1386606905013_0104
Loading data to table default.q1_pricing_summary_report
Table default.q1_pricing_summary_report stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 588, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 3653.43 sec   HDFS Read: 79582199808 HDFS Write: 8011 SUCCESS
Job 1: Map: 8  Reduce: 1   Cumulative CPU: 7.49 sec   HDFS Read: 26931 HDFS Write: 588 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 1 minutes 0 seconds 920 msec
OK
Time taken: 499.26 seconds
Time:507.83
Running Hive query: tpch/q2_minimum_cost_supplier.hive
13/12/09 23:55:13 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 23:55:13 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 23:55:13 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 23:55:13 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 23:55:13 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 23:55:13 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 23:55:13 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.085 seconds
OK
Time taken: 0.087 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.19 seconds
OK
Time taken: 0.096 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.2 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.025 seconds
OK
Time taken: 0.035 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.093 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 10
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 23:55:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 23:55:27 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_23-55-21_570_1311422317958241026-1/-local-10020/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 23:55:27 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_23-55-21_570_1311422317958241026-1/-local-10020/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 23:55:27 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 23:55:27 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 23:55:27 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 23:55:27 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 23:55:27 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 23:55:27 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 23:55:27 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 11:55:27	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 11:55:28	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_23-55-21_570_1311422317958241026-1/-local-10017/HashTable-Stage-18/MapJoin-mapfile61--.hashtable
2013-12-09 11:55:28	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_23-55-21_570_1311422317958241026-1/-local-10017/HashTable-Stage-18/MapJoin-mapfile61--.hashtable
2013-12-09 11:55:28	End of local task; Time Taken: 0.893 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0105, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0105/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0105
Hadoop job information for Stage-18: number of mappers: 1; number of reducers: 0
2013-12-09 23:55:41,313 Stage-18 map = 100%,  reduce = 0%, Cumulative CPU 1.04 sec
MapReduce Total cumulative CPU time: 1 seconds 40 msec
Ended Job = job_1386606905013_0105
Stage-23 is filtered out by condition resolver.
Stage-24 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0106, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0106/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0106
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2013-12-09 23:56:11,464 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 18.9 sec
MapReduce Total cumulative CPU time: 18 seconds 900 msec
Ended Job = job_1386606905013_0106
Stage-21 is filtered out by condition resolver.
Stage-22 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 3 out of 10
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0107, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0107/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0107
Hadoop job information for Stage-2: number of mappers: 47; number of reducers: 13
2013-12-09 23:57:58,269 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 760.53 sec
MapReduce Total cumulative CPU time: 12 minutes 40 seconds 530 msec
Ended Job = job_1386606905013_0107
Stage-19 is filtered out by condition resolver.
Stage-20 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 4 out of 10
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0108, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0108/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0108
Hadoop job information for Stage-3: number of mappers: 23; number of reducers: 6
2013-12-09 23:59:01,035 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 357.43 sec
MapReduce Total cumulative CPU time: 5 minutes 57 seconds 430 msec
Ended Job = job_1386606905013_0108
Loading data to table default.q2_minimum_cost_supplier_tmp1
Table default.q2_minimum_cost_supplier_tmp1 stats: [num_partitions: 0, num_files: 6, num_rows: 0, total_size: 10939639, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.04 sec   HDFS Read: 2424 HDFS Write: 231 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 18.9 sec   HDFS Read: 142874768 HDFS Write: 32875184 SUCCESS
Job 2: Map: 47  Reduce: 13   Cumulative CPU: 760.53 sec   HDFS Read: 12242467750 HDFS Write: 2763176536 SUCCESS
Job 3: Map: 23  Reduce: 6   Cumulative CPU: 357.43 sec   HDFS Read: 5216526965 HDFS Write: 10939639 SUCCESS
Total MapReduce CPU Time Spent: 18 minutes 57 seconds 900 msec
OK
Time taken: 219.927 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0109, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0109/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0109
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-12-09 23:59:23,540 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.2 sec
MapReduce Total cumulative CPU time: 12 seconds 200 msec
Ended Job = job_1386606905013_0109
Loading data to table default.q2_minimum_cost_supplier_tmp2
Table default.q2_minimum_cost_supplier_tmp2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 715857, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 12.2 sec   HDFS Read: 10940781 HDFS Write: 715857 SUCCESS
Total MapReduce CPU Time Spent: 12 seconds 200 msec
OK
Time taken: 22.482 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 23:59:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 23:59:26 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_23-59-23_981_8949824304307450065-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 23:59:26 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_23-59-23_981_8949824304307450065-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 23:59:26 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 23:59:26 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 23:59:26 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 23:59:26 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 23:59:26 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 23:59:26 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 23:59:26 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 11:59:26	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 11:59:27	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_23-59-23_981_8949824304307450065-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile71--.hashtable
2013-12-09 11:59:27	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_23-59-23_981_8949824304307450065-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile71--.hashtable
2013-12-09 11:59:27	End of local task; Time Taken: 1.0 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0110, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0110/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0110
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 1
2013-12-09 23:59:52,852 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 14.33 sec
MapReduce Total cumulative CPU time: 14 seconds 330 msec
Ended Job = job_1386606905013_0110
Loading data to table default.q2_minimum_cost_supplier
Table default.q2_minimum_cost_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16261, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 14.33 sec   HDFS Read: 10940781 HDFS Write: 16261 SUCCESS
Total MapReduce CPU Time Spent: 14 seconds 330 msec
OK
Time taken: 29.307 seconds
Time:281.23
Running Hive query: tpch/q3_shipping_priority.hive
13/12/09 23:59:54 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 23:59:54 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 23:59:54 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 23:59:54 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 23:59:54 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 23:59:54 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 23:59:54 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.958 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.214 seconds
OK
Time taken: 0.194 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.048 seconds
Total MapReduce jobs = 6
Stage-1 is selected by condition resolver.
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0111, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0111/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0111
Hadoop job information for Stage-1: number of mappers: 78; number of reducers: 20
2013-12-10 00:02:06,671 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 947.4 sec
MapReduce Total cumulative CPU time: 15 minutes 47 seconds 400 msec
Ended Job = job_1386606905013_0111
Stage-14 is filtered out by condition resolver.
Stage-15 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0112, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0112/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0112
Hadoop job information for Stage-2: number of mappers: 304; number of reducers: 79
2013-12-10 00:11:03,622 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4288.04 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 11 minutes 28 seconds 40 msec
Ended Job = job_1386606905013_0112
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0113, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0113/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0113
Hadoop job information for Stage-3: number of mappers: 9; number of reducers: 1
2013-12-10 00:11:30,336 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 44.53 sec
MapReduce Total cumulative CPU time: 44 seconds 530 msec
Ended Job = job_1386606905013_0113
Launching Job 4 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0114, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0114/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0114
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-10 00:11:59,448 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 12.03 sec
MapReduce Total cumulative CPU time: 12 seconds 30 msec
Ended Job = job_1386606905013_0114
Loading data to table default.q3_shipping_priority
Table default.q3_shipping_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 396, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 78  Reduce: 20   Cumulative CPU: 947.4 sec   HDFS Read: 20257241310 HDFS Write: 499838544 SUCCESS
Job 1: Map: 304  Reduce: 79   Cumulative CPU: 4288.04 sec   HDFS Read: 80082043767 HDFS Write: 47949847 SUCCESS
Job 2: Map: 9  Reduce: 1   Cumulative CPU: 44.53 sec   HDFS Read: 47968532 HDFS Write: 47942799 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 12.03 sec   HDFS Read: 47943164 HDFS Write: 396 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 28 minutes 12 seconds 0 msec
OK
Time taken: 717.801 seconds
Time:726.61
Running Hive query: tpch/q4_order_priority.hive
13/12/10 00:12:01 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 00:12:01 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 00:12:01 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 00:12:01 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 00:12:01 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 00:12:01 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 00:12:01 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.062 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.208 seconds
OK
Time taken: 0.137 seconds
OK
Time taken: 0.18 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.051 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0115, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0115/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0115
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-10 00:18:19,218 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2993.76 sec
MapReduce Total cumulative CPU time: 49 minutes 53 seconds 760 msec
Ended Job = job_1386606905013_0115
Loading data to table default.q4_order_priority_tmp
Table default.q4_order_priority_tmp stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 1350015083, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 2993.76 sec   HDFS Read: 79582199808 HDFS Write: 1350015083 SUCCESS
Total MapReduce CPU Time Spent: 49 minutes 53 seconds 760 msec
OK
Time taken: 370.917 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0116, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0116/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0116
Hadoop job information for Stage-1: number of mappers: 77; number of reducers: 20
2013-12-10 00:21:36,889 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1214.79 sec
MapReduce Total cumulative CPU time: 20 minutes 14 seconds 790 msec
Ended Job = job_1386606905013_0116
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0117, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0117/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0117
Hadoop job information for Stage-2: number of mappers: 7; number of reducers: 1
2013-12-10 00:21:56,582 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.35 sec
MapReduce Total cumulative CPU time: 5 seconds 350 msec
Ended Job = job_1386606905013_0117
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0118, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0118/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0118
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-10 00:22:14,428 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.8 sec
MapReduce Total cumulative CPU time: 1 seconds 800 msec
Ended Job = job_1386606905013_0118
Loading data to table default.q4_order_priority
Table default.q4_order_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 87, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 20   Cumulative CPU: 1214.79 sec   HDFS Read: 19143698264 HDFS Write: 4860 SUCCESS
Job 1: Map: 7  Reduce: 1   Cumulative CPU: 5.35 sec   HDFS Read: 10315 HDFS Write: 248 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.8 sec   HDFS Read: 615 HDFS Write: 87 SUCCESS
Total MapReduce CPU Time Spent: 20 minutes 21 seconds 940 msec
OK
Time taken: 235.01 seconds
Time:614.90
Running Hive query: tpch/q5_local_supplier_volume.hive
13/12/10 00:22:15 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 00:22:15 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 00:22:15 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 00:22:15 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 00:22:15 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 00:22:15 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 00:22:15 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.981 seconds
OK
Time taken: 0.096 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.183 seconds
OK
Time taken: 0.226 seconds
OK
Time taken: 0.035 seconds
OK
Time taken: 0.03 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 00:22:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 00:22:30 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_00-22-24_072_4002040583767370095-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 00:22:30 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_00-22-24_072_4002040583767370095-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 00:22:30 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 00:22:30 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 00:22:30 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 00:22:30 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 00:22:30 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 00:22:30 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 00:22:30 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 12:22:31	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 12:22:32	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_00-22-24_072_4002040583767370095-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile81--.hashtable
2013-12-10 12:22:32	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_00-22-24_072_4002040583767370095-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile81--.hashtable
2013-12-10 12:22:32	End of local task; Time Taken: 0.815 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0119, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0119/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0119
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-12-10 00:22:46,254 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 0.96 sec
MapReduce Total cumulative CPU time: 960 msec
Ended Job = job_1386606905013_0119
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0120, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0120/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0120
Hadoop job information for Stage-7: number of mappers: 2; number of reducers: 1
2013-12-10 00:23:09,934 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 12.54 sec
MapReduce Total cumulative CPU time: 12 seconds 540 msec
Ended Job = job_1386606905013_0120
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0121, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0121/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0121
Hadoop job information for Stage-8: number of mappers: 298; number of reducers: 80
2013-12-10 00:35:12,595 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 6505.99 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 48 minutes 25 seconds 990 msec
Ended Job = job_1386606905013_0121
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 24
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0122, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0122/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0122
Hadoop job information for Stage-1: number of mappers: 87; number of reducers: 24
2013-12-10 00:38:56,566 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1787.03 sec
MapReduce Total cumulative CPU time: 29 minutes 47 seconds 30 msec
Ended Job = job_1386606905013_0122
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0123, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0123/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0123
Hadoop job information for Stage-2: number of mappers: 19; number of reducers: 4
2013-12-10 00:40:12,258 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 304.88 sec
MapReduce Total cumulative CPU time: 5 minutes 4 seconds 880 msec
Ended Job = job_1386606905013_0123
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0124, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0124/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0124
Hadoop job information for Stage-3: number of mappers: 3; number of reducers: 1
2013-12-10 00:40:30,541 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.89 sec
MapReduce Total cumulative CPU time: 2 seconds 890 msec
Ended Job = job_1386606905013_0124
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0125, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0125/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0125
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-10 00:40:50,244 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.46 sec
MapReduce Total cumulative CPU time: 1 seconds 460 msec
Ended Job = job_1386606905013_0125
Loading data to table default.q5_local_supplier_volume
Table default.q5_local_supplier_volume stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 136, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.96 sec   HDFS Read: 2424 HDFS Write: 222 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 12.54 sec   HDFS Read: 142874759 HDFS Write: 5881000 SUCCESS
Job 2: Map: 298  Reduce: 80   Cumulative CPU: 6505.99 sec   HDFS Read: 79588081175 HDFS Write: 5591627629 SUCCESS
Job 3: Map: 87  Reduce: 24   Cumulative CPU: 1787.03 sec   HDFS Read: 23385322820 HDFS Write: 830634798 SUCCESS
Job 4: Map: 19  Reduce: 4   Cumulative CPU: 304.88 sec   HDFS Read: 3294208065 HDFS Write: 1028 SUCCESS
Job 5: Map: 3  Reduce: 1   Cumulative CPU: 2.89 sec   HDFS Read: 2351 HDFS Write: 257 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.46 sec   HDFS Read: 624 HDFS Write: 136 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 23 minutes 35 seconds 750 msec
OK
Time taken: 1106.644 seconds
Time:1115.89
Running Hive query: tpch/q6_forecast_revenue_change.hive
13/12/10 00:40:51 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 00:40:51 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 00:40:51 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 00:40:51 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 00:40:51 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 00:40:51 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 00:40:51 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.216 seconds
OK
Time taken: 0.191 seconds
OK
Time taken: 0.212 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0126, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0126/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0126
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 1
2013-12-10 00:45:00,700 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2036.41 sec
MapReduce Total cumulative CPU time: 33 minutes 56 seconds 410 msec
Ended Job = job_1386606905013_0126
Loading data to table default.q6_forecast_revenue_change
Table default.q6_forecast_revenue_change stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 22, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 1   Cumulative CPU: 2036.41 sec   HDFS Read: 79582199808 HDFS Write: 22 SUCCESS
Total MapReduce CPU Time Spent: 33 minutes 56 seconds 410 msec
OK
Time taken: 241.689 seconds
Time:250.49
Running Hive query: tpch/q7_volume_shipping.hive
13/12/10 00:45:02 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 00:45:02 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 00:45:02 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 00:45:02 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 00:45:02 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 00:45:02 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 00:45:02 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.072 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.193 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.205 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.035 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.059 seconds
Total MapReduce jobs = 3
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 00:45:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 00:45:13 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_00-45-10_566_3134834755573266205-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 00:45:13 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_00-45-10_566_3134834755573266205-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 00:45:13 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 00:45:13 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 00:45:13 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 00:45:13 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 00:45:13 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 00:45:13 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 00:45:13 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 12:45:14	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 12:45:15	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_00-45-10_566_3134834755573266205-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-12-10 12:45:15	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_00-45-10_566_3134834755573266205-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-12-10 12:45:15	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_00-45-10_566_3134834755573266205-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-12-10 12:45:15	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_00-45-10_566_3134834755573266205-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-12-10 12:45:15	End of local task; Time Taken: 0.844 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0127, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0127/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0127
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 0
2013-12-10 00:45:30,049 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.41 sec
MapReduce Total cumulative CPU time: 1 seconds 410 msec
Ended Job = job_1386606905013_0127
Stage-5 is selected by condition resolver.
Stage-4 is filtered out by condition resolver.
Stage-6 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-12-10_00-45-10_566_3134834755573266205-1/-ext-10000
Loading data to table default.q7_volume_shipping_tmp
Table default.q7_volume_shipping_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 38, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.41 sec   HDFS Read: 2424 HDFS Write: 38 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 410 msec
OK
Time taken: 19.982 seconds
Total MapReduce jobs = 9
Stage-6 is selected by condition resolver.
Launching Job 1 out of 9
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0128, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0128/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0128
Hadoop job information for Stage-6: number of mappers: 364; number of reducers: 80
2013-12-10 00:57:01,327 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 4909.55 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 21 minutes 49 seconds 550 msec
Ended Job = job_1386606905013_0128
Stage-24 is filtered out by condition resolver.
Stage-25 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 9
Number of reduce tasks not specified. Estimated from input data size: 10
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0129, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0129/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0129
Hadoop job information for Stage-7: number of mappers: 40; number of reducers: 10
2013-12-10 01:02:46,573 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 2123.79 sec
MapReduce Total cumulative CPU time: 35 minutes 23 seconds 790 msec
Ended Job = job_1386606905013_0129
Stage-22 is filtered out by condition resolver.
Stage-23 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 9
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0130, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0130/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0130
Hadoop job information for Stage-1: number of mappers: 32; number of reducers: 8
2013-12-10 01:07:06,952 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1999.12 sec
MapReduce Total cumulative CPU time: 33 minutes 19 seconds 120 msec
Ended Job = job_1386606905013_0130
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 01:07:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 01:07:09 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_00-45-30_551_5895090691752336689-1/-local-10024/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 01:07:09 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_00-45-30_551_5895090691752336689-1/-local-10024/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 01:07:09 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 01:07:09 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 01:07:09 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 01:07:09 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 01:07:09 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 01:07:09 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 01:07:09 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 01:07:10	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 01:07:10	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_00-45-30_551_5895090691752336689-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-12-10 01:07:10	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_00-45-30_551_5895090691752336689-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-12-10 01:07:10	End of local task; Time Taken: 0.593 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 4 out of 9
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0131, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0131/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0131
Hadoop job information for Stage-3: number of mappers: 29; number of reducers: 7
2013-12-10 01:08:15,884 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 439.45 sec
MapReduce Total cumulative CPU time: 7 minutes 19 seconds 450 msec
Ended Job = job_1386606905013_0131
Launching Job 5 out of 9
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0132, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0132/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0132
Hadoop job information for Stage-4: number of mappers: 5; number of reducers: 1
2013-12-10 01:08:35,643 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.83 sec
MapReduce Total cumulative CPU time: 3 seconds 830 msec
Ended Job = job_1386606905013_0132
Loading data to table default.q7_volume_shipping
Table default.q7_volume_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 157, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 364  Reduce: 80   Cumulative CPU: 4909.55 sec   HDFS Read: 97375874339 HDFS Write: 9560339811 SUCCESS
Job 1: Map: 40  Reduce: 10   Cumulative CPU: 2123.79 sec   HDFS Read: 12024017587 HDFS Write: 9009669073 SUCCESS
Job 2: Map: 32  Reduce: 8   Cumulative CPU: 1999.12 sec   HDFS Read: 9152693022 HDFS Write: 8468719588 SUCCESS
Job 3: Map: 29  Reduce: 7   Cumulative CPU: 439.45 sec   HDFS Read: 8468877441 HDFS Write: 844 SUCCESS
Job 4: Map: 5  Reduce: 1   Cumulative CPU: 3.83 sec   HDFS Read: 3123 HDFS Write: 157 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 37 minutes 55 seconds 740 msec
OK
Time taken: 1385.533 seconds
Time:1414.87
Running Hive query: tpch/q8_national_market_share.hive
13/12/10 01:08:37 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 01:08:37 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 01:08:37 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 01:08:37 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 01:08:37 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 01:08:37 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 01:08:37 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.033 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.111 seconds
OK
Time taken: 0.097 seconds
OK
Time taken: 0.088 seconds
OK
Time taken: 0.093 seconds
OK
Time taken: 0.198 seconds
OK
Time taken: 0.194 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.025 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 18
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 01:08:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 01:08:53 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_01-08-45_609_505168196450147053-1/-local-10035/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 01:08:53 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_01-08-45_609_505168196450147053-1/-local-10035/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 01:08:53 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 01:08:53 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 01:08:53 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 01:08:53 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 01:08:53 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 01:08:53 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 01:08:53 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 01:08:54	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 01:08:55	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_01-08-45_609_505168196450147053-1/-local-10032/HashTable-Stage-32/MapJoin-mapfile111--.hashtable
2013-12-10 01:08:55	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_01-08-45_609_505168196450147053-1/-local-10032/HashTable-Stage-32/MapJoin-mapfile111--.hashtable
2013-12-10 01:08:55	End of local task; Time Taken: 0.838 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 18
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0133, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0133/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0133
Hadoop job information for Stage-32: number of mappers: 1; number of reducers: 0
2013-12-10 01:09:08,361 Stage-32 map = 100%,  reduce = 0%, Cumulative CPU 1.02 sec
MapReduce Total cumulative CPU time: 1 seconds 20 msec
Ended Job = job_1386606905013_0133
Stage-42 is filtered out by condition resolver.
Stage-43 is filtered out by condition resolver.
Stage-9 is selected by condition resolver.
Launching Job 2 out of 18
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0134, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0134/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0134
Hadoop job information for Stage-9: number of mappers: 12; number of reducers: 3
2013-12-10 01:09:50,444 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 121.92 sec
MapReduce Total cumulative CPU time: 2 minutes 1 seconds 920 msec
Ended Job = job_1386606905013_0134
Stage-40 is filtered out by condition resolver.
Stage-41 is filtered out by condition resolver.
Stage-10 is selected by condition resolver.
Launching Job 3 out of 18
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0135, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0135/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0135
Hadoop job information for Stage-10: number of mappers: 69; number of reducers: 18
2013-12-10 01:11:30,862 Stage-10 map = 100%,  reduce = 100%, Cumulative CPU 741.45 sec
MapReduce Total cumulative CPU time: 12 minutes 21 seconds 450 msec
Ended Job = job_1386606905013_0135
Stage-38 is filtered out by condition resolver.
Stage-39 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 18
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0136, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0136/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0136
Hadoop job information for Stage-1: number of mappers: 305; number of reducers: 80
2013-12-10 01:23:13,850 Stage-1 map = 100%,  reduce = 78%
Ended Job = job_1386606905013_0136 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1386606905013_0136_m_000001 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000009 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000003 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000025 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000039 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000048 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000063 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000072 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000071 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000097 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000101 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000134 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000112 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000154 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000162 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000192 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000186 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000198 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000210 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000264 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000234 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000020 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000223 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000297 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000113 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000140 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000259 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000197 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000268 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000280 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_m_000281 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_r_000011 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_r_000031 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_r_000041 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_r_000019 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_r_000023 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_r_000066 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_r_000070 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_r_000067 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_r_000064 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_r_000055 (and more) from job job_1386606905013_0136
Examining task ID: task_1386606905013_0136_r_000069 (and more) from job job_1386606905013_0136

Task with the most failures(4): 
-----
Task ID:
  task_1386606905013_0136_r_000067

URL:
  http://hadoop11:8088/taskdetails.jsp?jobid=job_1386606905013_0136&tipid=task_1386606905013_0136_r_000067
-----
Diagnostic Messages for this Task:


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.02 sec   HDFS Read: 2424 HDFS Write: 186 SUCCESS
Job 1: Map: 12  Reduce: 3   Cumulative CPU: 121.92 sec   HDFS Read: 2463567331 HDFS Write: 63652457 SUCCESS
Job 2: Map: 69  Reduce: 18   Cumulative CPU: 741.45 sec   HDFS Read: 17857327941 HDFS Write: 303326332 SUCCESS
Job 3: Map: 305  Reduce: 80   FAIL
Total MapReduce CPU Time Spent: 14 minutes 24 seconds 389 msec
Command exited with non-zero status 2
Time:878.08
Running Hive query: tpch/q9_product_type_profit.hive
13/12/10 01:23:15 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 01:23:15 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 01:23:15 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 01:23:15 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 01:23:15 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 01:23:15 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 01:23:15 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.046 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.103 seconds
OK
Time taken: 0.083 seconds
OK
Time taken: 0.173 seconds
OK
Time taken: 0.186 seconds
OK
Time taken: 0.044 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.082 seconds
OK
Time taken: 0.042 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 01:23:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 01:23:30 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_01-23-23_420_9077705935924464973-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 01:23:30 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_01-23-23_420_9077705935924464973-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 01:23:30 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 01:23:30 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 01:23:30 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 01:23:30 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 01:23:30 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 01:23:30 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 01:23:30 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 01:23:31	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 01:23:31	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_01-23-23_420_9077705935924464973-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-12-10 01:23:31	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_01-23-23_420_9077705935924464973-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-12-10 01:23:31	End of local task; Time Taken: 0.572 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0137, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0137/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0137
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-12-10 01:23:58,778 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 5.96 sec
MapReduce Total cumulative CPU time: 5 seconds 960 msec
Ended Job = job_1386606905013_0137
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 78
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0138, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0138/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0138
Hadoop job information for Stage-8: number of mappers: 298; number of reducers: 78
2013-12-10 01:42:06,278 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 8793.15 sec
MapReduce Total cumulative CPU time: 0 days 2 hours 26 minutes 33 seconds 150 msec
Ended Job = job_1386606905013_0138
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 49
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0139, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0139/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0139
Hadoop job information for Stage-1: number of mappers: 171; number of reducers: 49
2013-12-10 01:59:08,987 Stage-1 map = 100%,  reduce = 37%
Ended Job = job_1386606905013_0139 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1386606905013_0139_m_000016 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000002 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000019 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000044 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000025 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000065 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000056 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000028 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000102 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000114 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000103 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000088 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000128 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000138 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000090 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000101 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000142 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_m_000152 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_r_000026 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_r_000032 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_r_000037 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_r_000023 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_r_000009 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_r_000027 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_r_000021 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_r_000027 (and more) from job job_1386606905013_0139
Examining task ID: task_1386606905013_0139_r_000019 (and more) from job job_1386606905013_0139

Task with the most failures(4): 
-----
Task ID:
  task_1386606905013_0139_r_000023

URL:
  http://hadoop11:8088/taskdetails.jsp?jobid=job_1386606905013_0139&tipid=task_1386606905013_0139_r_000023
-----
Diagnostic Messages for this Task:


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 5.96 sec   HDFS Read: 142874170 HDFS Write: 29304904 SUCCESS
Job 1: Map: 298  Reduce: 78   Cumulative CPU: 8793.15 sec   HDFS Read: 79611505079 HDFS Write: 37657255337 SUCCESS
Job 2: Map: 171  Reduce: 49   FAIL
Total MapReduce CPU Time Spent: 0 days 2 hours 26 minutes 39 seconds 109 msec
Command exited with non-zero status 2
Time:2155.07
Running Hive query: tpch/q10_returned_item.hive
13/12/10 01:59:10 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 01:59:10 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 01:59:10 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 01:59:10 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 01:59:10 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 01:59:10 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 01:59:10 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.127 seconds
OK
Time taken: 0.098 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.125 seconds
OK
Time taken: 0.191 seconds
OK
Time taken: 0.186 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.023 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 7
Stage-1 is selected by condition resolver.
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0140, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0140/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0140
Hadoop job information for Stage-1: number of mappers: 78; number of reducers: 20
2013-12-10 02:01:36,627 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 841.23 sec
MapReduce Total cumulative CPU time: 14 minutes 1 seconds 230 msec
Ended Job = job_1386606905013_0140
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 02:01:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 02:01:38 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_01-59-18_451_8492775383750989781-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 02:01:38 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_01-59-18_451_8492775383750989781-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 02:01:38 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 02:01:38 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 02:01:38 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 02:01:38 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 02:01:38 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 02:01:38 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 02:01:38 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 02:01:39	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 02:01:40	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_01-59-18_451_8492775383750989781-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-12-10 02:01:40	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_01-59-18_451_8492775383750989781-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-12-10 02:01:40	End of local task; Time Taken: 0.542 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 7
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0141, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0141/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0141
Hadoop job information for Stage-13: number of mappers: 6; number of reducers: 0
2013-12-10 02:02:20,328 Stage-13 map = 100%,  reduce = 0%, Cumulative CPU 67.54 sec
MapReduce Total cumulative CPU time: 1 minutes 7 seconds 540 msec
Ended Job = job_1386606905013_0141
Stage-17 is filtered out by condition resolver.
Stage-18 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0142, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0142/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0142
Hadoop job information for Stage-3: number of mappers: 304; number of reducers: 79
2013-12-10 02:09:32,436 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 3296.08 sec
MapReduce Total cumulative CPU time: 54 minutes 56 seconds 80 msec
Ended Job = job_1386606905013_0142
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0143, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0143/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0143
Hadoop job information for Stage-4: number of mappers: 8; number of reducers: 1
2013-12-10 02:12:11,897 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 131.67 sec
MapReduce Total cumulative CPU time: 2 minutes 11 seconds 670 msec
Ended Job = job_1386606905013_0143
Launching Job 5 out of 7
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0144, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0144/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0144
Hadoop job information for Stage-5: number of mappers: 4; number of reducers: 1
2013-12-10 02:13:42,625 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 72.71 sec
MapReduce Total cumulative CPU time: 1 minutes 12 seconds 710 msec
Ended Job = job_1386606905013_0144
Loading data to table default.q10_returned_item
Table default.q10_returned_item stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3567, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 78  Reduce: 20   Cumulative CPU: 841.23 sec   HDFS Read: 20257241310 HDFS Write: 980369893 SUCCESS
Job 1: Map: 6   Cumulative CPU: 67.54 sec   HDFS Read: 980375203 HDFS Write: 1021319236 SUCCESS
Job 2: Map: 304  Reduce: 79   Cumulative CPU: 3296.08 sec   HDFS Read: 80603533479 HDFS Write: 889025471 SUCCESS
Job 3: Map: 8  Reduce: 1   Cumulative CPU: 131.67 sec   HDFS Read: 889044169 HDFS Write: 704132397 SUCCESS
Job 4: Map: 4  Reduce: 1   Cumulative CPU: 72.71 sec   HDFS Read: 704143440 HDFS Write: 3567 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 13 minutes 29 seconds 230 msec
OK
Time taken: 864.613 seconds
Time:873.80
Running Hive query: tpch/q11_important_stock.hive
13/12/10 02:13:44 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 02:13:44 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 02:13:44 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 02:13:44 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 02:13:44 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 02:13:44 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 02:13:44 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.977 seconds
OK
Time taken: 0.088 seconds
OK
Time taken: 0.089 seconds
OK
Time taken: 0.193 seconds
OK
Time taken: 0.122 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.193 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.052 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.082 seconds
Total MapReduce jobs = 5
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 02:13:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 02:13:56 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_02-13-52_232_6112273117152467822-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 02:13:56 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_02-13-52_232_6112273117152467822-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 02:13:56 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 02:13:56 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 02:13:56 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 02:13:56 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 02:13:56 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 02:13:56 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 02:13:56 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 02:13:57	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 02:13:57	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_02-13-52_232_6112273117152467822-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-12-10 02:13:57	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_02-13-52_232_6112273117152467822-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-12-10 02:13:57	End of local task; Time Taken: 0.816 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0145, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0145/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0145
Hadoop job information for Stage-10: number of mappers: 1; number of reducers: 0
2013-12-10 02:14:17,164 Stage-10 map = 100%,  reduce = 0%, Cumulative CPU 4.68 sec
MapReduce Total cumulative CPU time: 4 seconds 680 msec
Ended Job = job_1386606905013_0145
Stage-11 is filtered out by condition resolver.
Stage-12 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0146, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0146/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0146
Hadoop job information for Stage-2: number of mappers: 47; number of reducers: 13
2013-12-10 02:15:55,200 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 719.94 sec
MapReduce Total cumulative CPU time: 11 minutes 59 seconds 940 msec
Ended Job = job_1386606905013_0146
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0147, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0147/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0147
Hadoop job information for Stage-3: number of mappers: 6; number of reducers: 1
2013-12-10 02:16:29,693 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 45.2 sec
MapReduce Total cumulative CPU time: 45 seconds 200 msec
Ended Job = job_1386606905013_0147
Loading data to table default.q11_part_tmp
Table default.q11_part_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 63173916, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 4.68 sec   HDFS Read: 142874170 HDFS Write: 846716 SUCCESS
Job 1: Map: 47  Reduce: 13   Cumulative CPU: 719.94 sec   HDFS Read: 12210439282 HDFS Write: 94289297 SUCCESS
Job 2: Map: 6  Reduce: 1   Cumulative CPU: 45.2 sec   HDFS Read: 94293053 HDFS Write: 63173916 SUCCESS
Total MapReduce CPU Time Spent: 12 minutes 49 seconds 820 msec
OK
Time taken: 157.927 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0148, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0148/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0148
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-12-10 02:16:51,855 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.15 sec
MapReduce Total cumulative CPU time: 6 seconds 150 msec
Ended Job = job_1386606905013_0148
Loading data to table default.q11_sum_tmp
Table default.q11_sum_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 6.15 sec   HDFS Read: 63174135 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 6 seconds 150 msec
OK
Time taken: 22.133 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 02:16:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 02:16:54 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_02-16-52_292_6370662749100647798-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 02:16:54 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_02-16-52_292_6370662749100647798-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 02:16:54 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 02:16:54 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 02:16:54 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 02:16:54 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 02:16:54 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 02:16:54 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 02:16:54 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 02:16:55	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 02:16:55	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_02-16-52_292_6370662749100647798-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-12-10 02:16:55	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_02-16-52_292_6370662749100647798-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-12-10 02:16:55	End of local task; Time Taken: 0.556 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0149, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0149/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0149
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-12-10 02:17:21,999 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 9.87 sec
MapReduce Total cumulative CPU time: 9 seconds 870 msec
Ended Job = job_1386606905013_0149
Loading data to table default.q11_important_stock
Table default.q11_important_stock stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 0, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 9.87 sec   HDFS Read: 63174135 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 9 seconds 870 msec
OK
Time taken: 30.124 seconds
Time:219.35
Running Hive query: tpch/q12_shipping.hive
13/12/10 02:17:23 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 02:17:23 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 02:17:23 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 02:17:23 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 02:17:23 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 02:17:23 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 02:17:23 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.991 seconds
OK
Time taken: 0.095 seconds
OK
Time taken: 0.218 seconds
OK
Time taken: 0.207 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.051 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0150, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0150/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0150
Hadoop job information for Stage-1: number of mappers: 364; number of reducers: 80
2013-12-10 02:25:35,909 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3602.79 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 0 minutes 2 seconds 790 msec
Ended Job = job_1386606905013_0150
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0151, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0151/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0151
Hadoop job information for Stage-2: number of mappers: 8; number of reducers: 1
2013-12-10 02:25:56,749 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.93 sec
MapReduce Total cumulative CPU time: 6 seconds 930 msec
Ended Job = job_1386606905013_0151
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0152, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0152/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0152
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-10 02:26:14,968 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.88 sec
MapReduce Total cumulative CPU time: 1 seconds 880 msec
Ended Job = job_1386606905013_0152
Loading data to table default.q12_shipping
Table default.q12_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 46, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 364  Reduce: 80   Cumulative CPU: 3602.79 sec   HDFS Read: 97375874339 HDFS Write: 9920 SUCCESS
Job 1: Map: 8  Reduce: 1   Cumulative CPU: 6.93 sec   HDFS Read: 28840 HDFS Write: 156 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.88 sec   HDFS Read: 523 HDFS Write: 46 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 0 minutes 11 seconds 600 msec
OK
Time taken: 524.316 seconds
Time:533.01
Running Hive query: tpch/q13_customer_distribution.hive
13/12/10 02:26:16 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 02:26:16 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 02:26:16 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 02:26:16 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 02:26:16 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 02:26:16 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 02:26:16 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.914 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.176 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.206 seconds
Total MapReduce jobs = 4
Stage-1 is selected by condition resolver.
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 21
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0153, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0153/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0153
Hadoop job information for Stage-1: number of mappers: 77; number of reducers: 21
2013-12-10 02:30:10,210 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1732.4 sec
MapReduce Total cumulative CPU time: 28 minutes 52 seconds 400 msec
Ended Job = job_1386606905013_0153
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0154, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0154/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0154
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1
2013-12-10 02:31:25,306 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 112.53 sec
MapReduce Total cumulative CPU time: 1 minutes 52 seconds 530 msec
Ended Job = job_1386606905013_0154
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0155, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0155/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0155
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-10 02:31:43,176 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.52 sec
MapReduce Total cumulative CPU time: 1 seconds 520 msec
Ended Job = job_1386606905013_0155
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0156, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0156/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0156
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-10 02:32:06,710 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 2.06 sec
MapReduce Total cumulative CPU time: 2 seconds 60 msec
Ended Job = job_1386606905013_0156
Loading data to table default.q13_customer_distribution
Table default.q13_customer_distribution stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 392, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 21   Cumulative CPU: 1732.4 sec   HDFS Read: 20257241173 HDFS Write: 333230859 SUCCESS
Job 1: Map: 6  Reduce: 1   Cumulative CPU: 112.53 sec   HDFS Read: 333236391 HDFS Write: 1054 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.52 sec   HDFS Read: 1421 HDFS Write: 1054 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 2.06 sec   HDFS Read: 1421 HDFS Write: 392 SUCCESS
Total MapReduce CPU Time Spent: 30 minutes 48 seconds 510 msec
OK
Time taken: 342.934 seconds
Time:351.73
Running Hive query: tpch/q14_promotion_effect.hive
13/12/10 02:32:08 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 02:32:08 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 02:32:08 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 02:32:08 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 02:32:08 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 02:32:08 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 02:32:08 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.889 seconds
OK
Time taken: 0.131 seconds
OK
Time taken: 0.225 seconds
OK
Time taken: 0.219 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.053 seconds
Total MapReduce jobs = 2
Stage-1 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0157, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0157/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0157
Hadoop job information for Stage-1: number of mappers: 308; number of reducers: 79
2013-12-10 02:38:33,566 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2521.41 sec
MapReduce Total cumulative CPU time: 42 minutes 1 seconds 410 msec
Ended Job = job_1386606905013_0157
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0158, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0158/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0158
Hadoop job information for Stage-2: number of mappers: 7; number of reducers: 1
2013-12-10 02:38:53,004 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.33 sec
MapReduce Total cumulative CPU time: 6 seconds 330 msec
Ended Job = job_1386606905013_0158
Loading data to table default.q14_promotion_effect
Table default.q14_promotion_effect stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 18, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 308  Reduce: 79   Cumulative CPU: 2521.41 sec   HDFS Read: 82035510322 HDFS Write: 10191 SUCCESS
Job 1: Map: 7  Reduce: 1   Cumulative CPU: 6.33 sec   HDFS Read: 28744 HDFS Write: 18 SUCCESS
Total MapReduce CPU Time Spent: 42 minutes 7 seconds 740 msec
OK
Time taken: 397.643 seconds
Time:406.30
Running Hive query: tpch/q15_top_supplier.hive
13/12/10 02:38:54 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 02:38:54 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 02:38:54 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 02:38:54 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 02:38:54 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 02:38:54 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 02:38:54 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.978 seconds
OK
Time taken: 0.088 seconds
OK
Time taken: 0.194 seconds
OK
Time taken: 0.13 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.193 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.036 seconds
OK
Time taken: 0.047 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0159, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0159/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0159
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-10 02:45:15,753 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2531.36 sec
MapReduce Total cumulative CPU time: 42 minutes 11 seconds 360 msec
Ended Job = job_1386606905013_0159
Loading data to table default.revenue
Table default.revenue stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 22450638, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 2531.36 sec   HDFS Read: 79582199808 HDFS Write: 22450638 SUCCESS
Total MapReduce CPU Time Spent: 42 minutes 11 seconds 360 msec
OK
Time taken: 373.837 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0160, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0160/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0160
Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 1
2013-12-10 02:45:37,158 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.07 sec
MapReduce Total cumulative CPU time: 17 seconds 70 msec
Ended Job = job_1386606905013_0160
Loading data to table default.max_revenue
Table default.max_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 13, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 8  Reduce: 1   Cumulative CPU: 17.07 sec   HDFS Read: 22457894 HDFS Write: 13 SUCCESS
Total MapReduce CPU Time Spent: 17 seconds 70 msec
OK
Time taken: 21.257 seconds
Total MapReduce jobs = 3
Stage-12 is selected by condition resolver.
Stage-1 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 02:45:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 02:45:40 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_02-45-37_613_246454901644673105-1/-local-10009/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 02:45:40 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_02-45-37_613_246454901644673105-1/-local-10009/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 02:45:40 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 02:45:40 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 02:45:40 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 02:45:40 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 02:45:40 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 02:45:40 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 02:45:40 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 02:45:41	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 02:45:43	Processing rows:	200000	Hashtable size:	199999	Memory usage:	61565368	percentage:	0.129
2013-12-10 02:45:44	Processing rows:	300000	Hashtable size:	299999	Memory usage:	84886744	percentage:	0.178
2013-12-10 02:45:44	Processing rows:	400000	Hashtable size:	399999	Memory usage:	105720632	percentage:	0.222
2013-12-10 02:45:44	Processing rows:	500000	Hashtable size:	499999	Memory usage:	200998640	percentage:	0.421
2013-12-10 02:45:46	Processing rows:	600000	Hashtable size:	599999	Memory usage:	168327848	percentage:	0.353
2013-12-10 02:45:47	Processing rows:	700000	Hashtable size:	699999	Memory usage:	173742912	percentage:	0.364
2013-12-10 02:45:47	Processing rows:	800000	Hashtable size:	799999	Memory usage:	223204576	percentage:	0.468
2013-12-10 02:45:48	Processing rows:	900000	Hashtable size:	899999	Memory usage:	227380768	percentage:	0.477
2013-12-10 02:45:48	Processing rows:	1000000	Hashtable size:	999999	Memory usage:	276859416	percentage:	0.58
2013-12-10 02:45:48	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_02-45-37_613_246454901644673105-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile11--.hashtable
2013-12-10 02:45:49	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_02-45-37_613_246454901644673105-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile11--.hashtable
2013-12-10 02:45:49	End of local task; Time Taken: 7.806 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0161, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0161/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0161
Hadoop job information for Stage-8: number of mappers: 1; number of reducers: 0
2013-12-10 02:46:14,262 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 16.24 sec
MapReduce Total cumulative CPU time: 16 seconds 240 msec
Ended Job = job_1386606905013_0161
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 02:46:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 02:46:16 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_02-45-37_613_246454901644673105-1/-local-10013/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 02:46:16 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_02-45-37_613_246454901644673105-1/-local-10013/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 02:46:16 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 02:46:16 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 02:46:16 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 02:46:16 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 02:46:16 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 02:46:16 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 02:46:16 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 02:46:17	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 02:46:17	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_02-45-37_613_246454901644673105-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-12-10 02:46:17	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_02-45-37_613_246454901644673105-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-12-10 02:46:17	End of local task; Time Taken: 0.533 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0162, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0162/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0162
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-10 02:46:40,241 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 5.97 sec
MapReduce Total cumulative CPU time: 5 seconds 970 msec
Ended Job = job_1386606905013_0162
Loading data to table default.q15_top_supplier
Table default.q15_top_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 77, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 16.24 sec   HDFS Read: 142874170 HDFS Write: 90810899 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 5.97 sec   HDFS Read: 90811265 HDFS Write: 77 SUCCESS
Total MapReduce CPU Time Spent: 22 seconds 210 msec
OK
Time taken: 63.065 seconds
Time:467.20
Running Hive query: tpch/q16_parts_supplier_relationship.hive
13/12/10 02:46:41 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 02:46:41 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 02:46:41 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 02:46:41 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 02:46:41 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 02:46:41 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 02:46:41 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.913 seconds
OK
Time taken: 0.096 seconds
OK
Time taken: 0.098 seconds
OK
Time taken: 0.201 seconds
OK
Time taken: 0.139 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.204 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.075 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0163, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0163/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0163
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2013-12-10 02:47:10,189 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.85 sec
MapReduce Total cumulative CPU time: 7 seconds 850 msec
Ended Job = job_1386606905013_0163
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-12-10_02-46-49_814_6243146330982105571-1/-ext-10000
Loading data to table default.supplier_tmp
Table default.supplier_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6885604, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 7.85 sec   HDFS Read: 142874170 HDFS Write: 6885604 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 850 msec
OK
Time taken: 20.954 seconds
Total MapReduce jobs = 2
Stage-3 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 15
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0164, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0164/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0164
Hadoop job information for Stage-3: number of mappers: 57; number of reducers: 15
2013-12-10 02:49:33,013 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 986.85 sec
MapReduce Total cumulative CPU time: 16 minutes 26 seconds 850 msec
Ended Job = job_1386606905013_0164
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 02:49:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 02:49:35 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_02-47-10_773_680288479074200204-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 02:49:35 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_02-47-10_773_680288479074200204-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 02:49:35 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 02:49:35 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 02:49:35 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 02:49:35 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 02:49:35 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 02:49:35 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 02:49:35 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 02:49:35	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 02:49:36	Processing rows:	200000	Hashtable size:	199999	Memory usage:	95599504	percentage:	0.20
2013-12-10 02:49:36	Processing rows:	300000	Hashtable size:	299999	Memory usage:	107382008	percentage:	0.225
2013-12-10 02:49:36	Processing rows:	400000	Hashtable size:	399999	Memory usage:	117067352	percentage:	0.245
2013-12-10 02:49:37	Processing rows:	500000	Hashtable size:	499999	Memory usage:	56447104	percentage:	0.118
2013-12-10 02:49:37	Processing rows:	600000	Hashtable size:	599999	Memory usage:	68281200	percentage:	0.143
2013-12-10 02:49:37	Processing rows:	700000	Hashtable size:	699999	Memory usage:	77830904	percentage:	0.163
2013-12-10 02:49:37	Processing rows:	800000	Hashtable size:	799999	Memory usage:	87380600	percentage:	0.183
2013-12-10 02:49:37	Processing rows:	900000	Hashtable size:	899999	Memory usage:	95020360	percentage:	0.199
2013-12-10 02:49:37	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_02-47-10_773_680288479074200204-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-12-10 02:49:38	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_02-47-10_773_680288479074200204-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-12-10 02:49:38	End of local task; Time Taken: 2.12 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0165, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0165/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0165
Hadoop job information for Stage-5: number of mappers: 16; number of reducers: 0
2013-12-10 02:51:01,514 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 498.84 sec
MapReduce Total cumulative CPU time: 8 minutes 18 seconds 840 msec
Ended Job = job_1386606905013_0165
Loading data to table default.q16_tmp
Table default.q16_tmp stats: [num_partitions: 0, num_files: 16, num_rows: 0, total_size: 2989814109, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 57  Reduce: 15   Cumulative CPU: 986.85 sec   HDFS Read: 14662902713 HDFS Write: 3937268529 SUCCESS
Job 1: Map: 16   Cumulative CPU: 498.84 sec   HDFS Read: 3937304747 HDFS Write: 2989814109 SUCCESS
Total MapReduce CPU Time Spent: 24 minutes 45 seconds 690 msec
OK
Time taken: 231.207 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0166, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0166/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0166
Hadoop job information for Stage-1: number of mappers: 12; number of reducers: 3
2013-12-10 02:52:00,955 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 302.89 sec
MapReduce Total cumulative CPU time: 5 minutes 2 seconds 890 msec
Ended Job = job_1386606905013_0166
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0167, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0167/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0167
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-12-10 02:52:24,575 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.17 sec
MapReduce Total cumulative CPU time: 8 seconds 170 msec
Ended Job = job_1386606905013_0167
Loading data to table default.q16_parts_supplier_relationship
Table default.q16_parts_supplier_relationship stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 1039440, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 12  Reduce: 3   Cumulative CPU: 302.89 sec   HDFS Read: 2989875407 HDFS Write: 1450608 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 8.17 sec   HDFS Read: 1451561 HDFS Write: 1039440 SUCCESS
Total MapReduce CPU Time Spent: 5 minutes 11 seconds 60 msec
OK
Time taken: 83.022 seconds
Time:344.31
Running Hive query: tpch/q17_small_quantity_order_revenue.hive
13/12/10 02:52:26 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 02:52:26 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 02:52:26 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 02:52:26 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 02:52:26 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 02:52:26 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 02:52:26 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.951 seconds
OK
Time taken: 0.085 seconds
OK
Time taken: 0.219 seconds
OK
Time taken: 0.147 seconds
OK
Time taken: 0.184 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.034 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0168, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0168/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0168
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-10 03:03:39,609 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6513.68 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 48 minutes 33 seconds 680 msec
Ended Job = job_1386606905013_0168
Loading data to table default.lineitem_tmp
Table default.lineitem_tmp stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 494293140, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 6513.68 sec   HDFS Read: 79582199808 HDFS Write: 494293140 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 48 minutes 33 seconds 680 msec
OK
Time taken: 666.357 seconds
Total MapReduce jobs = 5
Stage-1 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 83
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0169, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0169/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0169
Hadoop job information for Stage-1: number of mappers: 308; number of reducers: 83
2013-12-10 03:15:25,893 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6534.52 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 48 minutes 54 seconds 520 msec
Ended Job = job_1386606905013_0169
Stage-13 is filtered out by condition resolver.
Stage-14 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0170, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0170/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0170
Hadoop job information for Stage-2: number of mappers: 17; number of reducers: 1
2013-12-10 03:16:52,326 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 180.75 sec
MapReduce Total cumulative CPU time: 3 minutes 0 seconds 750 msec
Ended Job = job_1386606905013_0170
Launching Job 3 out of 5
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0171, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0171/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0171
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-10 03:17:11,386 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.88 sec
MapReduce Total cumulative CPU time: 1 seconds 880 msec
Ended Job = job_1386606905013_0171
Loading data to table default.q17_small_quantity_order_revenue
Table default.q17_small_quantity_order_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 308  Reduce: 83   Cumulative CPU: 6534.52 sec   HDFS Read: 82035510322 HDFS Write: 22558316 SUCCESS
Job 1: Map: 17  Reduce: 1   Cumulative CPU: 180.75 sec   HDFS Read: 516878752 HDFS Write: 121 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.88 sec   HDFS Read: 487 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 51 minutes 57 seconds 150 msec
OK
Time taken: 811.606 seconds
Time:1486.80
Running Hive query: tpch/q18_large_volume_customer.hive
13/12/10 03:17:12 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 03:17:12 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 03:17:12 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 03:17:12 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 03:17:12 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 03:17:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 03:17:12 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.929 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.213 seconds
OK
Time taken: 0.111 seconds
OK
Time taken: 0.174 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.042 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 69
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0172, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0172/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0172
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 69
2013-12-10 03:24:45,727 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3688.2 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 1 minutes 28 seconds 200 msec
Ended Job = job_1386606905013_0172
Loading data to table default.q18_tmp
Table default.q18_tmp stats: [num_partitions: 0, num_files: 69, num_rows: 0, total_size: 2291717657, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 69   Cumulative CPU: 3688.2 sec   HDFS Read: 79582199808 HDFS Write: 2291717657 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 1 minutes 28 seconds 200 msec
OK
Time taken: 445.504 seconds
Total MapReduce jobs = 5
Stage-5 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0173, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0173/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0173
Hadoop job information for Stage-5: number of mappers: 77; number of reducers: 18
2013-12-10 03:30:18,987 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 1932.01 sec
MapReduce Total cumulative CPU time: 32 minutes 12 seconds 10 msec
Ended Job = job_1386606905013_0173
Stage-15 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0174, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0174/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0174
Hadoop job information for Stage-1: number of mappers: 346; number of reducers: 79
2013-12-10 03:43:31,831 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6773.97 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 52 minutes 53 seconds 970 msec
Ended Job = job_1386606905013_0174
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0175, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0175/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0175
Hadoop job information for Stage-2: number of mappers: 7; number of reducers: 1
2013-12-10 03:44:01,409 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 9.95 sec
MapReduce Total cumulative CPU time: 9 seconds 950 msec
Ended Job = job_1386606905013_0175
Launching Job 4 out of 5
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0176, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0176/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0176
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-10 03:44:21,183 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.92 sec
MapReduce Total cumulative CPU time: 2 seconds 920 msec
Ended Job = job_1386606905013_0176
Loading data to table default.q18_large_volume_customer
Table default.q18_large_volume_customer stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6391, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 18   Cumulative CPU: 1932.01 sec   HDFS Read: 20257241173 HDFS Write: 9688881598 SUCCESS
Job 1: Map: 346  Reduce: 79   Cumulative CPU: 6773.97 sec   HDFS Read: 91562986372 HDFS Write: 471842 SUCCESS
Job 2: Map: 7  Reduce: 1   Cumulative CPU: 9.95 sec   HDFS Read: 490395 HDFS Write: 465114 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 2.92 sec   HDFS Read: 465481 HDFS Write: 6391 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 25 minutes 18 seconds 850 msec
OK
Time taken: 1175.332 seconds
Time:1629.81
Running Hive query: tpch/q19_discounted_revenue.hive
13/12/10 03:44:22 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 03:44:22 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 03:44:22 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 03:44:22 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 03:44:22 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 03:44:22 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 03:44:22 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.926 seconds
OK
Time taken: 0.087 seconds
OK
Time taken: 0.195 seconds
OK
Time taken: 0.229 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 2
Stage-1 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0177, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0177/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0177
Hadoop job information for Stage-1: number of mappers: 308; number of reducers: 79
2013-12-10 04:00:46,296 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9127.51 sec
MapReduce Total cumulative CPU time: 0 days 2 hours 32 minutes 7 seconds 510 msec
Ended Job = job_1386606905013_0177
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0178, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0178/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0178
Hadoop job information for Stage-2: number of mappers: 8; number of reducers: 1
2013-12-10 04:01:18,829 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.84 sec
MapReduce Total cumulative CPU time: 6 seconds 840 msec
Ended Job = job_1386606905013_0178
Loading data to table default.q19_discounted_revenue
Table default.q19_discounted_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 20, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 308  Reduce: 79   Cumulative CPU: 9127.51 sec   HDFS Read: 82035510322 HDFS Write: 9559 SUCCESS
Job 1: Map: 8  Reduce: 1   Cumulative CPU: 6.84 sec   HDFS Read: 28257 HDFS Write: 20 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 32 minutes 14 seconds 350 msec
OK
Time taken: 1008.967 seconds
Time:1017.65
Running Hive query: tpch/q20_potential_part_promotion.hive
13/12/10 04:01:20 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 04:01:20 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 04:01:20 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 04:01:20 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 04:01:20 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 04:01:20 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 04:01:20 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.912 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.089 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.199 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.098 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.198 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.06 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0179, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0179/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0179
Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 3
2013-12-10 04:02:02,612 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 76.39 sec
MapReduce Total cumulative CPU time: 1 minutes 16 seconds 390 msec
Ended Job = job_1386606905013_0179
Loading data to table default.q20_tmp1
Table default.q20_tmp1 stats: [num_partitions: 0, num_files: 3, num_rows: 0, total_size: 1834396, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 11  Reduce: 3   Cumulative CPU: 76.39 sec   HDFS Read: 2453310514 HDFS Write: 1834396 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 16 seconds 390 msec
OK
Time taken: 34.34 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0180, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0180/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0180
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-10 04:09:27,876 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3123.43 sec
MapReduce Total cumulative CPU time: 52 minutes 3 seconds 430 msec
Ended Job = job_1386606905013_0180
Loading data to table default.q20_tmp2
Table default.q20_tmp2 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 1096808654, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 3123.43 sec   HDFS Read: 79582199808 HDFS Write: 1096808654 SUCCESS
Total MapReduce CPU Time Spent: 52 minutes 3 seconds 430 msec
OK
Time taken: 445.181 seconds
Total MapReduce jobs = 4
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 04:09:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 04:09:30 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_04-09-28_311_7500964980605977516-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 04:09:30 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_04-09-28_311_7500964980605977516-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 04:09:31 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 04:09:31 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 04:09:31 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 04:09:31 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 04:09:31 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 04:09:31 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 04:09:31 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 04:09:31	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 04:09:32	Processing rows:	200000	Hashtable size:	199999	Memory usage:	42488448	percentage:	0.089
2013-12-10 04:09:32	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_04-09-28_311_7500964980605977516-1/-local-10007/HashTable-Stage-8/MapJoin-mapfile21--.hashtable
2013-12-10 04:09:33	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_04-09-28_311_7500964980605977516-1/-local-10007/HashTable-Stage-8/MapJoin-mapfile21--.hashtable
2013-12-10 04:09:33	End of local task; Time Taken: 1.328 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 4
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0181, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0181/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0181
Hadoop job information for Stage-8: number of mappers: 46; number of reducers: 0
2013-12-10 04:10:37,344 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 323.05 sec
MapReduce Total cumulative CPU time: 5 minutes 23 seconds 50 msec
Ended Job = job_1386606905013_0181
Stage-9 is filtered out by condition resolver.
Stage-10 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0182, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0182/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0182
Hadoop job information for Stage-1: number of mappers: 15; number of reducers: 2
2013-12-10 04:13:05,711 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 441.47 sec
MapReduce Total cumulative CPU time: 7 minutes 21 seconds 470 msec
Ended Job = job_1386606905013_0182
Loading data to table default.q20_tmp3
Table default.q20_tmp3 stats: [num_partitions: 0, num_files: 2, num_rows: 0, total_size: 9821178, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 46   Cumulative CPU: 323.05 sec   HDFS Read: 12209592199 HDFS Write: 24620613 SUCCESS
Job 1: Map: 15  Reduce: 2   Cumulative CPU: 441.47 sec   HDFS Read: 1121447830 HDFS Write: 9821178 SUCCESS
Total MapReduce CPU Time Spent: 12 minutes 44 seconds 520 msec
OK
Time taken: 217.82 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0183, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0183/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0183
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2013-12-10 04:13:32,239 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.57 sec
MapReduce Total cumulative CPU time: 11 seconds 570 msec
Ended Job = job_1386606905013_0183
Loading data to table default.q20_tmp4
Table default.q20_tmp4 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3082950, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 11.57 sec   HDFS Read: 9821608 HDFS Write: 3082950 SUCCESS
Total MapReduce CPU Time Spent: 11 seconds 570 msec
OK
Time taken: 26.532 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 04:13:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 04:13:34 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_04-13-32_665_8593696983112831178-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 04:13:34 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_04-13-32_665_8593696983112831178-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 04:13:34 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 04:13:34 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 04:13:34 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 04:13:34 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 04:13:34 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 04:13:34 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 04:13:34 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 04:13:35	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 04:13:36	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_04-13-32_665_8593696983112831178-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile41--.hashtable
2013-12-10 04:13:36	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_04-13-32_665_8593696983112831178-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile41--.hashtable
2013-12-10 04:13:36	Processing rows:	200000	Hashtable size:	199999	Memory usage:	85618848	percentage:	0.179
2013-12-10 04:13:36	Processing rows:	300000	Hashtable size:	299999	Memory usage:	94596008	percentage:	0.198
2013-12-10 04:13:36	Processing rows:	400000	Hashtable size:	399999	Memory usage:	107767464	percentage:	0.226
2013-12-10 04:13:37	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_04-13-32_665_8593696983112831178-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-12-10 04:13:37	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_04-13-32_665_8593696983112831178-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-12-10 04:13:37	End of local task; Time Taken: 2.044 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0184, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0184/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0184
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-10 04:14:01,651 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 9.14 sec
MapReduce Total cumulative CPU time: 9 seconds 140 msec
Ended Job = job_1386606905013_0184
Loading data to table default.q20_potential_part_promotion
Table default.q20_potential_part_promotion stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 810032, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 9.14 sec   HDFS Read: 142874170 HDFS Write: 810032 SUCCESS
Total MapReduce CPU Time Spent: 9 seconds 140 msec
OK
Time taken: 29.408 seconds
Time:762.78
Running Hive query: tpch/q21_suppliers_who_kept_orders_waiting.hive
13/12/10 04:14:03 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 04:14:03 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 04:14:03 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 04:14:03 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 04:14:03 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 04:14:03 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 04:14:03 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.957 seconds
OK
Time taken: 0.097 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.241 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.191 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0185, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0185/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0185
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-10 04:26:44,003 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6408.31 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 46 minutes 48 seconds 310 msec
Ended Job = job_1386606905013_0185
Loading data to table default.q21_tmp1
Table default.q21_tmp1 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 2819600047, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 6408.31 sec   HDFS Read: 79582199808 HDFS Write: 2819600047 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 46 minutes 48 seconds 310 msec
OK
Time taken: 753.144 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0186, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0186/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0186
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-10 04:35:44,472 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4763.78 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 19 minutes 23 seconds 780 msec
Ended Job = job_1386606905013_0186
Loading data to table default.q21_tmp2
Table default.q21_tmp2 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 2583844107, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 4763.78 sec   HDFS Read: 79582199808 HDFS Write: 2583844107 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 19 minutes 23 seconds 780 msec
OK
Time taken: 540.395 seconds
Total MapReduce jobs = 14
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 04:35:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 04:35:50 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_04-35-44_937_4542214442386221410-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 04:35:50 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_04-35-44_937_4542214442386221410-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 04:35:50 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 04:35:50 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 04:35:50 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 04:35:50 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 04:35:50 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 04:35:50 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 04:35:50 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 04:35:51	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 04:35:52	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_04-35-44_937_4542214442386221410-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-12-10 04:35:52	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_04-35-44_937_4542214442386221410-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-12-10 04:35:52	End of local task; Time Taken: 0.867 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 14
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0187, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0187/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0187
Hadoop job information for Stage-24: number of mappers: 1; number of reducers: 0
2013-12-10 04:36:14,085 Stage-24 map = 100%,  reduce = 0%, Cumulative CPU 4.19 sec
MapReduce Total cumulative CPU time: 4 seconds 190 msec
Ended Job = job_1386606905013_0187
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 14
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0188, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0188/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0188
Hadoop job information for Stage-8: number of mappers: 298; number of reducers: 80
2013-12-10 04:44:30,514 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 4050.68 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 7 minutes 30 seconds 680 msec
Ended Job = job_1386606905013_0188
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 14
Number of reduce tasks not specified. Estimated from input data size: 19
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0189, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0189/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0189
Hadoop job information for Stage-1: number of mappers: 76; number of reducers: 19
2013-12-10 04:46:37,630 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 905.48 sec
MapReduce Total cumulative CPU time: 15 minutes 5 seconds 480 msec
Ended Job = job_1386606905013_0189
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 14
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0190, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0190/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0190
Hadoop job information for Stage-2: number of mappers: 19; number of reducers: 4
2013-12-10 04:50:03,162 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 1019.36 sec
MapReduce Total cumulative CPU time: 16 minutes 59 seconds 360 msec
Ended Job = job_1386606905013_0190
Stage-25 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 5 out of 14
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0191, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0191/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0191
Hadoop job information for Stage-3: number of mappers: 16; number of reducers: 3
2013-12-10 04:53:23,674 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 985.29 sec
MapReduce Total cumulative CPU time: 16 minutes 25 seconds 290 msec
Ended Job = job_1386606905013_0191
Launching Job 6 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0192, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0192/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0192
Hadoop job information for Stage-4: number of mappers: 3; number of reducers: 1
2013-12-10 04:53:49,493 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 11.7 sec
MapReduce Total cumulative CPU time: 11 seconds 700 msec
Ended Job = job_1386606905013_0192
Launching Job 7 out of 14
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0193, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0193/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0193
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-12-10 04:54:09,577 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 4.95 sec
MapReduce Total cumulative CPU time: 4 seconds 950 msec
Ended Job = job_1386606905013_0193
Loading data to table default.q21_suppliers_who_kept_orders_waiting
Table default.q21_suppliers_who_kept_orders_waiting stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 2200, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 4.19 sec   HDFS Read: 142874170 HDFS Write: 1611510 SUCCESS
Job 1: Map: 298  Reduce: 80   Cumulative CPU: 4050.68 sec   HDFS Read: 79583811685 HDFS Write: 686901797 SUCCESS
Job 2: Map: 76  Reduce: 19   Cumulative CPU: 905.48 sec   HDFS Read: 18480595393 HDFS Write: 331675044 SUCCESS
Job 3: Map: 19  Reduce: 4   Cumulative CPU: 1019.36 sec   HDFS Read: 3151288208 HDFS Write: 319626840 SUCCESS
Job 4: Map: 16  Reduce: 3   Cumulative CPU: 985.29 sec   HDFS Read: 2903480299 HDFS Write: 4313565 SUCCESS
Job 5: Map: 3  Reduce: 1   Cumulative CPU: 11.7 sec   HDFS Read: 4314666 HDFS Write: 1492766 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 4.95 sec   HDFS Read: 1493133 HDFS Write: 2200 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 56 minutes 21 seconds 650 msec
OK
Time taken: 1105.052 seconds
Time:2407.90
Running Hive query: tpch/q22_global_sales_opportunity.hive
13/12/10 04:54:11 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 04:54:11 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 04:54:11 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 04:54:11 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 04:54:11 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 04:54:11 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 04:54:11 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.027 seconds
OK
Time taken: 0.097 seconds
OK
Time taken: 0.191 seconds
OK
Time taken: 0.148 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.195 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.052 seconds
OK
Time taken: 0.086 seconds
OK
Time taken: 0.063 seconds
OK
Time taken: 0.082 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0194, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0194/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0194
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 0
2013-12-10 04:54:49,630 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 94.7 sec
MapReduce Total cumulative CPU time: 1 minutes 34 seconds 700 msec
Ended Job = job_1386606905013_0194
Stage-4 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Stage-5 is filtered out by condition resolver.
Launching Job 3 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0195, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0195/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0195
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
2013-12-10 04:55:14,856 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 10.2 sec
MapReduce Total cumulative CPU time: 10 seconds 200 msec
Ended Job = job_1386606905013_0195
Loading data to table default.q22_customer_tmp
Table default.q22_customer_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 80028920, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10   Cumulative CPU: 94.7 sec   HDFS Read: 2463566642 HDFS Write: 80028920 SUCCESS
Job 1: Map: 1   Cumulative CPU: 10.2 sec   HDFS Read: 80030327 HDFS Write: 80028920 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 44 seconds 900 msec
OK
Time taken: 56.053 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0196, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0196/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0196
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-12-10 04:55:39,466 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.49 sec
MapReduce Total cumulative CPU time: 7 seconds 490 msec
Ended Job = job_1386606905013_0196
Loading data to table default.q22_customer_tmp1
Table default.q22_customer_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 18, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 7.49 sec   HDFS Read: 80029143 HDFS Write: 18 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 490 msec
OK
Time taken: 24.599 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0197, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0197/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0197
Hadoop job information for Stage-1: number of mappers: 67; number of reducers: 18
2013-12-10 04:57:54,396 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1023.3 sec
MapReduce Total cumulative CPU time: 17 minutes 3 seconds 300 msec
Ended Job = job_1386606905013_0197
Loading data to table default.q22_orders_tmp
Table default.q22_orders_tmp stats: [num_partitions: 0, num_files: 18, num_rows: 0, total_size: 82591212, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 67  Reduce: 18   Cumulative CPU: 1023.3 sec   HDFS Read: 17793674531 HDFS Write: 82591212 SUCCESS
Total MapReduce CPU Time Spent: 17 minutes 3 seconds 300 msec
OK
Time taken: 134.904 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0198, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0198/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0198
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 1
2013-12-10 04:59:07,916 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 108.32 sec
MapReduce Total cumulative CPU time: 1 minutes 48 seconds 320 msec
Ended Job = job_1386606905013_0198
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/10 04:59:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/10 04:59:09 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_04-57-54_858_1741358866489082459-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/10 04:59:10 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-10_04-57-54_858_1741358866489082459-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/10 04:59:10 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/10 04:59:10 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/10 04:59:10 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/10 04:59:10 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/10 04:59:10 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/10 04:59:10 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/10 04:59:10 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-10 04:59:10	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-10 04:59:11	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-10_04-57-54_858_1741358866489082459-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-12-10 04:59:11	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-10_04-57-54_858_1741358866489082459-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-12-10 04:59:11	End of local task; Time Taken: 0.529 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0199, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0199/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0199
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-10 04:59:35,365 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 7.84 sec
MapReduce Total cumulative CPU time: 7 seconds 840 msec
Ended Job = job_1386606905013_0199
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0200, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0200/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0200
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-10 04:59:53,469 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.83 sec
MapReduce Total cumulative CPU time: 1 seconds 830 msec
Ended Job = job_1386606905013_0200
Loading data to table default.q22_global_sales_opportunity
Table default.q22_global_sales_opportunity stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 202, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 7  Reduce: 1   Cumulative CPU: 108.32 sec   HDFS Read: 162622689 HDFS Write: 39621076 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 7.84 sec   HDFS Read: 39621443 HDFS Write: 320 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.83 sec   HDFS Read: 687 HDFS Write: 202 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 57 seconds 990 msec
OK
Time taken: 119.039 seconds
Time:343.90
