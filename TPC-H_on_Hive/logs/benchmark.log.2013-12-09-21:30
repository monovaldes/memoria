Running Hive from /opt/hive-0.12.0
Running Hadoop from 
Running Hive query: tpch/q1_pricing_summary_report.hive
13/12/09 15:05:36 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 15:05:36 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 15:05:36 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 15:05:36 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 15:05:36 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 15:05:36 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 15:05:36 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 8.182 seconds
OK
Time taken: 0.28 seconds
OK
Time taken: 0.234 seconds
OK
Time taken: 0.066 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0001, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0001/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0001
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-09 15:12:54,904 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3484.14 sec
MapReduce Total cumulative CPU time: 58 minutes 4 seconds 140 msec
Ended Job = job_1386606905013_0001
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0002, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0002/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0002
Hadoop job information for Stage-2: number of mappers: 9; number of reducers: 1
2013-12-09 15:13:13,559 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.68 sec
MapReduce Total cumulative CPU time: 7 seconds 680 msec
Ended Job = job_1386606905013_0002
Loading data to table default.q1_pricing_summary_report
Table default.q1_pricing_summary_report stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 585, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 3484.14 sec   HDFS Read: 79582199808 HDFS Write: 8011 SUCCESS
Job 1: Map: 9  Reduce: 1   Cumulative CPU: 7.68 sec   HDFS Read: 27076 HDFS Write: 585 SUCCESS
Total MapReduce CPU Time Spent: 58 minutes 11 seconds 820 msec
OK
Time taken: 447.768 seconds
Time:459.51
Running Hive query: tpch/q2_minimum_cost_supplier.hive
13/12/09 15:13:15 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 15:13:15 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 15:13:15 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 15:13:15 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 15:13:15 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 15:13:15 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 15:13:15 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.026 seconds
OK
Time taken: 0.198 seconds
OK
Time taken: 0.235 seconds
OK
Time taken: 0.098 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.194 seconds
OK
Time taken: 0.095 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.178 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.035 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.085 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.052 seconds
Total MapReduce jobs = 10
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 15:13:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 15:13:29 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_15-13-23_736_5913269406515008834-1/-local-10020/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 15:13:29 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_15-13-23_736_5913269406515008834-1/-local-10020/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 15:13:29 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 15:13:29 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 15:13:29 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 15:13:29 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 15:13:29 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 15:13:29 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 15:13:29 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 03:13:30	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 03:13:30	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_15-13-23_736_5913269406515008834-1/-local-10017/HashTable-Stage-18/MapJoin-mapfile61--.hashtable
2013-12-09 03:13:30	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_15-13-23_736_5913269406515008834-1/-local-10017/HashTable-Stage-18/MapJoin-mapfile61--.hashtable
2013-12-09 03:13:30	End of local task; Time Taken: 0.82 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0003, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0003/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0003
Hadoop job information for Stage-18: number of mappers: 1; number of reducers: 0
2013-12-09 15:13:43,797 Stage-18 map = 100%,  reduce = 0%, Cumulative CPU 0.93 sec
MapReduce Total cumulative CPU time: 930 msec
Ended Job = job_1386606905013_0003
Stage-23 is filtered out by condition resolver.
Stage-24 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0004, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0004/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0004
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2013-12-09 15:14:11,774 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.61 sec
MapReduce Total cumulative CPU time: 17 seconds 610 msec
Ended Job = job_1386606905013_0004
Stage-21 is filtered out by condition resolver.
Stage-22 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 3 out of 10
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0005, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0005/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0005
Hadoop job information for Stage-2: number of mappers: 47; number of reducers: 13
2013-12-09 15:16:11,764 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 754.46 sec
MapReduce Total cumulative CPU time: 12 minutes 34 seconds 460 msec
Ended Job = job_1386606905013_0005
Stage-19 is filtered out by condition resolver.
Stage-20 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 4 out of 10
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0006, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0006/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0006
Hadoop job information for Stage-3: number of mappers: 23; number of reducers: 6
2013-12-09 15:17:28,732 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 350.74 sec
MapReduce Total cumulative CPU time: 5 minutes 50 seconds 740 msec
Ended Job = job_1386606905013_0006
Loading data to table default.q2_minimum_cost_supplier_tmp1
Table default.q2_minimum_cost_supplier_tmp1 stats: [num_partitions: 0, num_files: 6, num_rows: 0, total_size: 10939639, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.93 sec   HDFS Read: 2424 HDFS Write: 231 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 17.61 sec   HDFS Read: 142874768 HDFS Write: 32875184 SUCCESS
Job 2: Map: 47  Reduce: 13   Cumulative CPU: 754.46 sec   HDFS Read: 12242467750 HDFS Write: 2763176676 SUCCESS
Job 3: Map: 23  Reduce: 6   Cumulative CPU: 350.74 sec   HDFS Read: 5216528660 HDFS Write: 10939639 SUCCESS
Total MapReduce CPU Time Spent: 18 minutes 43 seconds 740 msec
OK
Time taken: 245.474 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0007, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0007/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0007
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-12-09 15:17:50,722 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.8 sec
MapReduce Total cumulative CPU time: 11 seconds 800 msec
Ended Job = job_1386606905013_0007
Loading data to table default.q2_minimum_cost_supplier_tmp2
Table default.q2_minimum_cost_supplier_tmp2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 715857, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 11.8 sec   HDFS Read: 10940781 HDFS Write: 715857 SUCCESS
Total MapReduce CPU Time Spent: 11 seconds 800 msec
OK
Time taken: 21.931 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 15:17:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 15:17:53 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_15-17-51_143_3769195770627604036-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 15:17:53 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_15-17-51_143_3769195770627604036-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 15:17:53 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 15:17:53 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 15:17:53 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 15:17:53 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 15:17:53 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 15:17:53 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 15:17:53 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 03:17:54	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 03:17:54	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_15-17-51_143_3769195770627604036-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile71--.hashtable
2013-12-09 03:17:55	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_15-17-51_143_3769195770627604036-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile71--.hashtable
2013-12-09 03:17:55	End of local task; Time Taken: 1.025 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0008, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0008/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0008
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 1
2013-12-09 15:18:18,130 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 14.19 sec
MapReduce Total cumulative CPU time: 14 seconds 190 msec
Ended Job = job_1386606905013_0008
Loading data to table default.q2_minimum_cost_supplier
Table default.q2_minimum_cost_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16261, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 14.19 sec   HDFS Read: 10940781 HDFS Write: 16261 SUCCESS
Total MapReduce CPU Time Spent: 14 seconds 190 msec
OK
Time taken: 27.424 seconds
Time:304.47
Running Hive query: tpch/q3_shipping_priority.hive
13/12/09 15:18:19 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 15:18:19 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 15:18:19 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 15:18:19 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 15:18:19 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 15:18:19 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 15:18:19 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.138 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.258 seconds
OK
Time taken: 0.189 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.035 seconds
OK
Time taken: 0.04 seconds
Total MapReduce jobs = 6
Stage-1 is selected by condition resolver.
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0009, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0009/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0009
Hadoop job information for Stage-1: number of mappers: 78; number of reducers: 20
2013-12-09 15:20:36,011 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 947.7 sec
MapReduce Total cumulative CPU time: 15 minutes 47 seconds 700 msec
Ended Job = job_1386606905013_0009
Stage-14 is filtered out by condition resolver.
Stage-15 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0010, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0010/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0010
Hadoop job information for Stage-2: number of mappers: 306; number of reducers: 79
2013-12-09 15:37:14,578 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4278.92 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 11 minutes 18 seconds 920 msec
Ended Job = job_1386606905013_0010
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0011, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0011/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0011
Hadoop job information for Stage-3: number of mappers: 9; number of reducers: 1
2013-12-09 15:37:45,429 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 42.44 sec
MapReduce Total cumulative CPU time: 42 seconds 440 msec
Ended Job = job_1386606905013_0011
Launching Job 4 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0012, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0012/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0012
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-09 15:38:15,959 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 11.94 sec
MapReduce Total cumulative CPU time: 11 seconds 940 msec
Ended Job = job_1386606905013_0012
Loading data to table default.q3_shipping_priority
Table default.q3_shipping_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 368, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 78  Reduce: 20   Cumulative CPU: 947.7 sec   HDFS Read: 20257241310 HDFS Write: 499838524 SUCCESS
Job 1: Map: 306  Reduce: 79   Cumulative CPU: 4278.92 sec   HDFS Read: 80082044077 HDFS Write: 47949847 SUCCESS
Job 2: Map: 9  Reduce: 1   Cumulative CPU: 42.44 sec   HDFS Read: 47968690 HDFS Write: 47942799 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 11.94 sec   HDFS Read: 47943166 HDFS Write: 368 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 28 minutes 1 seconds 0 msec
OK
Time taken: 1188.741 seconds
Time:1197.83
Running Hive query: tpch/q4_order_priority.hive
13/12/09 15:38:17 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 15:38:17 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 15:38:17 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 15:38:17 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 15:38:17 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 15:38:17 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 15:38:17 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.072 seconds
OK
Time taken: 0.095 seconds
OK
Time taken: 0.279 seconds
OK
Time taken: 0.119 seconds
OK
Time taken: 0.22 seconds
OK
Time taken: 0.056 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.043 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0013, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0013/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0013
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-09 15:48:33,138 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3040.34 sec
MapReduce Total cumulative CPU time: 50 minutes 40 seconds 340 msec
Ended Job = job_1386606905013_0013
Loading data to table default.q4_order_priority_tmp
Table default.q4_order_priority_tmp stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 1350015083, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 3040.34 sec   HDFS Read: 79582199808 HDFS Write: 1350015083 SUCCESS
Total MapReduce CPU Time Spent: 50 minutes 40 seconds 340 msec
OK
Time taken: 608.265 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0014, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0014/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0014
Hadoop job information for Stage-1: number of mappers: 77; number of reducers: 20
2013-12-09 15:51:44,736 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1249.04 sec
MapReduce Total cumulative CPU time: 20 minutes 49 seconds 40 msec
Ended Job = job_1386606905013_0014
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0015, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0015/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0015
Hadoop job information for Stage-2: number of mappers: 7; number of reducers: 1
2013-12-09 15:52:02,108 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.91 sec
MapReduce Total cumulative CPU time: 4 seconds 910 msec
Ended Job = job_1386606905013_0015
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0016, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0016/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0016
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-09 15:52:20,982 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.82 sec
MapReduce Total cumulative CPU time: 1 seconds 820 msec
Ended Job = job_1386606905013_0016
Loading data to table default.q4_order_priority
Table default.q4_order_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 87, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 20   Cumulative CPU: 1249.04 sec   HDFS Read: 19143698264 HDFS Write: 4860 SUCCESS
Job 1: Map: 7  Reduce: 1   Cumulative CPU: 4.91 sec   HDFS Read: 10315 HDFS Write: 248 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.82 sec   HDFS Read: 615 HDFS Write: 87 SUCCESS
Total MapReduce CPU Time Spent: 20 minutes 55 seconds 770 msec
OK
Time taken: 227.671 seconds
Time:844.96
Running Hive query: tpch/q5_local_supplier_volume.hive
13/12/09 15:52:22 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 15:52:22 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 15:52:22 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 15:52:22 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 15:52:22 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 15:52:22 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 15:52:22 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.936 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.196 seconds
OK
Time taken: 0.19 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.068 seconds
OK
Time taken: 0.082 seconds
OK
Time taken: 0.057 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 15:52:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 15:52:37 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_15-52-30_603_2661352435623089088-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 15:52:37 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_15-52-30_603_2661352435623089088-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 15:52:37 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 15:52:37 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 15:52:37 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 15:52:37 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 15:52:37 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 15:52:37 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 15:52:37 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 03:52:38	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 03:52:39	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_15-52-30_603_2661352435623089088-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile81--.hashtable
2013-12-09 03:52:39	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_15-52-30_603_2661352435623089088-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile81--.hashtable
2013-12-09 03:52:39	End of local task; Time Taken: 0.821 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0017, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0017/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0017
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-12-09 15:52:52,573 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 0.95 sec
MapReduce Total cumulative CPU time: 950 msec
Ended Job = job_1386606905013_0017
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0018, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0018/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0018
Hadoop job information for Stage-7: number of mappers: 2; number of reducers: 1
2013-12-09 15:53:15,616 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 11.77 sec
MapReduce Total cumulative CPU time: 11 seconds 770 msec
Ended Job = job_1386606905013_0018
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0019, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0019/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0019
Hadoop job information for Stage-8: number of mappers: 298; number of reducers: 80
2013-12-09 16:05:21,851 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 6408.2 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 46 minutes 48 seconds 200 msec
Ended Job = job_1386606905013_0019
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 24
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0020, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0020/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0020
Hadoop job information for Stage-1: number of mappers: 87; number of reducers: 24
2013-12-09 16:09:10,616 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1761.65 sec
MapReduce Total cumulative CPU time: 29 minutes 21 seconds 650 msec
Ended Job = job_1386606905013_0020
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0021, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0021/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0021
Hadoop job information for Stage-2: number of mappers: 18; number of reducers: 4
2013-12-09 16:10:24,872 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 306.69 sec
MapReduce Total cumulative CPU time: 5 minutes 6 seconds 690 msec
Ended Job = job_1386606905013_0021
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0022, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0022/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0022
Hadoop job information for Stage-3: number of mappers: 4; number of reducers: 1
2013-12-09 16:10:44,862 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 3.18 sec
MapReduce Total cumulative CPU time: 3 seconds 180 msec
Ended Job = job_1386606905013_0022
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0023, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0023/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0023
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-09 16:11:02,891 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.4 sec
MapReduce Total cumulative CPU time: 1 seconds 400 msec
Ended Job = job_1386606905013_0023
Loading data to table default.q5_local_supplier_volume
Table default.q5_local_supplier_volume stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 137, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.95 sec   HDFS Read: 2424 HDFS Write: 222 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 11.77 sec   HDFS Read: 142874759 HDFS Write: 5881000 SUCCESS
Job 2: Map: 298  Reduce: 80   Cumulative CPU: 6408.2 sec   HDFS Read: 79588081175 HDFS Write: 5591627589 SUCCESS
Job 3: Map: 87  Reduce: 24   Cumulative CPU: 1761.65 sec   HDFS Read: 23385322780 HDFS Write: 830634778 SUCCESS
Job 4: Map: 18  Reduce: 4   Cumulative CPU: 306.69 sec   HDFS Read: 3294207900 HDFS Write: 1028 SUCCESS
Job 5: Map: 4  Reduce: 1   Cumulative CPU: 3.18 sec   HDFS Read: 2496 HDFS Write: 257 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.4 sec   HDFS Read: 624 HDFS Write: 137 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 21 minutes 33 seconds 840 msec
OK
Time taken: 1112.769 seconds
Time:1121.98
Running Hive query: tpch/q6_forecast_revenue_change.hive
13/12/09 16:11:04 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 16:11:04 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 16:11:04 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 16:11:04 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 16:11:04 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 16:11:04 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 16:11:04 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.044 seconds
OK
Time taken: 0.189 seconds
OK
Time taken: 0.249 seconds
OK
Time taken: 0.047 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0024, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0024/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0024
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 1
2013-12-09 16:15:38,855 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2031.6 sec
MapReduce Total cumulative CPU time: 33 minutes 51 seconds 600 msec
Ended Job = job_1386606905013_0024
Loading data to table default.q6_forecast_revenue_change
Table default.q6_forecast_revenue_change stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 22, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 1   Cumulative CPU: 2031.6 sec   HDFS Read: 79582199808 HDFS Write: 22 SUCCESS
Total MapReduce CPU Time Spent: 33 minutes 51 seconds 600 msec
OK
Time taken: 267.427 seconds
Time:276.02
Running Hive query: tpch/q7_volume_shipping.hive
13/12/09 16:15:40 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 16:15:40 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 16:15:40 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 16:15:40 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 16:15:40 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 16:15:40 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 16:15:40 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.095 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.118 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.32 seconds
OK
Time taken: 0.12 seconds
OK
Time taken: 0.209 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.08 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 3
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 16:15:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 16:15:51 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_16-15-48_970_8333642862193806445-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 16:15:51 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_16-15-48_970_8333642862193806445-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 16:15:51 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 16:15:51 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 16:15:51 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 16:15:51 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 16:15:51 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 16:15:51 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 16:15:51 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 04:15:52	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 04:15:53	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_16-15-48_970_8333642862193806445-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-12-09 04:15:53	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_16-15-48_970_8333642862193806445-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-12-09 04:15:53	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_16-15-48_970_8333642862193806445-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-12-09 04:15:53	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_16-15-48_970_8333642862193806445-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-12-09 04:15:53	End of local task; Time Taken: 0.843 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0025, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0025/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0025
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 0
2013-12-09 16:16:07,338 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.45 sec
MapReduce Total cumulative CPU time: 1 seconds 450 msec
Ended Job = job_1386606905013_0025
Stage-5 is selected by condition resolver.
Stage-4 is filtered out by condition resolver.
Stage-6 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-12-09_16-15-48_970_8333642862193806445-1/-ext-10000
Loading data to table default.q7_volume_shipping_tmp
Table default.q7_volume_shipping_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 38, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.45 sec   HDFS Read: 2424 HDFS Write: 38 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 450 msec
OK
Time taken: 18.84 seconds
Total MapReduce jobs = 9
Stage-6 is selected by condition resolver.
Launching Job 1 out of 9
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0026, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0026/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0026
Hadoop job information for Stage-6: number of mappers: 364; number of reducers: 80
2013-12-09 16:27:29,684 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 4889.3 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 21 minutes 29 seconds 300 msec
Ended Job = job_1386606905013_0026
Stage-24 is filtered out by condition resolver.
Stage-25 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 9
Number of reduce tasks not specified. Estimated from input data size: 10
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0027, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0027/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0027
Hadoop job information for Stage-7: number of mappers: 40; number of reducers: 10
2013-12-09 16:32:07,415 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 2079.08 sec
MapReduce Total cumulative CPU time: 34 minutes 39 seconds 80 msec
Ended Job = job_1386606905013_0027
Stage-22 is filtered out by condition resolver.
Stage-23 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 9
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0028, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0028/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0028
Hadoop job information for Stage-1: number of mappers: 33; number of reducers: 8
2013-12-09 16:38:32,630 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1948.36 sec
MapReduce Total cumulative CPU time: 32 minutes 28 seconds 360 msec
Ended Job = job_1386606905013_0028
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 16:38:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 16:38:34 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_16-16-07_813_8584495837600725478-1/-local-10024/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 16:38:34 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_16-16-07_813_8584495837600725478-1/-local-10024/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 16:38:34 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 16:38:34 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 16:38:34 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 16:38:34 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 16:38:34 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 16:38:34 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 16:38:34 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 04:38:35	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 04:38:36	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_16-16-07_813_8584495837600725478-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-12-09 04:38:36	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_16-16-07_813_8584495837600725478-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-12-09 04:38:36	End of local task; Time Taken: 0.57 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 4 out of 9
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0029, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0029/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0029
Hadoop job information for Stage-3: number of mappers: 29; number of reducers: 7
2013-12-09 16:39:38,407 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 428.41 sec
MapReduce Total cumulative CPU time: 7 minutes 8 seconds 410 msec
Ended Job = job_1386606905013_0029
Launching Job 5 out of 9
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0030, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0030/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0030
Hadoop job information for Stage-4: number of mappers: 5; number of reducers: 1
2013-12-09 16:39:56,262 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.73 sec
MapReduce Total cumulative CPU time: 3 seconds 730 msec
Ended Job = job_1386606905013_0030
Loading data to table default.q7_volume_shipping
Table default.q7_volume_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 158, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 364  Reduce: 80   Cumulative CPU: 4889.3 sec   HDFS Read: 97375874339 HDFS Write: 9560339811 SUCCESS
Job 1: Map: 40  Reduce: 10   Cumulative CPU: 2079.08 sec   HDFS Read: 12024017586 HDFS Write: 9009669053 SUCCESS
Job 2: Map: 33  Reduce: 8   Cumulative CPU: 1948.36 sec   HDFS Read: 9152698218 HDFS Write: 8468719588 SUCCESS
Job 3: Map: 29  Reduce: 7   Cumulative CPU: 428.41 sec   HDFS Read: 8468877441 HDFS Write: 844 SUCCESS
Job 4: Map: 5  Reduce: 1   Cumulative CPU: 3.73 sec   HDFS Read: 3123 HDFS Write: 158 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 35 minutes 48 seconds 880 msec
OK
Time taken: 1428.884 seconds
Time:1457.27
Running Hive query: tpch/q8_national_market_share.hive
13/12/09 16:39:57 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 16:39:57 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 16:39:57 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 16:39:57 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 16:39:57 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 16:39:57 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 16:39:57 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.113 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.133 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.158 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.092 seconds
OK
Time taken: 0.172 seconds
OK
Time taken: 0.187 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.074 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 18
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 16:40:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 16:40:14 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_16-40-06_331_7979513683913329145-1/-local-10035/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 16:40:14 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_16-40-06_331_7979513683913329145-1/-local-10035/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 16:40:14 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 16:40:14 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 16:40:14 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 16:40:14 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 16:40:14 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 16:40:14 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 16:40:14 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 04:40:15	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 04:40:16	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_16-40-06_331_7979513683913329145-1/-local-10032/HashTable-Stage-32/MapJoin-mapfile111--.hashtable
2013-12-09 04:40:16	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_16-40-06_331_7979513683913329145-1/-local-10032/HashTable-Stage-32/MapJoin-mapfile111--.hashtable
2013-12-09 04:40:16	End of local task; Time Taken: 0.846 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 18
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0031, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0031/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0031
Hadoop job information for Stage-32: number of mappers: 1; number of reducers: 0
2013-12-09 16:40:30,049 Stage-32 map = 100%,  reduce = 0%, Cumulative CPU 0.97 sec
MapReduce Total cumulative CPU time: 970 msec
Ended Job = job_1386606905013_0031
Stage-42 is filtered out by condition resolver.
Stage-43 is filtered out by condition resolver.
Stage-9 is selected by condition resolver.
Launching Job 2 out of 18
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0032, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0032/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0032
Hadoop job information for Stage-9: number of mappers: 12; number of reducers: 3
2013-12-09 16:41:18,215 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 121.39 sec
MapReduce Total cumulative CPU time: 2 minutes 1 seconds 390 msec
Ended Job = job_1386606905013_0032
Stage-40 is filtered out by condition resolver.
Stage-41 is filtered out by condition resolver.
Stage-10 is selected by condition resolver.
Launching Job 3 out of 18
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0033, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0033/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0033
Hadoop job information for Stage-10: number of mappers: 70; number of reducers: 18
2013-12-09 16:43:22,461 Stage-10 map = 100%,  reduce = 100%, Cumulative CPU 761.63 sec
MapReduce Total cumulative CPU time: 12 minutes 41 seconds 630 msec
Ended Job = job_1386606905013_0033
Stage-38 is filtered out by condition resolver.
Stage-39 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 18
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0034, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0034/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0034
Hadoop job information for Stage-1: number of mappers: 303; number of reducers: 80
2013-12-09 17:08:41,211 Stage-1 map = 98%,  reduce = 48%
Ended Job = job_1386606905013_0034 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1386606905013_0034_m_000297 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000001 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000017 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000030 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000024 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000057 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000059 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000088 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000070 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000092 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000061 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000095 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000130 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000149 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000167 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000142 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000144 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000191 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000198 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000186 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000223 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000200 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000209 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000216 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000225 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000233 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000241 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000260 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000268 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000284 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000295 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_r_000011 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_r_000029 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_r_000044 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_r_000033 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_r_000068 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_r_000059 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_r_000069 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_r_000023 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_r_000050 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_r_000058 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_r_000037 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000025 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000072 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_m_000184 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_r_000061 (and more) from job job_1386606905013_0034
Examining task ID: task_1386606905013_0034_r_000065 (and more) from job job_1386606905013_0034

Task with the most failures(4): 
-----
Task ID:
  task_1386606905013_0034_r_000023

URL:
  http://hadoop11:8088/taskdetails.jsp?jobid=job_1386606905013_0034&tipid=task_1386606905013_0034_r_000023
-----
Diagnostic Messages for this Task:


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.97 sec   HDFS Read: 2424 HDFS Write: 186 SUCCESS
Job 1: Map: 12  Reduce: 3   Cumulative CPU: 121.39 sec   HDFS Read: 2463567332 HDFS Write: 63652437 SUCCESS
Job 2: Map: 70  Reduce: 18   Cumulative CPU: 761.63 sec   HDFS Read: 17857328069 HDFS Write: 303326332 SUCCESS
Job 3: Map: 303  Reduce: 80   FAIL
Total MapReduce CPU Time Spent: 14 minutes 43 seconds 989 msec
Command exited with non-zero status 2
Time:1724.86
Running Hive query: tpch/q9_product_type_profit.hive
13/12/09 17:08:42 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 17:08:42 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 17:08:42 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 17:08:42 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 17:08:42 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 17:08:42 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 17:08:42 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.007 seconds
OK
Time taken: 0.097 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.081 seconds
OK
Time taken: 0.197 seconds
OK
Time taken: 0.17 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.034 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 17:08:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 17:08:57 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_17-08-50_890_6598700145275431481-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 17:08:57 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_17-08-50_890_6598700145275431481-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 17:08:57 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 17:08:57 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 17:08:57 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 17:08:57 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 17:08:57 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 17:08:57 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 17:08:57 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 05:08:58	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 05:08:59	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_17-08-50_890_6598700145275431481-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-12-09 05:08:59	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_17-08-50_890_6598700145275431481-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-12-09 05:08:59	End of local task; Time Taken: 0.558 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0035, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0035/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0035
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-12-09 17:09:17,969 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 6.65 sec
MapReduce Total cumulative CPU time: 6 seconds 650 msec
Ended Job = job_1386606905013_0035
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 78
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0036, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0036/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0036
Hadoop job information for Stage-8: number of mappers: 298; number of reducers: 78
2013-12-09 17:46:05,522 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 9013.52 sec
MapReduce Total cumulative CPU time: 0 days 2 hours 30 minutes 13 seconds 520 msec
Ended Job = job_1386606905013_0036
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0037, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0037/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0037
Hadoop job information for Stage-1: number of mappers: 51; number of reducers: 13
2013-12-09 17:48:48,858 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 933.94 sec
MapReduce Total cumulative CPU time: 15 minutes 33 seconds 940 msec
Ended Job = job_1386606905013_0037
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0038, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0038/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0038
Hadoop job information for Stage-2: number of mappers: 15; number of reducers: 4
2013-12-09 17:49:54,487 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 237.97 sec
MapReduce Total cumulative CPU time: 3 minutes 57 seconds 970 msec
Ended Job = job_1386606905013_0038
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0039, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0039/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0039
Hadoop job information for Stage-3: number of mappers: 70; number of reducers: 18
2013-12-09 17:52:54,167 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1314.84 sec
MapReduce Total cumulative CPU time: 21 minutes 54 seconds 840 msec
Ended Job = job_1386606905013_0039
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0040, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0040/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0040
Hadoop job information for Stage-4: number of mappers: 6; number of reducers: 1
2013-12-09 17:53:21,206 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 5.05 sec
MapReduce Total cumulative CPU time: 5 seconds 50 msec
Ended Job = job_1386606905013_0040
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0041, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0041/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0041
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-12-09 17:53:44,343 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 1.44 sec
MapReduce Total cumulative CPU time: 1 seconds 440 msec
Ended Job = job_1386606905013_0041
Loading data to table default.q9_product_type_profit
Table default.q9_product_type_profit stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 5691, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 6.65 sec   HDFS Read: 142874170 HDFS Write: 29304904 SUCCESS
Job 1: Map: 298  Reduce: 78   Cumulative CPU: 9013.52 sec   HDFS Read: 79611505079 HDFS Write: 37657254857 SUCCESS
Job 2: Map: 51  Reduce: 13   Cumulative CPU: 933.94 sec   HDFS Read: 13176121746 HDFS Write: 1029708936 SUCCESS
Job 3: Map: 15  Reduce: 4   Cumulative CPU: 237.97 sec   HDFS Read: 3483030655 HDFS Write: 52158864 SUCCESS
Job 4: Map: 70  Reduce: 18   Cumulative CPU: 1314.84 sec   HDFS Read: 17845834718 HDFS Write: 116460 SUCCESS
Job 5: Map: 6  Reduce: 1   Cumulative CPU: 5.05 sec   HDFS Read: 121326 HDFS Write: 6470 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.44 sec   HDFS Read: 6837 HDFS Write: 5691 SUCCESS
Total MapReduce CPU Time Spent: 0 days 3 hours 11 minutes 53 seconds 410 msec
OK
Time taken: 2693.933 seconds
Time:2703.26
Running Hive query: tpch/q10_returned_item.hive
13/12/09 17:53:46 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 17:53:46 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 17:53:46 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 17:53:46 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 17:53:46 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 17:53:46 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 17:53:46 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.991 seconds
OK
Time taken: 0.089 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.256 seconds
OK
Time taken: 0.221 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.038 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.074 seconds
Total MapReduce jobs = 7
Stage-1 is selected by condition resolver.
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0042, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0042/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0042
Hadoop job information for Stage-1: number of mappers: 77; number of reducers: 20
2013-12-09 17:56:17,339 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 813.79 sec
MapReduce Total cumulative CPU time: 13 minutes 33 seconds 790 msec
Ended Job = job_1386606905013_0042
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 17:56:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 17:56:19 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_17-53-53_955_2486311362607229454-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 17:56:19 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_17-53-53_955_2486311362607229454-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 17:56:19 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 17:56:19 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 17:56:19 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 17:56:19 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 17:56:19 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 17:56:19 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 17:56:19 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 05:56:20	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 05:56:20	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_17-53-53_955_2486311362607229454-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-12-09 05:56:20	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_17-53-53_955_2486311362607229454-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-12-09 05:56:20	End of local task; Time Taken: 0.579 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 7
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0043, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0043/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0043
Hadoop job information for Stage-13: number of mappers: 6; number of reducers: 0
2013-12-09 17:57:01,683 Stage-13 map = 100%,  reduce = 0%, Cumulative CPU 65.92 sec
MapReduce Total cumulative CPU time: 1 minutes 5 seconds 920 msec
Ended Job = job_1386606905013_0043
Stage-17 is filtered out by condition resolver.
Stage-18 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0044, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0044/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0044
Hadoop job information for Stage-3: number of mappers: 304; number of reducers: 79
2013-12-09 18:06:31,579 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 3257.79 sec
MapReduce Total cumulative CPU time: 54 minutes 17 seconds 790 msec
Ended Job = job_1386606905013_0044
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0045, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0045/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0045
Hadoop job information for Stage-4: number of mappers: 8; number of reducers: 1
2013-12-09 18:07:54,540 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 127.22 sec
MapReduce Total cumulative CPU time: 2 minutes 7 seconds 220 msec
Ended Job = job_1386606905013_0045
Launching Job 5 out of 7
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0046, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0046/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0046
Hadoop job information for Stage-5: number of mappers: 4; number of reducers: 1
2013-12-09 18:08:44,277 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 60.32 sec
MapReduce Total cumulative CPU time: 1 minutes 0 seconds 320 msec
Ended Job = job_1386606905013_0046
Loading data to table default.q10_returned_item
Table default.q10_returned_item stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3579, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 20   Cumulative CPU: 813.79 sec   HDFS Read: 20257241173 HDFS Write: 980369893 SUCCESS
Job 1: Map: 6   Cumulative CPU: 65.92 sec   HDFS Read: 980375203 HDFS Write: 1021319276 SUCCESS
Job 2: Map: 304  Reduce: 79   Cumulative CPU: 3257.79 sec   HDFS Read: 80603533094 HDFS Write: 889025471 SUCCESS
Job 3: Map: 8  Reduce: 1   Cumulative CPU: 127.22 sec   HDFS Read: 889044169 HDFS Write: 704132397 SUCCESS
Job 4: Map: 4  Reduce: 1   Cumulative CPU: 60.32 sec   HDFS Read: 704143440 HDFS Write: 3579 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 12 minutes 5 seconds 40 msec
OK
Time taken: 890.775 seconds
Time:899.90
Running Hive query: tpch/q11_important_stock.hive
13/12/09 18:08:45 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 18:08:45 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 18:08:45 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 18:08:45 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 18:08:45 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 18:08:45 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 18:08:45 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.865 seconds
OK
Time taken: 0.092 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.19 seconds
OK
Time taken: 0.14 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.19 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.084 seconds
Total MapReduce jobs = 5
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 18:08:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 18:08:57 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_18-08-53_782_7955561887067686638-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 18:08:57 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_18-08-53_782_7955561887067686638-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 18:08:57 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 18:08:57 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 18:08:57 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 18:08:57 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 18:08:57 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 18:08:57 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 18:08:57 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 06:08:58	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 06:08:59	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_18-08-53_782_7955561887067686638-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-12-09 06:08:59	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_18-08-53_782_7955561887067686638-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-12-09 06:08:59	End of local task; Time Taken: 0.815 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0047, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0047/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0047
Hadoop job information for Stage-10: number of mappers: 1; number of reducers: 0
2013-12-09 18:09:15,868 Stage-10 map = 100%,  reduce = 0%, Cumulative CPU 3.82 sec
MapReduce Total cumulative CPU time: 3 seconds 820 msec
Ended Job = job_1386606905013_0047
Stage-11 is filtered out by condition resolver.
Stage-12 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0048, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0048/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0048
Hadoop job information for Stage-2: number of mappers: 47; number of reducers: 13
2013-12-09 18:11:19,867 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 736.38 sec
MapReduce Total cumulative CPU time: 12 minutes 16 seconds 380 msec
Ended Job = job_1386606905013_0048
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0049, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0049/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0049
Hadoop job information for Stage-3: number of mappers: 6; number of reducers: 1
2013-12-09 18:12:00,293 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 46.81 sec
MapReduce Total cumulative CPU time: 46 seconds 810 msec
Ended Job = job_1386606905013_0049
Loading data to table default.q11_part_tmp
Table default.q11_part_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 63173975, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 3.82 sec   HDFS Read: 142874170 HDFS Write: 846716 SUCCESS
Job 1: Map: 47  Reduce: 13   Cumulative CPU: 736.38 sec   HDFS Read: 12210439282 HDFS Write: 94289297 SUCCESS
Job 2: Map: 6  Reduce: 1   Cumulative CPU: 46.81 sec   HDFS Read: 94293053 HDFS Write: 63173975 SUCCESS
Total MapReduce CPU Time Spent: 13 minutes 7 seconds 10 msec
OK
Time taken: 186.965 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0050, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0050/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0050
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-12-09 18:12:23,506 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.07 sec
MapReduce Total cumulative CPU time: 6 seconds 70 msec
Ended Job = job_1386606905013_0050
Loading data to table default.q11_sum_tmp
Table default.q11_sum_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 6.07 sec   HDFS Read: 63174194 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 6 seconds 70 msec
OK
Time taken: 23.207 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 18:12:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 18:12:25 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_18-12-23_954_2081023684476261265-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 18:12:26 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_18-12-23_954_2081023684476261265-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 18:12:26 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 18:12:26 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 18:12:26 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 18:12:26 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 18:12:26 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 18:12:26 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 18:12:26 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 06:12:26	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 06:12:27	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_18-12-23_954_2081023684476261265-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-12-09 06:12:27	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_18-12-23_954_2081023684476261265-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-12-09 06:12:27	End of local task; Time Taken: 0.573 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0051, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0051/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0051
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-12-09 18:12:57,591 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.66 sec
MapReduce Total cumulative CPU time: 8 seconds 660 msec
Ended Job = job_1386606905013_0051
Loading data to table default.q11_important_stock
Table default.q11_important_stock stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 0, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 8.66 sec   HDFS Read: 63174194 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 660 msec
OK
Time taken: 34.083 seconds
Time:253.30
Running Hive query: tpch/q12_shipping.hive
13/12/09 18:12:59 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 18:12:59 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 18:12:59 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 18:12:59 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 18:12:59 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 18:12:59 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 18:12:59 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.154 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.241 seconds
OK
Time taken: 0.184 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.056 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0052, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0052/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0052
Hadoop job information for Stage-1: number of mappers: 364; number of reducers: 80
2013-12-09 18:23:43,896 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3615.07 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 0 minutes 15 seconds 70 msec
Ended Job = job_1386606905013_0052
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0053, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0053/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0053
Hadoop job information for Stage-2: number of mappers: 7; number of reducers: 1
2013-12-09 18:24:03,193 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.68 sec
MapReduce Total cumulative CPU time: 6 seconds 680 msec
Ended Job = job_1386606905013_0053
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0054, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0054/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0054
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-09 18:24:20,810 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.87 sec
MapReduce Total cumulative CPU time: 1 seconds 870 msec
Ended Job = job_1386606905013_0054
Loading data to table default.q12_shipping
Table default.q12_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 46, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 364  Reduce: 80   Cumulative CPU: 3615.07 sec   HDFS Read: 97375874339 HDFS Write: 9920 SUCCESS
Job 1: Map: 7  Reduce: 1   Cumulative CPU: 6.68 sec   HDFS Read: 28695 HDFS Write: 156 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.87 sec   HDFS Read: 523 HDFS Write: 46 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 0 minutes 23 seconds 620 msec
OK
Time taken: 674.347 seconds
Time:683.22
Running Hive query: tpch/q13_customer_distribution.hive
13/12/09 18:24:22 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 18:24:22 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 18:24:22 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 18:24:22 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 18:24:22 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 18:24:22 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 18:24:22 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.917 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.215 seconds
OK
Time taken: 0.2 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.057 seconds
Total MapReduce jobs = 4
Stage-1 is selected by condition resolver.
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 21
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0055, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0055/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0055
Hadoop job information for Stage-1: number of mappers: 77; number of reducers: 21
2013-12-09 18:28:20,019 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1697.65 sec
MapReduce Total cumulative CPU time: 28 minutes 17 seconds 650 msec
Ended Job = job_1386606905013_0055
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0056, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0056/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0056
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1
2013-12-09 18:29:36,736 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 113.7 sec
MapReduce Total cumulative CPU time: 1 minutes 53 seconds 700 msec
Ended Job = job_1386606905013_0056
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0057, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0057/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0057
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-09 18:29:54,739 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.61 sec
MapReduce Total cumulative CPU time: 1 seconds 610 msec
Ended Job = job_1386606905013_0057
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0058, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0058/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0058
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-09 18:30:14,732 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.94 sec
MapReduce Total cumulative CPU time: 1 seconds 940 msec
Ended Job = job_1386606905013_0058
Loading data to table default.q13_customer_distribution
Table default.q13_customer_distribution stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 392, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 21   Cumulative CPU: 1697.65 sec   HDFS Read: 20257241173 HDFS Write: 333230859 SUCCESS
Job 1: Map: 6  Reduce: 1   Cumulative CPU: 113.7 sec   HDFS Read: 333236391 HDFS Write: 1054 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.61 sec   HDFS Read: 1421 HDFS Write: 1054 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 1.94 sec   HDFS Read: 1421 HDFS Write: 392 SUCCESS
Total MapReduce CPU Time Spent: 30 minutes 14 seconds 900 msec
OK
Time taken: 345.247 seconds
Time:353.91
Running Hive query: tpch/q14_promotion_effect.hive
13/12/09 18:30:16 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 18:30:16 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 18:30:16 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 18:30:16 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 18:30:16 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 18:30:16 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 18:30:16 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.087 seconds
OK
Time taken: 0.096 seconds
OK
Time taken: 0.208 seconds
OK
Time taken: 0.21 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.04 seconds
Total MapReduce jobs = 2
Stage-1 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0059, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0059/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0059
Hadoop job information for Stage-1: number of mappers: 307; number of reducers: 79
2013-12-09 18:38:17,114 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2503.2 sec
MapReduce Total cumulative CPU time: 41 minutes 43 seconds 200 msec
Ended Job = job_1386606905013_0059
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0060, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0060/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0060
Hadoop job information for Stage-2: number of mappers: 8; number of reducers: 1
2013-12-09 18:38:36,508 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.15 sec
MapReduce Total cumulative CPU time: 7 seconds 150 msec
Ended Job = job_1386606905013_0060
Loading data to table default.q14_promotion_effect
Table default.q14_promotion_effect stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 18, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 307  Reduce: 79   Cumulative CPU: 2503.2 sec   HDFS Read: 82035510185 HDFS Write: 10191 SUCCESS
Job 1: Map: 8  Reduce: 1   Cumulative CPU: 7.15 sec   HDFS Read: 28889 HDFS Write: 18 SUCCESS
Total MapReduce CPU Time Spent: 41 minutes 50 seconds 350 msec
OK
Time taken: 492.933 seconds
Time:501.77
Running Hive query: tpch/q15_top_supplier.hive
13/12/09 18:38:38 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 18:38:38 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 18:38:38 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 18:38:38 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 18:38:38 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 18:38:38 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 18:38:38 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.993 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.209 seconds
OK
Time taken: 0.12 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.181 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.043 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0061, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0061/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0061
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-09 18:46:50,183 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2572.92 sec
MapReduce Total cumulative CPU time: 42 minutes 52 seconds 920 msec
Ended Job = job_1386606905013_0061
Loading data to table default.revenue
Table default.revenue stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 22452781, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 2572.92 sec   HDFS Read: 79582199808 HDFS Write: 22452781 SUCCESS
Total MapReduce CPU Time Spent: 42 minutes 52 seconds 920 msec
OK
Time taken: 484.707 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0062, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0062/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0062
Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 1
2013-12-09 18:47:12,113 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 18.64 sec
MapReduce Total cumulative CPU time: 18 seconds 640 msec
Ended Job = job_1386606905013_0062
Loading data to table default.max_revenue
Table default.max_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 19, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 8  Reduce: 1   Cumulative CPU: 18.64 sec   HDFS Read: 22460037 HDFS Write: 19 SUCCESS
Total MapReduce CPU Time Spent: 18 seconds 640 msec
OK
Time taken: 21.883 seconds
Total MapReduce jobs = 3
Stage-12 is selected by condition resolver.
Stage-1 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 18:47:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 18:47:15 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_18-47-12_586_6896802578879211390-1/-local-10009/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 18:47:15 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_18-47-12_586_6896802578879211390-1/-local-10009/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 18:47:15 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 18:47:15 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 18:47:15 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 18:47:15 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 18:47:15 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 18:47:15 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 18:47:15 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 06:47:16	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 06:47:18	Processing rows:	200000	Hashtable size:	199999	Memory usage:	139335952	percentage:	0.292
2013-12-09 06:47:18	Processing rows:	300000	Hashtable size:	299999	Memory usage:	140763240	percentage:	0.295
2013-12-09 06:47:19	Processing rows:	400000	Hashtable size:	399999	Memory usage:	155426456	percentage:	0.326
2013-12-09 06:47:20	Processing rows:	500000	Hashtable size:	499999	Memory usage:	177069280	percentage:	0.371
2013-12-09 06:47:20	Processing rows:	600000	Hashtable size:	599999	Memory usage:	199319640	percentage:	0.418
2013-12-09 06:47:21	Processing rows:	700000	Hashtable size:	699999	Memory usage:	183728792	percentage:	0.385
2013-12-09 06:47:23	Processing rows:	800000	Hashtable size:	799999	Memory usage:	233001856	percentage:	0.488
2013-12-09 06:47:23	Processing rows:	900000	Hashtable size:	899999	Memory usage:	237311432	percentage:	0.497
2013-12-09 06:47:25	Processing rows:	1000000	Hashtable size:	999999	Memory usage:	242409800	percentage:	0.508
2013-12-09 06:47:25	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_18-47-12_586_6896802578879211390-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile11--.hashtable
2013-12-09 06:47:25	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_18-47-12_586_6896802578879211390-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile11--.hashtable
2013-12-09 06:47:25	End of local task; Time Taken: 9.292 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0063, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0063/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0063
Hadoop job information for Stage-8: number of mappers: 1; number of reducers: 0
2013-12-09 18:47:52,693 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 16.38 sec
MapReduce Total cumulative CPU time: 16 seconds 380 msec
Ended Job = job_1386606905013_0063
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 18:47:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 18:47:54 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_18-47-12_586_6896802578879211390-1/-local-10013/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 18:47:54 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_18-47-12_586_6896802578879211390-1/-local-10013/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 18:47:54 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 18:47:54 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 18:47:54 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 18:47:54 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 18:47:54 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 18:47:54 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 18:47:54 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 06:47:55	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 06:47:56	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_18-47-12_586_6896802578879211390-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-12-09 06:47:56	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_18-47-12_586_6896802578879211390-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-12-09 06:47:56	End of local task; Time Taken: 0.558 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0064, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0064/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0064
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-09 18:48:18,991 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 6.06 sec
MapReduce Total cumulative CPU time: 6 seconds 60 msec
Ended Job = job_1386606905013_0064
Loading data to table default.q15_top_supplier
Table default.q15_top_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 83, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 16.38 sec   HDFS Read: 142874170 HDFS Write: 90810899 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 6.06 sec   HDFS Read: 90811266 HDFS Write: 83 SUCCESS
Total MapReduce CPU Time Spent: 22 seconds 440 msec
OK
Time taken: 66.84 seconds
Time:582.46
Running Hive query: tpch/q16_parts_supplier_relationship.hive
13/12/09 18:48:20 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 18:48:20 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 18:48:20 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 18:48:20 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 18:48:20 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 18:48:20 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 18:48:20 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.98 seconds
OK
Time taken: 0.097 seconds
OK
Time taken: 0.098 seconds
OK
Time taken: 0.206 seconds
OK
Time taken: 0.126 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.18 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.084 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0065, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0065/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0065
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2013-12-09 18:48:47,364 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.35 sec
MapReduce Total cumulative CPU time: 7 seconds 350 msec
Ended Job = job_1386606905013_0065
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-12-09_18-48-28_648_5015489248907809762-1/-ext-10000
Loading data to table default.supplier_tmp
Table default.supplier_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6885604, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 7.35 sec   HDFS Read: 142874170 HDFS Write: 6885604 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 350 msec
OK
Time taken: 19.286 seconds
Total MapReduce jobs = 2
Stage-3 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 15
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0066, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0066/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0066
Hadoop job information for Stage-3: number of mappers: 56; number of reducers: 15
2013-12-09 18:51:30,420 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 978.79 sec
MapReduce Total cumulative CPU time: 16 minutes 18 seconds 790 msec
Ended Job = job_1386606905013_0066
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 18:51:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 18:51:32 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_18-48-47_936_1353997724460854482-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 18:51:32 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_18-48-47_936_1353997724460854482-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 18:51:32 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 18:51:32 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 18:51:32 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 18:51:32 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 18:51:32 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 18:51:32 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 18:51:32 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 06:51:33	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 06:51:34	Processing rows:	200000	Hashtable size:	199999	Memory usage:	38993776	percentage:	0.082
2013-12-09 06:51:34	Processing rows:	300000	Hashtable size:	299999	Memory usage:	49994040	percentage:	0.105
2013-12-09 06:51:34	Processing rows:	400000	Hashtable size:	399999	Memory usage:	58897128	percentage:	0.123
2013-12-09 06:51:34	Processing rows:	500000	Hashtable size:	499999	Memory usage:	67800200	percentage:	0.142
2013-12-09 06:51:34	Processing rows:	600000	Hashtable size:	599999	Memory usage:	80897592	percentage:	0.17
2013-12-09 06:51:34	Processing rows:	700000	Hashtable size:	699999	Memory usage:	89800656	percentage:	0.188
2013-12-09 06:51:34	Processing rows:	800000	Hashtable size:	799999	Memory usage:	98703736	percentage:	0.207
2013-12-09 06:51:34	Processing rows:	900000	Hashtable size:	899999	Memory usage:	105826184	percentage:	0.222
2013-12-09 06:51:35	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_18-48-47_936_1353997724460854482-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-12-09 06:51:35	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_18-48-47_936_1353997724460854482-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-12-09 06:51:35	End of local task; Time Taken: 2.678 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0067, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0067/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0067
Hadoop job information for Stage-5: number of mappers: 15; number of reducers: 0
2013-12-09 18:52:58,962 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 490.61 sec
MapReduce Total cumulative CPU time: 8 minutes 10 seconds 610 msec
Ended Job = job_1386606905013_0067
Loading data to table default.q16_tmp
Table default.q16_tmp stats: [num_partitions: 0, num_files: 15, num_rows: 0, total_size: 2989814109, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 56  Reduce: 15   Cumulative CPU: 978.79 sec   HDFS Read: 14662902576 HDFS Write: 3937268569 SUCCESS
Job 1: Map: 15   Cumulative CPU: 490.61 sec   HDFS Read: 3937306412 HDFS Write: 2989814109 SUCCESS
Total MapReduce CPU Time Spent: 24 minutes 29 seconds 400 msec
OK
Time taken: 251.468 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0068, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0068/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0068
Hadoop job information for Stage-1: number of mappers: 12; number of reducers: 3
2013-12-09 18:53:52,757 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 286.6 sec
MapReduce Total cumulative CPU time: 4 minutes 46 seconds 600 msec
Ended Job = job_1386606905013_0068
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0069, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0069/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0069
Hadoop job information for Stage-2: number of mappers: 3; number of reducers: 1
2013-12-09 18:54:12,636 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.86 sec
MapReduce Total cumulative CPU time: 7 seconds 860 msec
Ended Job = job_1386606905013_0069
Loading data to table default.q16_parts_supplier_relationship
Table default.q16_parts_supplier_relationship stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 1039440, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 12  Reduce: 3   Cumulative CPU: 286.6 sec   HDFS Read: 2989879503 HDFS Write: 1450608 SUCCESS
Job 1: Map: 3  Reduce: 1   Cumulative CPU: 7.86 sec   HDFS Read: 1451709 HDFS Write: 1039440 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 54 seconds 460 msec
OK
Time taken: 73.694 seconds
Time:353.66
Running Hive query: tpch/q17_small_quantity_order_revenue.hive
13/12/09 18:54:14 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 18:54:14 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 18:54:14 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 18:54:14 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 18:54:14 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 18:54:14 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 18:54:14 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.945 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.181 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.202 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0070, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0070/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0070
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-09 19:08:58,247 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6563.87 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 49 minutes 23 seconds 870 msec
Ended Job = job_1386606905013_0070
Loading data to table default.lineitem_tmp
Table default.lineitem_tmp stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 494293140, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 6563.87 sec   HDFS Read: 79582199808 HDFS Write: 494293140 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 49 minutes 23 seconds 870 msec
OK
Time taken: 876.87 seconds
Total MapReduce jobs = 5
Stage-1 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 83
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0071, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0071/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0071
Hadoop job information for Stage-1: number of mappers: 307; number of reducers: 83
2013-12-09 19:24:19,964 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6504.16 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 48 minutes 24 seconds 160 msec
Ended Job = job_1386606905013_0071
Stage-13 is filtered out by condition resolver.
Stage-14 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0072, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0072/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0072
Hadoop job information for Stage-2: number of mappers: 15; number of reducers: 1
2013-12-09 19:25:48,696 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 173.29 sec
MapReduce Total cumulative CPU time: 2 minutes 53 seconds 290 msec
Ended Job = job_1386606905013_0072
Launching Job 3 out of 5
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0073, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0073/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0073
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-09 19:26:07,886 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.15 sec
MapReduce Total cumulative CPU time: 2 seconds 150 msec
Ended Job = job_1386606905013_0073
Loading data to table default.q17_small_quantity_order_revenue
Table default.q17_small_quantity_order_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 307  Reduce: 83   Cumulative CPU: 6504.16 sec   HDFS Read: 82035510185 HDFS Write: 22558316 SUCCESS
Job 1: Map: 15  Reduce: 1   Cumulative CPU: 173.29 sec   HDFS Read: 516878553 HDFS Write: 121 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 2.15 sec   HDFS Read: 488 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 51 minutes 19 seconds 600 msec
OK
Time taken: 1029.495 seconds
Time:1915.21
Running Hive query: tpch/q18_large_volume_customer.hive
13/12/09 19:26:09 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 19:26:09 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 19:26:09 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 19:26:09 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 19:26:09 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 19:26:09 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 19:26:09 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.922 seconds
OK
Time taken: 0.097 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.194 seconds
OK
Time taken: 0.138 seconds
OK
Time taken: 0.177 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 69
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0074, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0074/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0074
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 69
2013-12-09 19:36:10,506 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3651.74 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 0 minutes 51 seconds 740 msec
Ended Job = job_1386606905013_0074
Loading data to table default.q18_tmp
Table default.q18_tmp stats: [num_partitions: 0, num_files: 69, num_rows: 0, total_size: 2291717657, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 69   Cumulative CPU: 3651.74 sec   HDFS Read: 79582199808 HDFS Write: 2291717657 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 0 minutes 51 seconds 740 msec
OK
Time taken: 593.79 seconds
Total MapReduce jobs = 5
Stage-5 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0075, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0075/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0075
Hadoop job information for Stage-5: number of mappers: 77; number of reducers: 18
2013-12-09 19:42:31,964 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 1922.65 sec
MapReduce Total cumulative CPU time: 32 minutes 2 seconds 650 msec
Ended Job = job_1386606905013_0075
Stage-15 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0076, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0076/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0076
Hadoop job information for Stage-1: number of mappers: 345; number of reducers: 79
2013-12-09 19:58:54,857 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6738.14 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 52 minutes 18 seconds 140 msec
Ended Job = job_1386606905013_0076
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0077, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0077/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0077
Hadoop job information for Stage-2: number of mappers: 7; number of reducers: 1
2013-12-09 19:59:24,566 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 10.23 sec
MapReduce Total cumulative CPU time: 10 seconds 230 msec
Ended Job = job_1386606905013_0077
Launching Job 4 out of 5
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0078, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0078/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0078
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-09 19:59:47,113 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.95 sec
MapReduce Total cumulative CPU time: 2 seconds 950 msec
Ended Job = job_1386606905013_0078
Loading data to table default.q18_large_volume_customer
Table default.q18_large_volume_customer stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6391, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 18   Cumulative CPU: 1922.65 sec   HDFS Read: 20257241173 HDFS Write: 9688881598 SUCCESS
Job 1: Map: 345  Reduce: 79   Cumulative CPU: 6738.14 sec   HDFS Read: 91562986233 HDFS Write: 471842 SUCCESS
Job 2: Map: 7  Reduce: 1   Cumulative CPU: 10.23 sec   HDFS Read: 490395 HDFS Write: 465114 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 2.95 sec   HDFS Read: 465481 HDFS Write: 6391 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 24 minutes 33 seconds 970 msec
OK
Time taken: 1416.494 seconds
Time:2019.24
Running Hive query: tpch/q19_discounted_revenue.hive
13/12/09 19:59:48 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 19:59:48 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 19:59:48 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 19:59:48 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 19:59:48 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 19:59:48 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 19:59:48 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.182 seconds
OK
Time taken: 0.086 seconds
OK
Time taken: 0.235 seconds
OK
Time taken: 0.225 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.04 seconds
Total MapReduce jobs = 2
Stage-1 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0079, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0079/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0079
Hadoop job information for Stage-1: number of mappers: 307; number of reducers: 79
2013-12-09 20:20:01,263 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9071.68 sec
MapReduce Total cumulative CPU time: 0 days 2 hours 31 minutes 11 seconds 680 msec
Ended Job = job_1386606905013_0079
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0080, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0080/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0080
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1
2013-12-09 20:20:30,375 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.96 sec
MapReduce Total cumulative CPU time: 5 seconds 960 msec
Ended Job = job_1386606905013_0080
Loading data to table default.q19_discounted_revenue
Table default.q19_discounted_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 307  Reduce: 79   Cumulative CPU: 9071.68 sec   HDFS Read: 82035510185 HDFS Write: 9559 SUCCESS
Job 1: Map: 6  Reduce: 1   Cumulative CPU: 5.96 sec   HDFS Read: 27967 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 31 minutes 17 seconds 640 msec
OK
Time taken: 1234.279 seconds
Time:1243.22
Running Hive query: tpch/q20_potential_part_promotion.hive
13/12/09 20:20:31 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 20:20:31 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 20:20:31 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 20:20:31 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 20:20:31 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 20:20:31 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 20:20:31 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.064 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.161 seconds
OK
Time taken: 0.192 seconds
OK
Time taken: 0.097 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.184 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.076 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0081, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0081/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0081
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 3
2013-12-09 20:21:16,758 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 73.56 sec
MapReduce Total cumulative CPU time: 1 minutes 13 seconds 560 msec
Ended Job = job_1386606905013_0081
Loading data to table default.q20_tmp1
Table default.q20_tmp1 stats: [num_partitions: 0, num_files: 3, num_rows: 0, total_size: 1834396, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10  Reduce: 3   Cumulative CPU: 73.56 sec   HDFS Read: 2453310377 HDFS Write: 1834396 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 13 seconds 560 msec
OK
Time taken: 36.721 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0082, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0082/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0082
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-09 20:30:08,750 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3100.31 sec
MapReduce Total cumulative CPU time: 51 minutes 40 seconds 310 msec
Ended Job = job_1386606905013_0082
Loading data to table default.q20_tmp2
Table default.q20_tmp2 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 1096808654, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 3100.31 sec   HDFS Read: 79582199808 HDFS Write: 1096808654 SUCCESS
Total MapReduce CPU Time Spent: 51 minutes 40 seconds 310 msec
OK
Time taken: 531.93 seconds
Total MapReduce jobs = 4
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 20:30:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 20:30:11 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_20-30-09_198_2367981050367758899-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 20:30:11 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_20-30-09_198_2367981050367758899-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 20:30:12 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 20:30:12 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 20:30:12 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 20:30:12 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 20:30:12 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 20:30:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 20:30:12 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 08:30:12	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 08:30:13	Processing rows:	200000	Hashtable size:	199999	Memory usage:	41710056	percentage:	0.087
2013-12-09 08:30:13	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_20-30-09_198_2367981050367758899-1/-local-10007/HashTable-Stage-8/MapJoin-mapfile21--.hashtable
2013-12-09 08:30:14	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_20-30-09_198_2367981050367758899-1/-local-10007/HashTable-Stage-8/MapJoin-mapfile21--.hashtable
2013-12-09 08:30:14	End of local task; Time Taken: 1.26 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 4
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0083, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0083/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0083
Hadoop job information for Stage-8: number of mappers: 46; number of reducers: 0
2013-12-09 20:31:22,759 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 319.59 sec
MapReduce Total cumulative CPU time: 5 minutes 19 seconds 590 msec
Ended Job = job_1386606905013_0083
Stage-9 is filtered out by condition resolver.
Stage-10 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0084, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0084/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0084
Hadoop job information for Stage-1: number of mappers: 13; number of reducers: 2
2013-12-09 20:33:52,874 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 412.23 sec
MapReduce Total cumulative CPU time: 6 minutes 52 seconds 230 msec
Ended Job = job_1386606905013_0084
Loading data to table default.q20_tmp3
Table default.q20_tmp3 stats: [num_partitions: 0, num_files: 2, num_rows: 0, total_size: 9821178, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 46   Cumulative CPU: 319.59 sec   HDFS Read: 12209592199 HDFS Write: 24620633 SUCCESS
Job 1: Map: 13  Reduce: 2   Cumulative CPU: 412.23 sec   HDFS Read: 1121447576 HDFS Write: 9821178 SUCCESS
Total MapReduce CPU Time Spent: 12 minutes 11 seconds 820 msec
OK
Time taken: 224.105 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0085, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0085/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0085
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2013-12-09 20:34:22,956 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.81 sec
MapReduce Total cumulative CPU time: 11 seconds 810 msec
Ended Job = job_1386606905013_0085
Loading data to table default.q20_tmp4
Table default.q20_tmp4 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3082950, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 11.81 sec   HDFS Read: 9821608 HDFS Write: 3082950 SUCCESS
Total MapReduce CPU Time Spent: 11 seconds 810 msec
OK
Time taken: 30.098 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 20:34:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 20:34:25 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_20-34-23_402_3544249247376521730-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 20:34:25 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_20-34-23_402_3544249247376521730-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 20:34:25 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 20:34:25 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 20:34:25 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 20:34:25 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 20:34:25 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 20:34:25 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 20:34:25 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 08:34:26	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 08:34:27	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_20-34-23_402_3544249247376521730-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile41--.hashtable
2013-12-09 08:34:27	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_20-34-23_402_3544249247376521730-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile41--.hashtable
2013-12-09 08:34:27	Processing rows:	200000	Hashtable size:	199999	Memory usage:	85517544	percentage:	0.179
2013-12-09 08:34:27	Processing rows:	300000	Hashtable size:	299999	Memory usage:	94482928	percentage:	0.198
2013-12-09 08:34:27	Processing rows:	400000	Hashtable size:	399999	Memory usage:	107642640	percentage:	0.226
2013-12-09 08:34:28	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_20-34-23_402_3544249247376521730-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-12-09 08:34:28	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_20-34-23_402_3544249247376521730-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-12-09 08:34:28	End of local task; Time Taken: 2.106 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0086, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0086/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0086
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-09 20:34:53,494 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 9.05 sec
MapReduce Total cumulative CPU time: 9 seconds 50 msec
Ended Job = job_1386606905013_0086
Loading data to table default.q20_potential_part_promotion
Table default.q20_potential_part_promotion stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 810032, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 9.05 sec   HDFS Read: 142874170 HDFS Write: 810032 SUCCESS
Total MapReduce CPU Time Spent: 9 seconds 50 msec
OK
Time taken: 30.566 seconds
Time:863.16
Running Hive query: tpch/q21_suppliers_who_kept_orders_waiting.hive
13/12/09 20:34:55 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 20:34:55 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 20:34:55 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 20:34:55 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 20:34:55 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 20:34:55 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 20:34:55 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.038 seconds
OK
Time taken: 0.102 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.239 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.37 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.059 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.051 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0087, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0087/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0087
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-09 20:49:33,847 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6321.82 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 45 minutes 21 seconds 820 msec
Ended Job = job_1386606905013_0087
Loading data to table default.q21_tmp1
Table default.q21_tmp1 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 2819600047, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 6321.82 sec   HDFS Read: 79582199808 HDFS Write: 2819600047 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 45 minutes 21 seconds 820 msec
OK
Time taken: 870.837 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0088, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0088/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0088
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-09 21:01:11,148 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4745.58 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 19 minutes 5 seconds 580 msec
Ended Job = job_1386606905013_0088
Loading data to table default.q21_tmp2
Table default.q21_tmp2 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 2583844107, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 4745.58 sec   HDFS Read: 79582199808 HDFS Write: 2583844107 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 19 minutes 5 seconds 580 msec
OK
Time taken: 697.217 seconds
Total MapReduce jobs = 14
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 21:01:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 21:01:17 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_21-01-11_619_6425059732265360196-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 21:01:17 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_21-01-11_619_6425059732265360196-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 21:01:17 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 21:01:17 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 21:01:17 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 21:01:17 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 21:01:17 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 21:01:17 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 21:01:17 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 09:01:18	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 09:01:18	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_21-01-11_619_6425059732265360196-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-12-09 09:01:18	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_21-01-11_619_6425059732265360196-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-12-09 09:01:18	End of local task; Time Taken: 0.846 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 14
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0089, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0089/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0089
Hadoop job information for Stage-24: number of mappers: 1; number of reducers: 0
2013-12-09 21:02:04,967 Stage-24 map = 100%,  reduce = 0%, Cumulative CPU 6.78 sec
MapReduce Total cumulative CPU time: 6 seconds 780 msec
Ended Job = job_1386606905013_0089
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 14
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0090, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0090/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0090
Hadoop job information for Stage-8: number of mappers: 298; number of reducers: 80
2013-12-09 21:12:59,374 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 4087.52 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 8 minutes 7 seconds 520 msec
Ended Job = job_1386606905013_0090
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 14
Number of reduce tasks not specified. Estimated from input data size: 19
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0091, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0091/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0091
Hadoop job information for Stage-1: number of mappers: 74; number of reducers: 19
2013-12-09 21:15:41,667 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 900.71 sec
MapReduce Total cumulative CPU time: 15 minutes 0 seconds 710 msec
Ended Job = job_1386606905013_0091
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 14
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0092, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0092/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0092
Hadoop job information for Stage-2: number of mappers: 17; number of reducers: 4
2013-12-09 21:18:48,634 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 994.12 sec
MapReduce Total cumulative CPU time: 16 minutes 34 seconds 120 msec
Ended Job = job_1386606905013_0092
Stage-25 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 5 out of 14
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0093, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0093/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0093
Hadoop job information for Stage-3: number of mappers: 15; number of reducers: 3
2013-12-09 21:23:06,689 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 975.15 sec
MapReduce Total cumulative CPU time: 16 minutes 15 seconds 150 msec
Ended Job = job_1386606905013_0093
Launching Job 6 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0094, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0094/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0094
Hadoop job information for Stage-4: number of mappers: 3; number of reducers: 1
2013-12-09 21:23:34,224 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 12.09 sec
MapReduce Total cumulative CPU time: 12 seconds 90 msec
Ended Job = job_1386606905013_0094
Launching Job 7 out of 14
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0095, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0095/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0095
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-12-09 21:23:56,106 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 4.85 sec
MapReduce Total cumulative CPU time: 4 seconds 850 msec
Ended Job = job_1386606905013_0095
Loading data to table default.q21_suppliers_who_kept_orders_waiting
Table default.q21_suppliers_who_kept_orders_waiting stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 2200, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 6.78 sec   HDFS Read: 142874170 HDFS Write: 1611510 SUCCESS
Job 1: Map: 298  Reduce: 80   Cumulative CPU: 4087.52 sec   HDFS Read: 79583811685 HDFS Write: 686901797 SUCCESS
Job 2: Map: 74  Reduce: 19   Cumulative CPU: 900.71 sec   HDFS Read: 18480595103 HDFS Write: 331675044 SUCCESS
Job 3: Map: 17  Reduce: 4   Cumulative CPU: 994.12 sec   HDFS Read: 3151287926 HDFS Write: 319626840 SUCCESS
Job 4: Map: 15  Reduce: 3   Cumulative CPU: 975.15 sec   HDFS Read: 2903480162 HDFS Write: 4938137 SUCCESS
Job 5: Map: 3  Reduce: 1   Cumulative CPU: 12.09 sec   HDFS Read: 4939238 HDFS Write: 1492766 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 4.85 sec   HDFS Read: 1493133 HDFS Write: 2200 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 56 minutes 21 seconds 220 msec
OK
Time taken: 1364.914 seconds
Time:2942.56
Running Hive query: tpch/q22_global_sales_opportunity.hive
13/12/09 21:23:57 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 21:23:57 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 21:23:57 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 21:23:57 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 21:23:57 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 21:23:57 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 21:23:57 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.023 seconds
OK
Time taken: 0.087 seconds
OK
Time taken: 0.205 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.175 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.084 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0096, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0096/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0096
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 0
2013-12-09 21:24:47,183 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 95.78 sec
MapReduce Total cumulative CPU time: 1 minutes 35 seconds 780 msec
Ended Job = job_1386606905013_0096
Stage-4 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Stage-5 is filtered out by condition resolver.
Launching Job 3 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386606905013_0097, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0097/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0097
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
2013-12-09 21:25:08,491 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 10.22 sec
MapReduce Total cumulative CPU time: 10 seconds 220 msec
Ended Job = job_1386606905013_0097
Loading data to table default.q22_customer_tmp
Table default.q22_customer_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 80028920, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10   Cumulative CPU: 95.78 sec   HDFS Read: 2463566642 HDFS Write: 80028920 SUCCESS
Job 1: Map: 1   Cumulative CPU: 10.22 sec   HDFS Read: 80030327 HDFS Write: 80028920 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 46 seconds 0 msec
OK
Time taken: 63.272 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0098, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0098/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0098
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-12-09 21:25:32,625 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.56 sec
MapReduce Total cumulative CPU time: 7 seconds 560 msec
Ended Job = job_1386606905013_0098
Loading data to table default.q22_customer_tmp1
Table default.q22_customer_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 18, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 7.56 sec   HDFS Read: 80029143 HDFS Write: 18 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 560 msec
OK
Time taken: 24.04 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0099, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0099/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0099
Hadoop job information for Stage-1: number of mappers: 67; number of reducers: 18
2013-12-09 21:28:11,672 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1024.8 sec
MapReduce Total cumulative CPU time: 17 minutes 4 seconds 800 msec
Ended Job = job_1386606905013_0099
Loading data to table default.q22_orders_tmp
Table default.q22_orders_tmp stats: [num_partitions: 0, num_files: 18, num_rows: 0, total_size: 82591212, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 67  Reduce: 18   Cumulative CPU: 1024.8 sec   HDFS Read: 17793674531 HDFS Write: 82591212 SUCCESS
Total MapReduce CPU Time Spent: 17 minutes 4 seconds 800 msec
OK
Time taken: 159.063 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0100, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0100/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0100
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1
2013-12-09 21:29:28,411 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 113.27 sec
MapReduce Total cumulative CPU time: 1 minutes 53 seconds 270 msec
Ended Job = job_1386606905013_0100
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/09 21:29:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/09 21:29:30 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_21-28-12_127_1857767177394292651-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/09 21:29:30 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-09_21-28-12_127_1857767177394292651-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/09 21:29:30 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/09 21:29:30 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/09 21:29:30 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/09 21:29:30 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/09 21:29:30 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/09 21:29:30 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/09 21:29:30 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-09 09:29:31	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-09 09:29:31	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-09_21-28-12_127_1857767177394292651-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-12-09 09:29:31	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-09_21-28-12_127_1857767177394292651-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-12-09 09:29:31	End of local task; Time Taken: 0.564 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0101, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0101/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0101
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-09 21:29:55,271 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 7.99 sec
MapReduce Total cumulative CPU time: 7 seconds 990 msec
Ended Job = job_1386606905013_0101
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386606905013_0102, Tracking URL = http://10.6.40.110/proxy/application_1386606905013_0102/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386606905013_0102
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-09 21:30:13,863 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.88 sec
MapReduce Total cumulative CPU time: 1 seconds 880 msec
Ended Job = job_1386606905013_0102
Loading data to table default.q22_global_sales_opportunity
Table default.q22_global_sales_opportunity stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 202, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 6  Reduce: 1   Cumulative CPU: 113.27 sec   HDFS Read: 162622552 HDFS Write: 39621076 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 7.99 sec   HDFS Read: 39621443 HDFS Write: 320 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.88 sec   HDFS Read: 687 HDFS Write: 202 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 3 seconds 140 msec
OK
Time taken: 122.172 seconds
Time:377.77
