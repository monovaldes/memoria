Running Hive from /opt/hive-0.12.0
Running Hadoop from 
Running Hive query: tpch/q1_pricing_summary_report.hive
13/12/02 14:02:29 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 14:02:29 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 14:02:29 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 14:02:29 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 14:02:29 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 14:02:29 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 14:02:29 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.97 seconds
OK
Time taken: 0.258 seconds
OK
Time taken: 0.219 seconds
OK
Time taken: 0.067 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0001, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0001/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0001
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-02 14:09:26,026 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3583.57 sec
MapReduce Total cumulative CPU time: 59 minutes 43 seconds 570 msec
Ended Job = job_1386003473374_0001
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0002, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0002/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0002
Hadoop job information for Stage-2: number of mappers: 8; number of reducers: 1
2013-12-02 14:09:46,779 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.59 sec
MapReduce Total cumulative CPU time: 7 seconds 590 msec
Ended Job = job_1386003473374_0002
Loading data to table default.q1_pricing_summary_report
Table default.q1_pricing_summary_report stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 592, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 3583.57 sec   HDFS Read: 79582199808 HDFS Write: 8011 SUCCESS
Job 1: Map: 8  Reduce: 1   Cumulative CPU: 7.59 sec   HDFS Read: 26931 HDFS Write: 592 SUCCESS
Total MapReduce CPU Time Spent: 59 minutes 51 seconds 160 msec
OK
Time taken: 429.517 seconds
Time:439.55
Running Hive query: tpch/q2_minimum_cost_supplier.hive
13/12/02 14:09:48 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 14:09:48 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 14:09:48 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 14:09:48 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 14:09:48 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 14:09:48 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 14:09:48 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.733 seconds
OK
Time taken: 0.102 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.211 seconds
OK
Time taken: 0.112 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.228 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.102 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.052 seconds
Total MapReduce jobs = 10
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 14:10:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 14:10:04 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_14-09-57_857_1704649025917078366-1/-local-10020/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 14:10:04 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_14-09-57_857_1704649025917078366-1/-local-10020/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 14:10:04 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 14:10:04 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 14:10:04 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 14:10:04 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 14:10:04 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 14:10:04 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 14:10:04 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 02:10:05	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 02:10:06	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_14-09-57_857_1704649025917078366-1/-local-10017/HashTable-Stage-18/MapJoin-mapfile61--.hashtable
2013-12-02 02:10:06	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_14-09-57_857_1704649025917078366-1/-local-10017/HashTable-Stage-18/MapJoin-mapfile61--.hashtable
2013-12-02 02:10:06	End of local task; Time Taken: 1.03 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386003473374_0003, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0003/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0003
Hadoop job information for Stage-18: number of mappers: 1; number of reducers: 0
2013-12-02 14:10:18,993 Stage-18 map = 100%,  reduce = 0%, Cumulative CPU 0.98 sec
MapReduce Total cumulative CPU time: 980 msec
Ended Job = job_1386003473374_0003
Stage-23 is filtered out by condition resolver.
Stage-24 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0004, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0004/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0004
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-12-02 14:10:47,969 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 21.67 sec
MapReduce Total cumulative CPU time: 21 seconds 670 msec
Ended Job = job_1386003473374_0004
Stage-21 is filtered out by condition resolver.
Stage-22 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 3 out of 10
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0005, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0005/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0005
Hadoop job information for Stage-2: number of mappers: 47; number of reducers: 13
2013-12-02 14:12:41,277 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 770.46 sec
MapReduce Total cumulative CPU time: 12 minutes 50 seconds 460 msec
Ended Job = job_1386003473374_0005
Stage-19 is filtered out by condition resolver.
Stage-20 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 4 out of 10
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0006, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0006/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0006
Hadoop job information for Stage-3: number of mappers: 22; number of reducers: 6
2013-12-02 14:13:39,376 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 346.18 sec
MapReduce Total cumulative CPU time: 5 minutes 46 seconds 180 msec
Ended Job = job_1386003473374_0006
Loading data to table default.q2_minimum_cost_supplier_tmp1
Table default.q2_minimum_cost_supplier_tmp1 stats: [num_partitions: 0, num_files: 6, num_rows: 0, total_size: 10939639, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.98 sec   HDFS Read: 2424 HDFS Write: 231 SUCCESS
Job 1: Map: 3  Reduce: 1   Cumulative CPU: 21.67 sec   HDFS Read: 142874905 HDFS Write: 32875244 SUCCESS
Job 2: Map: 47  Reduce: 13   Cumulative CPU: 770.46 sec   HDFS Read: 12242467810 HDFS Write: 2763176536 SUCCESS
Job 3: Map: 22  Reduce: 6   Cumulative CPU: 346.18 sec   HDFS Read: 5216530897 HDFS Write: 10939639 SUCCESS
Total MapReduce CPU Time Spent: 18 minutes 59 seconds 290 msec
OK
Time taken: 222.056 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0007, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0007/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0007
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-12-02 14:14:03,236 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.03 sec
MapReduce Total cumulative CPU time: 12 seconds 30 msec
Ended Job = job_1386003473374_0007
Loading data to table default.q2_minimum_cost_supplier_tmp2
Table default.q2_minimum_cost_supplier_tmp2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 715857, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 12.03 sec   HDFS Read: 10940781 HDFS Write: 715857 SUCCESS
Total MapReduce CPU Time Spent: 12 seconds 30 msec
OK
Time taken: 23.797 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 14:14:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 14:14:06 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_14-14-03_712_6266087907868379982-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 14:14:06 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_14-14-03_712_6266087907868379982-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 14:14:06 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 14:14:06 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 14:14:06 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 14:14:06 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 14:14:06 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 14:14:06 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 14:14:06 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 02:14:07	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 02:14:08	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_14-14-03_712_6266087907868379982-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile71--.hashtable
2013-12-02 02:14:08	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_14-14-03_712_6266087907868379982-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile71--.hashtable
2013-12-02 02:14:08	End of local task; Time Taken: 1.243 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0008, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0008/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0008
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 1
2013-12-02 14:14:32,122 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 14.4 sec
MapReduce Total cumulative CPU time: 14 seconds 400 msec
Ended Job = job_1386003473374_0008
Loading data to table default.q2_minimum_cost_supplier
Table default.q2_minimum_cost_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16261, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 14.4 sec   HDFS Read: 10940781 HDFS Write: 16261 SUCCESS
Total MapReduce CPU Time Spent: 14 seconds 400 msec
OK
Time taken: 28.957 seconds
Time:285.35
Running Hive query: tpch/q3_shipping_priority.hive
13/12/02 14:14:33 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 14:14:33 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 14:14:33 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 14:14:33 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 14:14:33 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 14:14:33 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 14:14:33 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.775 seconds
OK
Time taken: 0.142 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.225 seconds
OK
Time taken: 0.208 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 6
Stage-1 is selected by condition resolver.
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0009, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0009/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0009
Hadoop job information for Stage-1: number of mappers: 77; number of reducers: 20
2013-12-02 14:16:49,511 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 975.42 sec
MapReduce Total cumulative CPU time: 16 minutes 15 seconds 420 msec
Ended Job = job_1386003473374_0009
Stage-14 is filtered out by condition resolver.
Stage-15 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0010, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0010/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0010
Hadoop job information for Stage-2: number of mappers: 305; number of reducers: 79
2013-12-02 14:24:17,951 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4240.36 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 10 minutes 40 seconds 360 msec
Ended Job = job_1386003473374_0010
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0011, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0011/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0011
Hadoop job information for Stage-3: number of mappers: 9; number of reducers: 1
2013-12-02 14:24:44,239 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 43.0 sec
MapReduce Total cumulative CPU time: 43 seconds 0 msec
Ended Job = job_1386003473374_0011
Launching Job 4 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0012, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0012/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0012
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-02 14:25:12,864 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 12.49 sec
MapReduce Total cumulative CPU time: 12 seconds 490 msec
Ended Job = job_1386003473374_0012
Loading data to table default.q3_shipping_priority
Table default.q3_shipping_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 382, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 20   Cumulative CPU: 975.42 sec   HDFS Read: 20257241173 HDFS Write: 499838544 SUCCESS
Job 1: Map: 305  Reduce: 79   Cumulative CPU: 4240.36 sec   HDFS Read: 80082043952 HDFS Write: 47949847 SUCCESS
Job 2: Map: 9  Reduce: 1   Cumulative CPU: 43.0 sec   HDFS Read: 47968690 HDFS Write: 47942799 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 12.49 sec   HDFS Read: 47943166 HDFS Write: 382 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 27 minutes 51 seconds 270 msec
OK
Time taken: 630.778 seconds
Time:640.69
Running Hive query: tpch/q4_order_priority.hive
13/12/02 14:25:14 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 14:25:14 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 14:25:14 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 14:25:14 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 14:25:14 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 14:25:14 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 14:25:14 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.513 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.229 seconds
OK
Time taken: 0.125 seconds
OK
Time taken: 0.212 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.046 seconds
OK
Time taken: 0.042 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0013, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0013/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0013
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-02 14:32:04,109 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3081.99 sec
MapReduce Total cumulative CPU time: 51 minutes 21 seconds 990 msec
Ended Job = job_1386003473374_0013
Loading data to table default.q4_order_priority_tmp
Table default.q4_order_priority_tmp stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 1350015083, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 3081.99 sec   HDFS Read: 79582199808 HDFS Write: 1350015083 SUCCESS
Total MapReduce CPU Time Spent: 51 minutes 21 seconds 990 msec
OK
Time taken: 401.693 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0014, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0014/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0014
Hadoop job information for Stage-1: number of mappers: 77; number of reducers: 20
2013-12-02 14:34:34,603 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1158.91 sec
MapReduce Total cumulative CPU time: 19 minutes 18 seconds 910 msec
Ended Job = job_1386003473374_0014
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0015, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0015/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0015
Hadoop job information for Stage-2: number of mappers: 8; number of reducers: 1
2013-12-02 14:34:55,085 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.01 sec
MapReduce Total cumulative CPU time: 6 seconds 10 msec
Ended Job = job_1386003473374_0015
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0016, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0016/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0016
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-02 14:35:14,402 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.81 sec
MapReduce Total cumulative CPU time: 1 seconds 810 msec
Ended Job = job_1386003473374_0016
Loading data to table default.q4_order_priority
Table default.q4_order_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 87, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 20   Cumulative CPU: 1158.91 sec   HDFS Read: 19143698264 HDFS Write: 4860 SUCCESS
Job 1: Map: 8  Reduce: 1   Cumulative CPU: 6.01 sec   HDFS Read: 10460 HDFS Write: 248 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.81 sec   HDFS Read: 615 HDFS Write: 87 SUCCESS
Total MapReduce CPU Time Spent: 19 minutes 26 seconds 730 msec
OK
Time taken: 190.097 seconds
Time:601.51
Running Hive query: tpch/q5_local_supplier_volume.hive
13/12/02 14:35:16 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 14:35:16 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 14:35:16 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 14:35:16 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 14:35:16 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 14:35:16 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 14:35:16 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.65 seconds
OK
Time taken: 0.138 seconds
OK
Time taken: 0.136 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.2 seconds
OK
Time taken: 0.21 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 14:35:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 14:35:32 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_14-35-25_165_6495461224843937057-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 14:35:32 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_14-35-25_165_6495461224843937057-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 14:35:33 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 14:35:33 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 14:35:33 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 14:35:33 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 14:35:33 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 14:35:33 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 14:35:33 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 02:35:33	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 02:35:34	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_14-35-25_165_6495461224843937057-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile81--.hashtable
2013-12-02 02:35:34	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_14-35-25_165_6495461224843937057-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile81--.hashtable
2013-12-02 02:35:34	End of local task; Time Taken: 1.017 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386003473374_0017, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0017/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0017
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-12-02 14:35:47,205 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 0.97 sec
MapReduce Total cumulative CPU time: 970 msec
Ended Job = job_1386003473374_0017
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0018, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0018/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0018
Hadoop job information for Stage-7: number of mappers: 3; number of reducers: 1
2013-12-02 14:36:10,621 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 13.61 sec
MapReduce Total cumulative CPU time: 13 seconds 610 msec
Ended Job = job_1386003473374_0018
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0019, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0019/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0019
Hadoop job information for Stage-8: number of mappers: 298; number of reducers: 80
2013-12-02 14:47:12,698 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 6577.82 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 49 minutes 37 seconds 820 msec
Ended Job = job_1386003473374_0019
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 24
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0020, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0020/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0020
Hadoop job information for Stage-1: number of mappers: 87; number of reducers: 24
2013-12-02 14:50:35,671 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1827.53 sec
MapReduce Total cumulative CPU time: 30 minutes 27 seconds 530 msec
Ended Job = job_1386003473374_0020
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0021, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0021/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0021
Hadoop job information for Stage-2: number of mappers: 17; number of reducers: 4
2013-12-02 14:51:52,001 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 306.53 sec
MapReduce Total cumulative CPU time: 5 minutes 6 seconds 530 msec
Ended Job = job_1386003473374_0021
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0022, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0022/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0022
Hadoop job information for Stage-3: number of mappers: 3; number of reducers: 1
2013-12-02 14:52:09,546 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.59 sec
MapReduce Total cumulative CPU time: 2 seconds 590 msec
Ended Job = job_1386003473374_0022
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0023, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0023/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0023
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-02 14:52:27,994 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.41 sec
MapReduce Total cumulative CPU time: 1 seconds 410 msec
Ended Job = job_1386003473374_0023
Loading data to table default.q5_local_supplier_volume
Table default.q5_local_supplier_volume stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 138, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.97 sec   HDFS Read: 2424 HDFS Write: 222 SUCCESS
Job 1: Map: 3  Reduce: 1   Cumulative CPU: 13.61 sec   HDFS Read: 142874896 HDFS Write: 5881020 SUCCESS
Job 2: Map: 298  Reduce: 80   Cumulative CPU: 6577.82 sec   HDFS Read: 79588081195 HDFS Write: 5591627709 SUCCESS
Job 3: Map: 87  Reduce: 24   Cumulative CPU: 1827.53 sec   HDFS Read: 23385322900 HDFS Write: 830634798 SUCCESS
Job 4: Map: 17  Reduce: 4   Cumulative CPU: 306.53 sec   HDFS Read: 3294207783 HDFS Write: 1028 SUCCESS
Job 5: Map: 3  Reduce: 1   Cumulative CPU: 2.59 sec   HDFS Read: 2351 HDFS Write: 257 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.41 sec   HDFS Read: 624 HDFS Write: 138 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 25 minutes 30 seconds 460 msec
OK
Time taken: 1023.345 seconds
Time:1033.61
Running Hive query: tpch/q6_forecast_revenue_change.hive
13/12/02 14:52:29 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 14:52:29 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 14:52:29 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 14:52:29 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 14:52:29 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 14:52:29 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 14:52:29 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.767 seconds
OK
Time taken: 0.2 seconds
OK
Time taken: 0.224 seconds
OK
Time taken: 0.048 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0024, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0024/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0024
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 1
2013-12-02 14:56:47,428 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2122.06 sec
MapReduce Total cumulative CPU time: 35 minutes 22 seconds 60 msec
Ended Job = job_1386003473374_0024
Loading data to table default.q6_forecast_revenue_change
Table default.q6_forecast_revenue_change stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 20, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 1   Cumulative CPU: 2122.06 sec   HDFS Read: 79582199808 HDFS Write: 20 SUCCESS
Total MapReduce CPU Time Spent: 35 minutes 22 seconds 60 msec
OK
Time taken: 249.958 seconds
Time:259.50
Running Hive query: tpch/q7_volume_shipping.hive
13/12/02 14:56:49 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 14:56:49 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 14:56:49 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 14:56:49 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 14:56:49 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 14:56:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 14:56:49 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.56 seconds
OK
Time taken: 0.11 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.209 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.194 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.083 seconds
OK
Time taken: 0.06 seconds
Total MapReduce jobs = 3
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 14:57:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 14:57:01 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_14-56-58_116_5933676969184812305-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 14:57:01 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_14-56-58_116_5933676969184812305-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 14:57:01 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 14:57:01 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 14:57:01 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 14:57:01 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 14:57:01 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 14:57:01 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 14:57:01 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 02:57:02	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 02:57:03	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_14-56-58_116_5933676969184812305-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-12-02 02:57:03	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_14-56-58_116_5933676969184812305-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-12-02 02:57:03	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_14-56-58_116_5933676969184812305-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-12-02 02:57:03	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_14-56-58_116_5933676969184812305-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-12-02 02:57:03	End of local task; Time Taken: 1.228 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386003473374_0025, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0025/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0025
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 0
2013-12-02 14:57:17,988 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec
MapReduce Total cumulative CPU time: 1 seconds 460 msec
Ended Job = job_1386003473374_0025
Stage-5 is selected by condition resolver.
Stage-4 is filtered out by condition resolver.
Stage-6 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-12-02_14-56-58_116_5933676969184812305-1/-ext-10000
Loading data to table default.q7_volume_shipping_tmp
Table default.q7_volume_shipping_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 38, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.46 sec   HDFS Read: 2424 HDFS Write: 38 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 460 msec
OK
Time taken: 20.433 seconds
Total MapReduce jobs = 9
Stage-6 is selected by condition resolver.
Launching Job 1 out of 9
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0026, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0026/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0026
Hadoop job information for Stage-6: number of mappers: 364; number of reducers: 80
2013-12-02 15:26:21,730 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 5040.73 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 24 minutes 0 seconds 730 msec
Ended Job = job_1386003473374_0026
Stage-24 is filtered out by condition resolver.
Stage-25 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 9
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0027, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0027/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0027
Hadoop job information for Stage-7: number of mappers: 18; number of reducers: 4
2013-12-02 15:28:50,725 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 488.27 sec
MapReduce Total cumulative CPU time: 8 minutes 8 seconds 270 msec
Ended Job = job_1386003473374_0027
Stage-22 is filtered out by condition resolver.
Stage-23 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 9
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0028, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0028/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0028
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 2
2013-12-02 15:30:58,178 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 362.16 sec
MapReduce Total cumulative CPU time: 6 minutes 2 seconds 160 msec
Ended Job = job_1386003473374_0028
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 15:31:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 15:31:00 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_14-57-18_552_8737899946412398503-1/-local-10024/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 15:31:00 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_14-57-18_552_8737899946412398503-1/-local-10024/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 15:31:00 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 15:31:00 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 15:31:00 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 15:31:00 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 15:31:00 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 15:31:00 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 15:31:00 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 03:31:01	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 03:31:01	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_14-57-18_552_8737899946412398503-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-12-02 03:31:02	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_14-57-18_552_8737899946412398503-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-12-02 03:31:02	End of local task; Time Taken: 0.689 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 4 out of 9
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0029, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0029/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0029
Hadoop job information for Stage-3: number of mappers: 8; number of reducers: 2
2013-12-02 15:31:40,657 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 92.39 sec
MapReduce Total cumulative CPU time: 1 minutes 32 seconds 390 msec
Ended Job = job_1386003473374_0029
Launching Job 5 out of 9
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0030, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0030/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0030
Hadoop job information for Stage-4: number of mappers: 2; number of reducers: 1
2013-12-02 15:32:00,935 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 2.04 sec
MapReduce Total cumulative CPU time: 2 seconds 40 msec
Ended Job = job_1386003473374_0030
Loading data to table default.q7_volume_shipping
Table default.q7_volume_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 160, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 364  Reduce: 80   Cumulative CPU: 5040.73 sec   HDFS Read: 97375874339 HDFS Write: 9560339811 SUCCESS
Job 1: Map: 18  Reduce: 4   Cumulative CPU: 488.27 sec   HDFS Read: 4136641490 HDFS Write: 1576690073 SUCCESS
Job 2: Map: 10  Reduce: 2   Cumulative CPU: 362.16 sec   HDFS Read: 1719586689 HDFS Write: 1482023798 SUCCESS
Job 3: Map: 8  Reduce: 2   Cumulative CPU: 92.39 sec   HDFS Read: 1482055754 HDFS Write: 364 SUCCESS
Job 4: Map: 2  Reduce: 1   Cumulative CPU: 2.04 sec   HDFS Read: 1098 HDFS Write: 160 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 39 minutes 45 seconds 590 msec
OK
Time taken: 2082.878 seconds
Time:2113.41
Running Hive query: tpch/q8_national_market_share.hive
13/12/02 15:32:02 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 15:32:02 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 15:32:02 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 15:32:02 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 15:32:02 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 15:32:02 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 15:32:02 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.706 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.134 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.098 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.213 seconds
OK
Time taken: 0.223 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.059 seconds
OK
Time taken: 0.067 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 18
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 15:32:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 15:32:21 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_15-32-11_883_8781652400764325307-1/-local-10035/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 15:32:21 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_15-32-11_883_8781652400764325307-1/-local-10035/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 15:32:21 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 15:32:21 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 15:32:21 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 15:32:21 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 15:32:21 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 15:32:21 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 15:32:21 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 03:32:22	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 03:32:23	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_15-32-11_883_8781652400764325307-1/-local-10032/HashTable-Stage-32/MapJoin-mapfile111--.hashtable
2013-12-02 03:32:23	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_15-32-11_883_8781652400764325307-1/-local-10032/HashTable-Stage-32/MapJoin-mapfile111--.hashtable
2013-12-02 03:32:23	End of local task; Time Taken: 0.992 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 18
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386003473374_0031, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0031/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0031
Hadoop job information for Stage-32: number of mappers: 1; number of reducers: 0
2013-12-02 15:32:36,559 Stage-32 map = 100%,  reduce = 0%, Cumulative CPU 0.98 sec
MapReduce Total cumulative CPU time: 980 msec
Ended Job = job_1386003473374_0031
Stage-42 is filtered out by condition resolver.
Stage-43 is filtered out by condition resolver.
Stage-9 is selected by condition resolver.
Launching Job 2 out of 18
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0032, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0032/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0032
Hadoop job information for Stage-9: number of mappers: 11; number of reducers: 3
2013-12-02 15:33:19,311 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 117.32 sec
MapReduce Total cumulative CPU time: 1 minutes 57 seconds 320 msec
Ended Job = job_1386003473374_0032
Stage-40 is filtered out by condition resolver.
Stage-41 is filtered out by condition resolver.
Stage-10 is selected by condition resolver.
Launching Job 3 out of 18
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0033, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0033/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0033
Hadoop job information for Stage-10: number of mappers: 70; number of reducers: 18
2013-12-02 15:35:04,810 Stage-10 map = 100%,  reduce = 100%, Cumulative CPU 743.39 sec
MapReduce Total cumulative CPU time: 12 minutes 23 seconds 390 msec
Ended Job = job_1386003473374_0033
Stage-38 is filtered out by condition resolver.
Stage-39 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 18
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0034, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0034/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0034
Hadoop job information for Stage-1: number of mappers: 304; number of reducers: 80
2013-12-02 16:16:15,039 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6547.43 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 49 minutes 7 seconds 430 msec
Ended Job = job_1386003473374_0034
Stage-36 is filtered out by condition resolver.
Stage-37 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 18
Number of reduce tasks not specified. Estimated from input data size: 5
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0035, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0035/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0035
Hadoop job information for Stage-2: number of mappers: 22; number of reducers: 5
2013-12-02 16:17:53,359 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 436.37 sec
MapReduce Total cumulative CPU time: 7 minutes 16 seconds 370 msec
Ended Job = job_1386003473374_0035
Stage-34 is filtered out by condition resolver.
Stage-35 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 6 out of 18
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0036, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0036/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0036
Hadoop job information for Stage-3: number of mappers: 5; number of reducers: 1
2013-12-02 16:18:17,389 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 25.59 sec
MapReduce Total cumulative CPU time: 25 seconds 590 msec
Ended Job = job_1386003473374_0036
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 16:18:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 16:18:19 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_15-32-11_883_8781652400764325307-1/-local-10049/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 16:18:19 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_15-32-11_883_8781652400764325307-1/-local-10049/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 16:18:19 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 16:18:19 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 16:18:19 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 16:18:19 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 16:18:19 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 16:18:19 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 16:18:19 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 04:18:20	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 04:18:21	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_15-32-11_883_8781652400764325307-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-12-02 04:18:21	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_15-32-11_883_8781652400764325307-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-12-02 04:18:21	End of local task; Time Taken: 0.663 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 7 out of 18
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0037, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0037/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0037
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-12-02 16:18:52,002 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 7.35 sec
MapReduce Total cumulative CPU time: 7 seconds 350 msec
Ended Job = job_1386003473374_0037
Launching Job 8 out of 18
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0038, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0038/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0038
Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 1
2013-12-02 16:19:10,369 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 1.4 sec
MapReduce Total cumulative CPU time: 1 seconds 400 msec
Ended Job = job_1386003473374_0038
Loading data to table default.q8_national_market_share
Table default.q8_national_market_share stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 49, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.98 sec   HDFS Read: 2424 HDFS Write: 186 SUCCESS
Job 1: Map: 11  Reduce: 3   Cumulative CPU: 117.32 sec   HDFS Read: 2463567195 HDFS Write: 63652437 SUCCESS
Job 2: Map: 70  Reduce: 18   Cumulative CPU: 743.39 sec   HDFS Read: 17857328069 HDFS Write: 303326332 SUCCESS
Job 3: Map: 304  Reduce: 80   Cumulative CPU: 6547.43 sec   HDFS Read: 79885531151 HDFS Write: 1917158445 SUCCESS
Job 4: Map: 22  Reduce: 5   Cumulative CPU: 436.37 sec   HDFS Read: 4370488322 HDFS Write: 11836897 SUCCESS
Job 5: Map: 5  Reduce: 1   Cumulative CPU: 25.59 sec   HDFS Read: 154712757 HDFS Write: 11111526 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 7.35 sec   HDFS Read: 11111893 HDFS Write: 152 SUCCESS
Job 7: Map: 1  Reduce: 1   Cumulative CPU: 1.4 sec   HDFS Read: 519 HDFS Write: 49 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 11 minutes 19 seconds 830 msec
OK
Time taken: 2818.959 seconds
Time:2829.39
Running Hive query: tpch/q9_product_type_profit.hive
13/12/02 16:19:12 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 16:19:12 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 16:19:12 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 16:19:12 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 16:19:12 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 16:19:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 16:19:12 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.615 seconds
OK
Time taken: 0.134 seconds
OK
Time taken: 0.138 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.096 seconds
OK
Time taken: 0.087 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.203 seconds
OK
Time taken: 0.047 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.059 seconds
OK
Time taken: 0.057 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 16:19:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 16:19:28 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_16-19-21_038_7796266817130706612-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 16:19:29 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_16-19-21_038_7796266817130706612-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 16:19:29 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 16:19:29 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 16:19:29 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 16:19:29 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 16:19:29 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 16:19:29 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 16:19:29 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 04:19:29	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 04:19:30	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_16-19-21_038_7796266817130706612-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-12-02 04:19:30	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_16-19-21_038_7796266817130706612-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-12-02 04:19:30	End of local task; Time Taken: 0.736 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386003473374_0039, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0039/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0039
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-12-02 16:19:52,247 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 5.91 sec
MapReduce Total cumulative CPU time: 5 seconds 910 msec
Ended Job = job_1386003473374_0039
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 78
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0040, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0040/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0040
Hadoop job information for Stage-8: number of mappers: 298; number of reducers: 78
2013-12-02 16:36:51,282 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 8890.21 sec
MapReduce Total cumulative CPU time: 0 days 2 hours 28 minutes 10 seconds 210 msec
Ended Job = job_1386003473374_0040
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 49
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0041, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0041/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0041
Hadoop job information for Stage-1: number of mappers: 173; number of reducers: 49
2013-12-02 16:55:20,374 Stage-1 map = 100%,  reduce = 45%
Ended Job = job_1386003473374_0041 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1386003473374_0041_m_000065 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000089 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000099 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000070 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000024 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000073 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000085 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000142 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000050 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000122 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000076 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000156 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000059 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000053 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000128 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000170 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000154 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000077 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_r_000013 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000092 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_r_000020 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000008 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000147 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_m_000022 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_r_000040 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_r_000045 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_r_000042 (and more) from job job_1386003473374_0041
Examining task ID: task_1386003473374_0041_r_000029 (and more) from job job_1386003473374_0041

Task with the most failures(4): 
-----
Task ID:
  task_1386003473374_0041_r_000023

URL:
  http://hadoop11:8088/taskdetails.jsp?jobid=job_1386003473374_0041&tipid=task_1386003473374_0041_r_000023
-----
Diagnostic Messages for this Task:


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 5.91 sec   HDFS Read: 142874170 HDFS Write: 29304904 SUCCESS
Job 1: Map: 298  Reduce: 78   Cumulative CPU: 8890.21 sec   HDFS Read: 79611505079 HDFS Write: 37657255497 SUCCESS
Job 2: Map: 173  Reduce: 49   FAIL
Total MapReduce CPU Time Spent: 0 days 2 hours 28 minutes 16 seconds 119 msec
Command exited with non-zero status 2
Time:2169.86
Running Hive query: tpch/q10_returned_item.hive
13/12/02 16:55:21 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 16:55:21 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 16:55:21 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 16:55:21 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 16:55:21 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 16:55:21 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 16:55:21 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.758 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.225 seconds
OK
Time taken: 0.207 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.056 seconds
Total MapReduce jobs = 7
Stage-1 is selected by condition resolver.
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0042, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0042/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0042
Hadoop job information for Stage-1: number of mappers: 78; number of reducers: 20
2013-12-02 17:07:20,778 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 908.44 sec
MapReduce Total cumulative CPU time: 15 minutes 8 seconds 440 msec
Ended Job = job_1386003473374_0042
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 17:07:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 17:07:23 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_16-55-30_711_5541573677989733017-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 17:07:23 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_16-55-30_711_5541573677989733017-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 17:07:23 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 17:07:23 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 17:07:23 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 17:07:23 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 17:07:23 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 17:07:23 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 17:07:23 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 05:07:24	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 05:07:25	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_16-55-30_711_5541573677989733017-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-12-02 05:07:25	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_16-55-30_711_5541573677989733017-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-12-02 05:07:25	End of local task; Time Taken: 0.886 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 7
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386003473374_0043, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0043/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0043
Hadoop job information for Stage-13: number of mappers: 5; number of reducers: 0
2013-12-02 17:09:53,279 Stage-13 map = 100%,  reduce = 0%, Cumulative CPU 69.67 sec
MapReduce Total cumulative CPU time: 1 minutes 9 seconds 670 msec
Ended Job = job_1386003473374_0043
Stage-17 is filtered out by condition resolver.
Stage-18 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0044, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0044/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0044
Hadoop job information for Stage-3: number of mappers: 303; number of reducers: 79
2013-12-02 17:18:34,885 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 3263.87 sec
MapReduce Total cumulative CPU time: 54 minutes 23 seconds 870 msec
Ended Job = job_1386003473374_0044
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0045, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0045/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0045
Hadoop job information for Stage-4: number of mappers: 8; number of reducers: 1
2013-12-02 17:19:50,484 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 121.94 sec
MapReduce Total cumulative CPU time: 2 minutes 1 seconds 940 msec
Ended Job = job_1386003473374_0045
Launching Job 5 out of 7
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0046, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0046/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0046
Hadoop job information for Stage-5: number of mappers: 4; number of reducers: 1
2013-12-02 17:20:36,644 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 61.44 sec
MapReduce Total cumulative CPU time: 1 minutes 1 seconds 440 msec
Ended Job = job_1386003473374_0046
Loading data to table default.q10_returned_item
Table default.q10_returned_item stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3597, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 78  Reduce: 20   Cumulative CPU: 908.44 sec   HDFS Read: 20257241310 HDFS Write: 980369913 SUCCESS
Job 1: Map: 5   Cumulative CPU: 69.67 sec   HDFS Read: 980375078 HDFS Write: 1021319200 SUCCESS
Job 2: Map: 303  Reduce: 79   Cumulative CPU: 3263.87 sec   HDFS Read: 80603539067 HDFS Write: 889025471 SUCCESS
Job 3: Map: 8  Reduce: 1   Cumulative CPU: 121.94 sec   HDFS Read: 889044169 HDFS Write: 704132397 SUCCESS
Job 4: Map: 4  Reduce: 1   Cumulative CPU: 61.44 sec   HDFS Read: 704143440 HDFS Write: 3597 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 13 minutes 45 seconds 360 msec
OK
Time taken: 1506.428 seconds
Time:1516.44
Running Hive query: tpch/q11_important_stock.hive
13/12/02 17:20:38 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 17:20:38 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 17:20:38 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 17:20:38 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 17:20:38 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 17:20:38 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 17:20:38 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.76 seconds
OK
Time taken: 0.104 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.225 seconds
OK
Time taken: 0.121 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.177 seconds
OK
Time taken: 0.076 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.077 seconds
Total MapReduce jobs = 5
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 17:20:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 17:20:51 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_17-20-47_287_7161652874056167956-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 17:20:51 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_17-20-47_287_7161652874056167956-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 17:20:52 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 17:20:52 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 17:20:52 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 17:20:52 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 17:20:52 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 17:20:52 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 17:20:52 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 05:20:52	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 05:20:54	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_17-20-47_287_7161652874056167956-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-12-02 05:20:54	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_17-20-47_287_7161652874056167956-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-12-02 05:20:54	End of local task; Time Taken: 1.159 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386003473374_0047, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0047/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0047
Hadoop job information for Stage-10: number of mappers: 1; number of reducers: 0
2013-12-02 17:21:09,607 Stage-10 map = 100%,  reduce = 0%, Cumulative CPU 4.43 sec
MapReduce Total cumulative CPU time: 4 seconds 430 msec
Ended Job = job_1386003473374_0047
Stage-11 is filtered out by condition resolver.
Stage-12 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0048, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0048/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0048
Hadoop job information for Stage-2: number of mappers: 47; number of reducers: 13
2013-12-02 17:23:28,453 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 732.19 sec
MapReduce Total cumulative CPU time: 12 minutes 12 seconds 190 msec
Ended Job = job_1386003473374_0048
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0049, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0049/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0049
Hadoop job information for Stage-3: number of mappers: 5; number of reducers: 1
2013-12-02 17:24:06,894 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 40.1 sec
MapReduce Total cumulative CPU time: 40 seconds 100 msec
Ended Job = job_1386003473374_0049
Loading data to table default.q11_part_tmp
Table default.q11_part_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 63173855, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 4.43 sec   HDFS Read: 142874170 HDFS Write: 846716 SUCCESS
Job 1: Map: 47  Reduce: 13   Cumulative CPU: 732.19 sec   HDFS Read: 12210439282 HDFS Write: 94289297 SUCCESS
Job 2: Map: 5  Reduce: 1   Cumulative CPU: 40.1 sec   HDFS Read: 94292908 HDFS Write: 63173855 SUCCESS
Total MapReduce CPU Time Spent: 12 minutes 56 seconds 720 msec
OK
Time taken: 200.099 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0050, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0050/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0050
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-12-02 17:24:32,237 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.85 sec
MapReduce Total cumulative CPU time: 6 seconds 850 msec
Ended Job = job_1386003473374_0050
Loading data to table default.q11_sum_tmp
Table default.q11_sum_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 6.85 sec   HDFS Read: 63174074 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 6 seconds 850 msec
OK
Time taken: 25.347 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 17:24:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 17:24:35 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_17-24-32_735_8054358952116118904-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 17:24:35 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_17-24-32_735_8054358952116118904-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 17:24:35 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 17:24:35 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 17:24:35 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 17:24:35 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 17:24:35 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 17:24:35 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 17:24:35 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 05:24:35	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 05:24:36	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_17-24-32_735_8054358952116118904-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-12-02 05:24:36	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_17-24-32_735_8054358952116118904-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-12-02 05:24:36	End of local task; Time Taken: 0.625 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0051, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0051/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0051
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-12-02 17:25:04,705 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.07 sec
MapReduce Total cumulative CPU time: 8 seconds 70 msec
Ended Job = job_1386003473374_0051
Loading data to table default.q11_important_stock
Table default.q11_important_stock stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 0, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 8.07 sec   HDFS Read: 63174074 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 70 msec
OK
Time taken: 32.441 seconds
Time:268.04
Running Hive query: tpch/q12_shipping.hive
13/12/02 17:25:06 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 17:25:06 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 17:25:06 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 17:25:06 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 17:25:06 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 17:25:06 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 17:25:06 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.83 seconds
OK
Time taken: 0.151 seconds
OK
Time taken: 0.264 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.056 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0052, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0052/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0052
Hadoop job information for Stage-1: number of mappers: 364; number of reducers: 80
2013-12-02 17:37:11,475 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3664.79 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 1 minutes 4 seconds 790 msec
Ended Job = job_1386003473374_0052
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0053, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0053/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0053
Hadoop job information for Stage-2: number of mappers: 7; number of reducers: 1
2013-12-02 17:37:29,937 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.66 sec
MapReduce Total cumulative CPU time: 6 seconds 660 msec
Ended Job = job_1386003473374_0053
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0054, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0054/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0054
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-02 17:37:48,939 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.02 sec
MapReduce Total cumulative CPU time: 2 seconds 20 msec
Ended Job = job_1386003473374_0054
Loading data to table default.q12_shipping
Table default.q12_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 46, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 364  Reduce: 80   Cumulative CPU: 3664.79 sec   HDFS Read: 97375874339 HDFS Write: 9920 SUCCESS
Job 1: Map: 7  Reduce: 1   Cumulative CPU: 6.66 sec   HDFS Read: 28695 HDFS Write: 156 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 2.02 sec   HDFS Read: 523 HDFS Write: 46 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 1 minutes 13 seconds 470 msec
OK
Time taken: 754.364 seconds
Time:764.27
Running Hive query: tpch/q13_customer_distribution.hive
13/12/02 17:37:50 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 17:37:50 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 17:37:50 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 17:37:50 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 17:37:50 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 17:37:50 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 17:37:50 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.736 seconds
OK
Time taken: 0.12 seconds
OK
Time taken: 0.225 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.057 seconds
Total MapReduce jobs = 4
Stage-1 is selected by condition resolver.
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 21
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0055, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0055/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0055
Hadoop job information for Stage-1: number of mappers: 77; number of reducers: 21
2013-12-02 17:41:59,263 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1734.32 sec
MapReduce Total cumulative CPU time: 28 minutes 54 seconds 320 msec
Ended Job = job_1386003473374_0055
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0056, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0056/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0056
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1
2013-12-02 17:43:47,327 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 113.54 sec
MapReduce Total cumulative CPU time: 1 minutes 53 seconds 540 msec
Ended Job = job_1386003473374_0056
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0057, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0057/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0057
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-02 17:44:19,139 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 12.46 sec
MapReduce Total cumulative CPU time: 12 seconds 460 msec
Ended Job = job_1386003473374_0057
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0058, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0058/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0058
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-02 17:44:50,160 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.98 sec
MapReduce Total cumulative CPU time: 1 seconds 980 msec
Ended Job = job_1386003473374_0058
Loading data to table default.q13_customer_distribution
Table default.q13_customer_distribution stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 392, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 21   Cumulative CPU: 1734.32 sec   HDFS Read: 20257241173 HDFS Write: 333230859 SUCCESS
Job 1: Map: 6  Reduce: 1   Cumulative CPU: 113.54 sec   HDFS Read: 333236349 HDFS Write: 1054 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 12.46 sec   HDFS Read: 1419 HDFS Write: 1054 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 1.98 sec   HDFS Read: 1419 HDFS Write: 392 SUCCESS
Total MapReduce CPU Time Spent: 31 minutes 2 seconds 300 msec
OK
Time taken: 411.521 seconds
Time:421.21
Running Hive query: tpch/q14_promotion_effect.hive
13/12/02 17:44:51 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 17:44:51 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 17:44:51 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 17:44:51 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 17:44:51 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 17:44:51 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 17:44:51 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.878 seconds
OK
Time taken: 0.328 seconds
OK
Time taken: 0.241 seconds
OK
Time taken: 0.235 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.048 seconds
Total MapReduce jobs = 2
Stage-1 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0059, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0059/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0059
Hadoop job information for Stage-1: number of mappers: 307; number of reducers: 79
2013-12-02 17:54:49,024 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2733.23 sec
MapReduce Total cumulative CPU time: 45 minutes 33 seconds 230 msec
Ended Job = job_1386003473374_0059
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0060, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0060/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0060
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1
2013-12-02 17:55:10,390 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.51 sec
MapReduce Total cumulative CPU time: 6 seconds 510 msec
Ended Job = job_1386003473374_0060
Loading data to table default.q14_promotion_effect
Table default.q14_promotion_effect stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 19, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 307  Reduce: 79   Cumulative CPU: 2733.23 sec   HDFS Read: 82035510185 HDFS Write: 10191 SUCCESS
Job 1: Map: 6  Reduce: 1   Cumulative CPU: 6.51 sec   HDFS Read: 28599 HDFS Write: 19 SUCCESS
Total MapReduce CPU Time Spent: 45 minutes 39 seconds 740 msec
OK
Time taken: 610.134 seconds
Time:620.25
Running Hive query: tpch/q15_top_supplier.hive
13/12/02 17:55:12 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 17:55:12 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 17:55:12 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 17:55:12 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 17:55:12 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 17:55:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 17:55:12 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.931 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.206 seconds
OK
Time taken: 0.111 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.229 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0061, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0061/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0061
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-02 18:03:50,363 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2644.36 sec
MapReduce Total cumulative CPU time: 44 minutes 4 seconds 360 msec
Ended Job = job_1386003473374_0061
Loading data to table default.revenue
Table default.revenue stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 22449343, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 2644.36 sec   HDFS Read: 79582199808 HDFS Write: 22449343 SUCCESS
Total MapReduce CPU Time Spent: 44 minutes 4 seconds 360 msec
OK
Time taken: 509.867 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0062, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0062/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0062
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 1
2013-12-02 18:04:10,570 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 16.59 sec
MapReduce Total cumulative CPU time: 16 seconds 590 msec
Ended Job = job_1386003473374_0062
Loading data to table default.max_revenue
Table default.max_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 13, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 7  Reduce: 1   Cumulative CPU: 16.59 sec   HDFS Read: 22456462 HDFS Write: 13 SUCCESS
Total MapReduce CPU Time Spent: 16 seconds 590 msec
OK
Time taken: 20.023 seconds
Total MapReduce jobs = 3
Stage-12 is selected by condition resolver.
Stage-1 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 18:04:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 18:04:14 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_18-04-11_060_2300181380639616183-1/-local-10009/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 18:04:14 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_18-04-11_060_2300181380639616183-1/-local-10009/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 18:04:14 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 18:04:14 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 18:04:14 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 18:04:14 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 18:04:14 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 18:04:14 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 18:04:14 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 06:04:15	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 06:04:17	Processing rows:	200000	Hashtable size:	199999	Memory usage:	96683824	percentage:	0.203
2013-12-02 06:04:18	Processing rows:	300000	Hashtable size:	299999	Memory usage:	86864680	percentage:	0.182
2013-12-02 06:04:19	Processing rows:	400000	Hashtable size:	399999	Memory usage:	186979992	percentage:	0.392
2013-12-02 06:04:20	Processing rows:	500000	Hashtable size:	499999	Memory usage:	177095616	percentage:	0.371
2013-12-02 06:04:21	Processing rows:	600000	Hashtable size:	599999	Memory usage:	204365720	percentage:	0.428
2013-12-02 06:04:21	Processing rows:	700000	Hashtable size:	699999	Memory usage:	175391272	percentage:	0.368
2013-12-02 06:04:23	Processing rows:	800000	Hashtable size:	799999	Memory usage:	214022192	percentage:	0.449
2013-12-02 06:04:25	Processing rows:	900000	Hashtable size:	899999	Memory usage:	218304200	percentage:	0.458
2013-12-02 06:04:25	Processing rows:	1000000	Hashtable size:	999999	Memory usage:	267772824	percentage:	0.561
2013-12-02 06:04:25	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_18-04-11_060_2300181380639616183-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile11--.hashtable
2013-12-02 06:04:26	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_18-04-11_060_2300181380639616183-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile11--.hashtable
2013-12-02 06:04:26	End of local task; Time Taken: 11.0 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386003473374_0063, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0063/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0063
Hadoop job information for Stage-8: number of mappers: 1; number of reducers: 0
2013-12-02 18:04:56,644 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 17.57 sec
MapReduce Total cumulative CPU time: 17 seconds 570 msec
Ended Job = job_1386003473374_0063
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 18:04:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 18:04:59 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_18-04-11_060_2300181380639616183-1/-local-10013/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 18:04:59 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_18-04-11_060_2300181380639616183-1/-local-10013/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 18:04:59 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 18:04:59 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 18:04:59 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 18:04:59 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 18:04:59 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 18:04:59 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 18:04:59 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 06:04:59	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 06:05:00	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_18-04-11_060_2300181380639616183-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-12-02 06:05:00	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_18-04-11_060_2300181380639616183-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-12-02 06:05:00	End of local task; Time Taken: 0.615 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0064, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0064/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0064
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-02 18:05:23,012 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 5.84 sec
MapReduce Total cumulative CPU time: 5 seconds 840 msec
Ended Job = job_1386003473374_0064
Loading data to table default.q15_top_supplier
Table default.q15_top_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 77, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 17.57 sec   HDFS Read: 142874170 HDFS Write: 90810899 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 5.84 sec   HDFS Read: 90811266 HDFS Write: 77 SUCCESS
Total MapReduce CPU Time Spent: 23 seconds 410 msec
OK
Time taken: 72.457 seconds
Time:612.57
Running Hive query: tpch/q16_parts_supplier_relationship.hive
13/12/02 18:05:24 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 18:05:24 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 18:05:24 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 18:05:24 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 18:05:24 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 18:05:24 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 18:05:24 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.605 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.206 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.195 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.081 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386003473374_0065, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0065/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0065
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2013-12-02 18:05:52,549 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.23 sec
MapReduce Total cumulative CPU time: 7 seconds 230 msec
Ended Job = job_1386003473374_0065
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-12-02_18-05-33_603_5282318956514125049-1/-ext-10000
Loading data to table default.supplier_tmp
Table default.supplier_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6885604, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 7.23 sec   HDFS Read: 142874170 HDFS Write: 6885604 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 230 msec
OK
Time taken: 19.613 seconds
Total MapReduce jobs = 2
Stage-3 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 15
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0066, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0066/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0066
Hadoop job information for Stage-3: number of mappers: 56; number of reducers: 15
2013-12-02 18:08:55,398 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1009.14 sec
MapReduce Total cumulative CPU time: 16 minutes 49 seconds 140 msec
Ended Job = job_1386003473374_0066
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 18:08:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 18:08:57 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_18-05-53_217_2897030094968287790-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 18:08:57 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_18-05-53_217_2897030094968287790-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 18:08:57 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 18:08:57 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 18:08:57 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 18:08:57 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 18:08:57 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 18:08:57 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 18:08:57 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 06:08:58	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 06:08:59	Processing rows:	200000	Hashtable size:	199999	Memory usage:	38614272	percentage:	0.081
2013-12-02 06:08:59	Processing rows:	300000	Hashtable size:	299999	Memory usage:	49664920	percentage:	0.104
2013-12-02 06:08:59	Processing rows:	400000	Hashtable size:	399999	Memory usage:	56827704	percentage:	0.119
2013-12-02 06:08:59	Processing rows:	500000	Hashtable size:	499999	Memory usage:	65781192	percentage:	0.138
2013-12-02 06:08:59	Processing rows:	600000	Hashtable size:	599999	Memory usage:	78929008	percentage:	0.165
2013-12-02 06:08:59	Processing rows:	700000	Hashtable size:	699999	Memory usage:	87882472	percentage:	0.184
2013-12-02 06:08:59	Processing rows:	800000	Hashtable size:	799999	Memory usage:	96835952	percentage:	0.203
2013-12-02 06:08:59	Processing rows:	900000	Hashtable size:	899999	Memory usage:	105789416	percentage:	0.222
2013-12-02 06:09:00	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_18-05-53_217_2897030094968287790-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-12-02 06:09:00	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_18-05-53_217_2897030094968287790-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-12-02 06:09:00	End of local task; Time Taken: 1.905 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386003473374_0067, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0067/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0067
Hadoop job information for Stage-5: number of mappers: 15; number of reducers: 0
2013-12-02 18:10:34,776 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 490.58 sec
MapReduce Total cumulative CPU time: 8 minutes 10 seconds 580 msec
Ended Job = job_1386003473374_0067
Loading data to table default.q16_tmp
Table default.q16_tmp stats: [num_partitions: 0, num_files: 15, num_rows: 0, total_size: 2989814109, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 56  Reduce: 15   Cumulative CPU: 1009.14 sec   HDFS Read: 14662902576 HDFS Write: 3937268429 SUCCESS
Job 1: Map: 15   Cumulative CPU: 490.58 sec   HDFS Read: 3937306272 HDFS Write: 2989814109 SUCCESS
Total MapReduce CPU Time Spent: 24 minutes 59 seconds 720 msec
OK
Time taken: 282.062 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0068, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0068/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0068
Hadoop job information for Stage-1: number of mappers: 11; number of reducers: 3
2013-12-02 18:11:42,673 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 306.48 sec
MapReduce Total cumulative CPU time: 5 minutes 6 seconds 480 msec
Ended Job = job_1386003473374_0068
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0069, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0069/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0069
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-12-02 18:12:05,962 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.12 sec
MapReduce Total cumulative CPU time: 8 seconds 120 msec
Ended Job = job_1386003473374_0069
Loading data to table default.q16_parts_supplier_relationship
Table default.q16_parts_supplier_relationship stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 1039440, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 11  Reduce: 3   Cumulative CPU: 306.48 sec   HDFS Read: 2989879366 HDFS Write: 1450608 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 8.12 sec   HDFS Read: 1451564 HDFS Write: 1039440 SUCCESS
Total MapReduce CPU Time Spent: 5 minutes 14 seconds 600 msec
OK
Time taken: 91.166 seconds
Time:402.91
Running Hive query: tpch/q17_small_quantity_order_revenue.hive
13/12/02 18:12:07 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 18:12:07 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 18:12:07 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 18:12:07 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 18:12:07 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 18:12:07 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 18:12:07 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.747 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.211 seconds
OK
Time taken: 0.125 seconds
OK
Time taken: 0.197 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.048 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0070, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0070/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0070
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-02 18:28:48,910 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6781.24 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 53 minutes 1 seconds 240 msec
Ended Job = job_1386003473374_0070
Loading data to table default.lineitem_tmp
Table default.lineitem_tmp stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 494293140, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 6781.24 sec   HDFS Read: 79582199808 HDFS Write: 494293140 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 53 minutes 1 seconds 240 msec
OK
Time taken: 993.194 seconds
Total MapReduce jobs = 5
Stage-1 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 83
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0071, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0071/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0071
Hadoop job information for Stage-1: number of mappers: 307; number of reducers: 83
2013-12-02 18:54:24,414 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7249.38 sec
MapReduce Total cumulative CPU time: 0 days 2 hours 0 minutes 49 seconds 380 msec
Ended Job = job_1386003473374_0071
Stage-13 is filtered out by condition resolver.
Stage-14 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0072, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0072/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0072
Hadoop job information for Stage-2: number of mappers: 12; number of reducers: 1
2013-12-02 18:55:50,760 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 168.83 sec
MapReduce Total cumulative CPU time: 2 minutes 48 seconds 830 msec
Ended Job = job_1386003473374_0072
Launching Job 3 out of 5
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0073, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0073/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0073
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-02 18:56:10,229 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.87 sec
MapReduce Total cumulative CPU time: 1 seconds 870 msec
Ended Job = job_1386003473374_0073
Loading data to table default.q17_small_quantity_order_revenue
Table default.q17_small_quantity_order_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 20, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 307  Reduce: 83   Cumulative CPU: 7249.38 sec   HDFS Read: 82035510185 HDFS Write: 22558316 SUCCESS
Job 1: Map: 12  Reduce: 1   Cumulative CPU: 168.83 sec   HDFS Read: 516878126 HDFS Write: 121 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.87 sec   HDFS Read: 488 HDFS Write: 20 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 3 minutes 40 seconds 80 msec
OK
Time taken: 1641.157 seconds
Time:2644.27
Running Hive query: tpch/q18_large_volume_customer.hive
13/12/02 18:56:12 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 18:56:12 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 18:56:12 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 18:56:12 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 18:56:12 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 18:56:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 18:56:12 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.629 seconds
OK
Time taken: 0.102 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.199 seconds
OK
Time taken: 0.162 seconds
OK
Time taken: 0.215 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.044 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 69
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0074, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0074/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0074
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 69
2013-12-02 19:06:12,238 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3752.93 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 2 minutes 32 seconds 930 msec
Ended Job = job_1386003473374_0074
Loading data to table default.q18_tmp
Table default.q18_tmp stats: [num_partitions: 0, num_files: 69, num_rows: 0, total_size: 2291717657, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 69   Cumulative CPU: 3752.93 sec   HDFS Read: 79582199808 HDFS Write: 2291717657 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 2 minutes 32 seconds 930 msec
OK
Time taken: 592.153 seconds
Total MapReduce jobs = 5
Stage-5 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0075, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0075/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0075
Hadoop job information for Stage-5: number of mappers: 77; number of reducers: 18
2013-12-02 19:13:34,649 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 2049.82 sec
MapReduce Total cumulative CPU time: 34 minutes 9 seconds 820 msec
Ended Job = job_1386003473374_0075
Stage-15 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0076, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0076/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0076
Hadoop job information for Stage-1: number of mappers: 344; number of reducers: 79
2013-12-02 19:31:39,323 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6851.89 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 54 minutes 11 seconds 890 msec
Ended Job = job_1386003473374_0076
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0077, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0077/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0077
Hadoop job information for Stage-2: number of mappers: 7; number of reducers: 1
2013-12-02 19:32:03,397 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 10.82 sec
MapReduce Total cumulative CPU time: 10 seconds 820 msec
Ended Job = job_1386003473374_0077
Launching Job 4 out of 5
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0078, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0078/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0078
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-02 19:32:33,085 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.96 sec
MapReduce Total cumulative CPU time: 2 seconds 960 msec
Ended Job = job_1386003473374_0078
Loading data to table default.q18_large_volume_customer
Table default.q18_large_volume_customer stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6391, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 18   Cumulative CPU: 2049.82 sec   HDFS Read: 20257241173 HDFS Write: 9688881598 SUCCESS
Job 1: Map: 344  Reduce: 79   Cumulative CPU: 6851.89 sec   HDFS Read: 91562986108 HDFS Write: 471842 SUCCESS
Job 2: Map: 7  Reduce: 1   Cumulative CPU: 10.82 sec   HDFS Read: 490395 HDFS Write: 465114 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 2.96 sec   HDFS Read: 465481 HDFS Write: 6391 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 28 minutes 35 seconds 490 msec
OK
Time taken: 1580.693 seconds
Time:2182.85
Running Hive query: tpch/q19_discounted_revenue.hive
13/12/02 19:32:34 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 19:32:34 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 19:32:34 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 19:32:34 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 19:32:34 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 19:32:34 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 19:32:34 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.86 seconds
OK
Time taken: 0.11 seconds
OK
Time taken: 0.213 seconds
OK
Time taken: 0.22 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.051 seconds
Total MapReduce jobs = 2
Stage-1 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0079, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0079/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0079
Hadoop job information for Stage-1: number of mappers: 307; number of reducers: 79
2013-12-02 19:54:17,180 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9278.38 sec
MapReduce Total cumulative CPU time: 0 days 2 hours 34 minutes 38 seconds 380 msec
Ended Job = job_1386003473374_0079
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0080, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0080/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0080
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1
2013-12-02 19:54:42,670 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.05 sec
MapReduce Total cumulative CPU time: 6 seconds 50 msec
Ended Job = job_1386003473374_0080
Loading data to table default.q19_discounted_revenue
Table default.q19_discounted_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 22, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 307  Reduce: 79   Cumulative CPU: 9278.38 sec   HDFS Read: 82035510185 HDFS Write: 9559 SUCCESS
Job 1: Map: 6  Reduce: 1   Cumulative CPU: 6.05 sec   HDFS Read: 27888 HDFS Write: 22 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 34 minutes 44 seconds 430 msec
OK
Time taken: 1319.771 seconds
Time:1329.61
Running Hive query: tpch/q20_potential_part_promotion.hive
13/12/02 19:54:44 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 19:54:44 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 19:54:44 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 19:54:44 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 19:54:44 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 19:54:44 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 19:54:44 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.661 seconds
OK
Time taken: 0.121 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.097 seconds
OK
Time taken: 0.209 seconds
OK
Time taken: 0.112 seconds
OK
Time taken: 0.125 seconds
OK
Time taken: 0.133 seconds
OK
Time taken: 0.125 seconds
OK
Time taken: 0.217 seconds
OK
Time taken: 0.037 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.085 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0081, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0081/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0081
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 3
2013-12-02 19:55:29,482 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 73.74 sec
MapReduce Total cumulative CPU time: 1 minutes 13 seconds 740 msec
Ended Job = job_1386003473374_0081
Loading data to table default.q20_tmp1
Table default.q20_tmp1 stats: [num_partitions: 0, num_files: 3, num_rows: 0, total_size: 1834396, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10  Reduce: 3   Cumulative CPU: 73.74 sec   HDFS Read: 2453310377 HDFS Write: 1834396 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 13 seconds 740 msec
OK
Time taken: 36.24 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0082, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0082/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0082
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-02 20:05:01,567 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3192.41 sec
MapReduce Total cumulative CPU time: 53 minutes 12 seconds 410 msec
Ended Job = job_1386003473374_0082
Loading data to table default.q20_tmp2
Table default.q20_tmp2 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 1096808654, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 3192.41 sec   HDFS Read: 79582199808 HDFS Write: 1096808654 SUCCESS
Total MapReduce CPU Time Spent: 53 minutes 12 seconds 410 msec
OK
Time taken: 572.069 seconds
Total MapReduce jobs = 4
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 20:05:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 20:05:05 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_20-05-02_137_1891779843468473279-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 20:05:05 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_20-05-02_137_1891779843468473279-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 20:05:05 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 20:05:05 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 20:05:05 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 20:05:05 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 20:05:05 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 20:05:05 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 20:05:05 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 08:05:06	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 08:05:07	Processing rows:	200000	Hashtable size:	199999	Memory usage:	41520608	percentage:	0.087
2013-12-02 08:05:07	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_20-05-02_137_1891779843468473279-1/-local-10007/HashTable-Stage-8/MapJoin-mapfile21--.hashtable
2013-12-02 08:05:07	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_20-05-02_137_1891779843468473279-1/-local-10007/HashTable-Stage-8/MapJoin-mapfile21--.hashtable
2013-12-02 08:05:07	End of local task; Time Taken: 1.349 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 4
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386003473374_0083, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0083/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0083
Hadoop job information for Stage-8: number of mappers: 46; number of reducers: 0
2013-12-02 20:06:18,739 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 324.7 sec
MapReduce Total cumulative CPU time: 5 minutes 24 seconds 700 msec
Ended Job = job_1386003473374_0083
Stage-9 is filtered out by condition resolver.
Stage-10 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0084, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0084/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0084
Hadoop job information for Stage-1: number of mappers: 15; number of reducers: 2
2013-12-02 20:08:56,055 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 447.98 sec
MapReduce Total cumulative CPU time: 7 minutes 27 seconds 980 msec
Ended Job = job_1386003473374_0084
Loading data to table default.q20_tmp3
Table default.q20_tmp3 stats: [num_partitions: 0, num_files: 2, num_rows: 0, total_size: 9821178, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 46   Cumulative CPU: 324.7 sec   HDFS Read: 12209592199 HDFS Write: 24620613 SUCCESS
Job 1: Map: 15  Reduce: 2   Cumulative CPU: 447.98 sec   HDFS Read: 1121447822 HDFS Write: 9821178 SUCCESS
Total MapReduce CPU Time Spent: 12 minutes 52 seconds 680 msec
OK
Time taken: 234.388 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0085, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0085/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0085
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2013-12-02 20:09:23,382 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.22 sec
MapReduce Total cumulative CPU time: 12 seconds 220 msec
Ended Job = job_1386003473374_0085
Loading data to table default.q20_tmp4
Table default.q20_tmp4 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3082950, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 12.22 sec   HDFS Read: 9821608 HDFS Write: 3082950 SUCCESS
Total MapReduce CPU Time Spent: 12 seconds 220 msec
OK
Time taken: 27.306 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 20:09:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 20:09:26 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_20-09-23_832_7306377824886900359-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 20:09:26 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_20-09-23_832_7306377824886900359-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 20:09:26 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 20:09:26 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 20:09:26 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 20:09:26 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 20:09:26 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 20:09:26 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 20:09:26 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 08:09:27	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 08:09:28	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_20-09-23_832_7306377824886900359-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile41--.hashtable
2013-12-02 08:09:28	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_20-09-23_832_7306377824886900359-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile41--.hashtable
2013-12-02 08:09:28	Processing rows:	200000	Hashtable size:	199999	Memory usage:	54693544	percentage:	0.115
2013-12-02 08:09:28	Processing rows:	300000	Hashtable size:	299999	Memory usage:	62388304	percentage:	0.131
2013-12-02 08:09:28	Processing rows:	400000	Hashtable size:	399999	Memory usage:	76201048	percentage:	0.16
2013-12-02 08:09:28	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_20-09-23_832_7306377824886900359-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-12-02 08:09:28	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_20-09-23_832_7306377824886900359-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-12-02 08:09:28	End of local task; Time Taken: 1.772 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0086, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0086/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0086
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-02 20:09:54,000 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 9.34 sec
MapReduce Total cumulative CPU time: 9 seconds 340 msec
Ended Job = job_1386003473374_0086
Loading data to table default.q20_potential_part_promotion
Table default.q20_potential_part_promotion stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 810032, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 9.34 sec   HDFS Read: 142874170 HDFS Write: 810032 SUCCESS
Total MapReduce CPU Time Spent: 9 seconds 340 msec
OK
Time taken: 30.615 seconds
Time:911.24
Running Hive query: tpch/q21_suppliers_who_kept_orders_waiting.hive
13/12/02 20:09:55 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 20:09:55 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 20:09:55 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 20:09:55 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 20:09:55 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 20:09:55 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 20:09:55 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.582 seconds
OK
Time taken: 0.12 seconds
OK
Time taken: 0.133 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.217 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.221 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0087, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0087/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0087
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-02 20:26:30,791 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6483.08 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 48 minutes 3 seconds 80 msec
Ended Job = job_1386003473374_0087
Loading data to table default.q21_tmp1
Table default.q21_tmp1 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 2819600047, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 6483.08 sec   HDFS Read: 79582199808 HDFS Write: 2819600047 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 48 minutes 3 seconds 80 msec
OK
Time taken: 986.655 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0088, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0088/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0088
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-02 20:38:43,360 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4849.04 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 20 minutes 49 seconds 40 msec
Ended Job = job_1386003473374_0088
Loading data to table default.q21_tmp2
Table default.q21_tmp2 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 2583844107, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 4849.04 sec   HDFS Read: 79582199808 HDFS Write: 2583844107 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 20 minutes 49 seconds 40 msec
OK
Time taken: 732.458 seconds
Total MapReduce jobs = 14
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 20:38:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 20:38:50 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_20-38-43_869_3032254139015109857-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 20:38:50 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_20-38-43_869_3032254139015109857-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 20:38:50 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 20:38:50 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 20:38:50 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 20:38:50 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 20:38:50 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 20:38:50 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 20:38:50 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 08:38:51	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 08:38:52	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_20-38-43_869_3032254139015109857-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-12-02 08:38:52	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_20-38-43_869_3032254139015109857-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-12-02 08:38:52	End of local task; Time Taken: 0.953 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 14
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386003473374_0089, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0089/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0089
Hadoop job information for Stage-24: number of mappers: 1; number of reducers: 0
2013-12-02 20:39:12,476 Stage-24 map = 100%,  reduce = 0%, Cumulative CPU 4.23 sec
MapReduce Total cumulative CPU time: 4 seconds 230 msec
Ended Job = job_1386003473374_0089
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 14
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0090, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0090/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0090
Hadoop job information for Stage-8: number of mappers: 298; number of reducers: 80
2013-12-02 20:49:46,338 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 4094.89 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 8 minutes 14 seconds 890 msec
Ended Job = job_1386003473374_0090
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 14
Number of reduce tasks not specified. Estimated from input data size: 19
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0091, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0091/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0091
Hadoop job information for Stage-1: number of mappers: 74; number of reducers: 19
2013-12-02 20:52:35,392 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 904.91 sec
MapReduce Total cumulative CPU time: 15 minutes 4 seconds 910 msec
Ended Job = job_1386003473374_0091
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 14
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0092, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0092/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0092
Hadoop job information for Stage-2: number of mappers: 15; number of reducers: 4
2013-12-02 20:55:33,402 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 984.81 sec
MapReduce Total cumulative CPU time: 16 minutes 24 seconds 810 msec
Ended Job = job_1386003473374_0092
Stage-25 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 5 out of 14
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0093, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0093/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0093
Hadoop job information for Stage-3: number of mappers: 15; number of reducers: 3
2013-12-02 21:01:40,892 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 980.15 sec
MapReduce Total cumulative CPU time: 16 minutes 20 seconds 150 msec
Ended Job = job_1386003473374_0093
Launching Job 6 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0094, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0094/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0094
Hadoop job information for Stage-4: number of mappers: 3; number of reducers: 1
2013-12-02 21:02:20,289 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 12.36 sec
MapReduce Total cumulative CPU time: 12 seconds 360 msec
Ended Job = job_1386003473374_0094
Launching Job 7 out of 14
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0095, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0095/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0095
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-12-02 21:02:41,244 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 5.01 sec
MapReduce Total cumulative CPU time: 5 seconds 10 msec
Ended Job = job_1386003473374_0095
Loading data to table default.q21_suppliers_who_kept_orders_waiting
Table default.q21_suppliers_who_kept_orders_waiting stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 2200, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 4.23 sec   HDFS Read: 142874170 HDFS Write: 1611510 SUCCESS
Job 1: Map: 298  Reduce: 80   Cumulative CPU: 4094.89 sec   HDFS Read: 79583811685 HDFS Write: 686901797 SUCCESS
Job 2: Map: 74  Reduce: 19   Cumulative CPU: 904.91 sec   HDFS Read: 18480595103 HDFS Write: 331675044 SUCCESS
Job 3: Map: 15  Reduce: 4   Cumulative CPU: 984.81 sec   HDFS Read: 3151287636 HDFS Write: 319626840 SUCCESS
Job 4: Map: 15  Reduce: 3   Cumulative CPU: 980.15 sec   HDFS Read: 2903480154 HDFS Write: 4525304 SUCCESS
Job 5: Map: 3  Reduce: 1   Cumulative CPU: 12.36 sec   HDFS Read: 4526405 HDFS Write: 1492766 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 5.01 sec   HDFS Read: 1493133 HDFS Write: 2200 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 56 minutes 26 seconds 360 msec
OK
Time taken: 1437.836 seconds
Time:3167.25
Running Hive query: tpch/q22_global_sales_opportunity.hive
13/12/02 21:02:42 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 21:02:42 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 21:02:42 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 21:02:42 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 21:02:42 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 21:02:42 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 21:02:42 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.834 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.112 seconds
OK
Time taken: 0.19 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.068 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386003473374_0096, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0096/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0096
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 0
2013-12-02 21:03:22,039 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 92.95 sec
MapReduce Total cumulative CPU time: 1 minutes 32 seconds 950 msec
Ended Job = job_1386003473374_0096
Stage-4 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Stage-5 is filtered out by condition resolver.
Launching Job 3 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1386003473374_0097, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0097/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0097
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
2013-12-02 21:03:42,557 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 10.07 sec
MapReduce Total cumulative CPU time: 10 seconds 70 msec
Ended Job = job_1386003473374_0097
Loading data to table default.q22_customer_tmp
Table default.q22_customer_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 80028920, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10   Cumulative CPU: 92.95 sec   HDFS Read: 2463566642 HDFS Write: 80028920 SUCCESS
Job 1: Map: 1   Cumulative CPU: 10.07 sec   HDFS Read: 80030327 HDFS Write: 80028920 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 43 seconds 20 msec
OK
Time taken: 51.137 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0098, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0098/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0098
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-12-02 21:04:11,699 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.62 sec
MapReduce Total cumulative CPU time: 7 seconds 620 msec
Ended Job = job_1386003473374_0098
Loading data to table default.q22_customer_tmp1
Table default.q22_customer_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 18, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 7.62 sec   HDFS Read: 80029143 HDFS Write: 18 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 620 msec
OK
Time taken: 29.049 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0099, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0099/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0099
Hadoop job information for Stage-1: number of mappers: 67; number of reducers: 18
2013-12-02 21:07:08,663 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1053.88 sec
MapReduce Total cumulative CPU time: 17 minutes 33 seconds 880 msec
Ended Job = job_1386003473374_0099
Loading data to table default.q22_orders_tmp
Table default.q22_orders_tmp stats: [num_partitions: 0, num_files: 18, num_rows: 0, total_size: 82591212, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 67  Reduce: 18   Cumulative CPU: 1053.88 sec   HDFS Read: 17793674531 HDFS Write: 82591212 SUCCESS
Total MapReduce CPU Time Spent: 17 minutes 33 seconds 880 msec
OK
Time taken: 176.946 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0100, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0100/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0100
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 1
2013-12-02 21:08:26,862 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 112.93 sec
MapReduce Total cumulative CPU time: 1 minutes 52 seconds 930 msec
Ended Job = job_1386003473374_0100
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/02 21:08:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/02 21:08:29 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_21-07-09_157_6579519670797554753-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/02 21:08:29 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-02_21-07-09_157_6579519670797554753-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/02 21:08:29 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/02 21:08:29 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/02 21:08:29 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/02 21:08:29 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/02 21:08:29 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/02 21:08:29 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/02 21:08:29 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-02 09:08:30	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-02 09:08:30	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-02_21-07-09_157_6579519670797554753-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-12-02 09:08:30	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-02_21-07-09_157_6579519670797554753-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-12-02 09:08:30	End of local task; Time Taken: 0.669 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0101, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0101/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0101
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-02 21:08:55,241 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 7.97 sec
MapReduce Total cumulative CPU time: 7 seconds 970 msec
Ended Job = job_1386003473374_0101
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1386003473374_0102, Tracking URL = http://10.6.40.110/proxy/application_1386003473374_0102/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1386003473374_0102
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-02 21:09:13,086 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 2.05 sec
MapReduce Total cumulative CPU time: 2 seconds 50 msec
Ended Job = job_1386003473374_0102
Loading data to table default.q22_global_sales_opportunity
Table default.q22_global_sales_opportunity stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 202, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 7  Reduce: 1   Cumulative CPU: 112.93 sec   HDFS Read: 162622689 HDFS Write: 39621076 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 7.97 sec   HDFS Read: 39621443 HDFS Write: 320 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 2.05 sec   HDFS Read: 687 HDFS Write: 202 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 2 seconds 950 msec
OK
Time taken: 124.397 seconds
Time:391.85
