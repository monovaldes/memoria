Running Hive from /opt/hive-0.12.0
Running Hadoop from 
Running Hive query: tpch/q1_pricing_summary_report.hive
13/12/01 15:05:15 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 15:05:15 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 15:05:15 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 15:05:15 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 15:05:15 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 15:05:15 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 15:05:15 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.855 seconds
OK
Time taken: 0.233 seconds
OK
Time taken: 0.249 seconds
OK
Time taken: 0.059 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0105, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0105/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0105
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-01 15:12:39,981 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3639.75 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 0 minutes 39 seconds 750 msec
Ended Job = job_1385815888185_0105
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0106, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0106/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0106
Hadoop job information for Stage-2: number of mappers: 8; number of reducers: 1
2013-12-01 15:13:01,117 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.41 sec
MapReduce Total cumulative CPU time: 7 seconds 410 msec
Ended Job = job_1385815888185_0106
Loading data to table default.q1_pricing_summary_report
Table default.q1_pricing_summary_report stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 589, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 3639.75 sec   HDFS Read: 79582199808 HDFS Write: 8011 SUCCESS
Job 1: Map: 8  Reduce: 1   Cumulative CPU: 7.41 sec   HDFS Read: 26931 HDFS Write: 589 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 0 minutes 47 seconds 160 msec
OK
Time taken: 457.135 seconds
Time:467.05
Running Hive query: tpch/q2_minimum_cost_supplier.hive
13/12/01 15:13:02 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 15:13:02 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 15:13:02 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 15:13:02 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 15:13:02 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 15:13:02 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 15:13:02 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.791 seconds
OK
Time taken: 0.121 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.202 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.207 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.059 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.102 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.06 seconds
Total MapReduce jobs = 10
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 15:13:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 15:13:18 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_15-13-12_214_2395995840595718755-1/-local-10020/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 15:13:18 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_15-13-12_214_2395995840595718755-1/-local-10020/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 15:13:18 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 15:13:18 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 15:13:18 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 15:13:18 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 15:13:18 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 15:13:18 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 15:13:18 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 03:13:19	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 03:13:20	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_15-13-12_214_2395995840595718755-1/-local-10017/HashTable-Stage-18/MapJoin-mapfile61--.hashtable
2013-12-01 03:13:20	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_15-13-12_214_2395995840595718755-1/-local-10017/HashTable-Stage-18/MapJoin-mapfile61--.hashtable
2013-12-01 03:13:20	End of local task; Time Taken: 0.987 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0107, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0107/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0107
Hadoop job information for Stage-18: number of mappers: 1; number of reducers: 0
2013-12-01 15:13:33,956 Stage-18 map = 100%,  reduce = 0%, Cumulative CPU 0.95 sec
MapReduce Total cumulative CPU time: 950 msec
Ended Job = job_1385815888185_0107
Stage-23 is filtered out by condition resolver.
Stage-24 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0108, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0108/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0108
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-12-01 15:14:02,568 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 20.72 sec
MapReduce Total cumulative CPU time: 20 seconds 720 msec
Ended Job = job_1385815888185_0108
Stage-21 is filtered out by condition resolver.
Stage-22 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 3 out of 10
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0109, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0109/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0109
Hadoop job information for Stage-2: number of mappers: 47; number of reducers: 13
2013-12-01 15:15:46,054 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 769.8 sec
MapReduce Total cumulative CPU time: 12 minutes 49 seconds 800 msec
Ended Job = job_1385815888185_0109
Stage-19 is filtered out by condition resolver.
Stage-20 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 4 out of 10
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0110, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0110/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0110
Hadoop job information for Stage-3: number of mappers: 23; number of reducers: 6
2013-12-01 15:17:06,063 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 358.43 sec
MapReduce Total cumulative CPU time: 5 minutes 58 seconds 430 msec
Ended Job = job_1385815888185_0110
Loading data to table default.q2_minimum_cost_supplier_tmp1
Table default.q2_minimum_cost_supplier_tmp1 stats: [num_partitions: 0, num_files: 6, num_rows: 0, total_size: 10939639, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.95 sec   HDFS Read: 2424 HDFS Write: 231 SUCCESS
Job 1: Map: 3  Reduce: 1   Cumulative CPU: 20.72 sec   HDFS Read: 142874905 HDFS Write: 32875244 SUCCESS
Job 2: Map: 47  Reduce: 13   Cumulative CPU: 769.8 sec   HDFS Read: 12242467810 HDFS Write: 2763176656 SUCCESS
Job 3: Map: 23  Reduce: 6   Cumulative CPU: 358.43 sec   HDFS Read: 5216532227 HDFS Write: 10939639 SUCCESS
Total MapReduce CPU Time Spent: 19 minutes 9 seconds 900 msec
OK
Time taken: 234.336 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0111, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0111/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0111
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1
2013-12-01 15:17:35,958 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 16.27 sec
MapReduce Total cumulative CPU time: 16 seconds 270 msec
Ended Job = job_1385815888185_0111
Loading data to table default.q2_minimum_cost_supplier_tmp2
Table default.q2_minimum_cost_supplier_tmp2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 715857, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 6  Reduce: 1   Cumulative CPU: 16.27 sec   HDFS Read: 10941055 HDFS Write: 715857 SUCCESS
Total MapReduce CPU Time Spent: 16 seconds 270 msec
OK
Time taken: 29.906 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 15:17:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 15:17:38 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_15-17-36_457_1106584519483672873-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 15:17:38 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_15-17-36_457_1106584519483672873-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 15:17:39 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 15:17:39 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 15:17:39 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 15:17:39 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 15:17:39 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 15:17:39 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 15:17:39 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 03:17:39	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 03:17:40	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_15-17-36_457_1106584519483672873-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile71--.hashtable
2013-12-01 03:17:40	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_15-17-36_457_1106584519483672873-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile71--.hashtable
2013-12-01 03:17:40	End of local task; Time Taken: 1.157 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0112, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0112/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0112
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1
2013-12-01 15:18:04,123 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 18.85 sec
MapReduce Total cumulative CPU time: 18 seconds 850 msec
Ended Job = job_1385815888185_0112
Loading data to table default.q2_minimum_cost_supplier
Table default.q2_minimum_cost_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16261, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 6  Reduce: 1   Cumulative CPU: 18.85 sec   HDFS Read: 10941055 HDFS Write: 16261 SUCCESS
Total MapReduce CPU Time Spent: 18 seconds 850 msec
OK
Time taken: 28.132 seconds
Time:302.97
Running Hive query: tpch/q3_shipping_priority.hive
13/12/01 15:18:05 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 15:18:05 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 15:18:05 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 15:18:05 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 15:18:05 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 15:18:05 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 15:18:05 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.823 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.193 seconds
OK
Time taken: 0.198 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.058 seconds
Total MapReduce jobs = 6
Stage-1 is selected by condition resolver.
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0113, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0113/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0113
Hadoop job information for Stage-1: number of mappers: 78; number of reducers: 20
2013-12-01 15:20:07,755 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 948.39 sec
MapReduce Total cumulative CPU time: 15 minutes 48 seconds 390 msec
Ended Job = job_1385815888185_0113
Stage-14 is filtered out by condition resolver.
Stage-15 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0114, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0114/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0114
Hadoop job information for Stage-2: number of mappers: 305; number of reducers: 79
2013-12-01 15:27:33,520 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4230.72 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 10 minutes 30 seconds 720 msec
Ended Job = job_1385815888185_0114
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0115, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0115/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0115
Hadoop job information for Stage-3: number of mappers: 10; number of reducers: 1
2013-12-01 15:28:01,443 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 47.14 sec
MapReduce Total cumulative CPU time: 47 seconds 140 msec
Ended Job = job_1385815888185_0115
Launching Job 4 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0116, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0116/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0116
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-01 15:28:30,478 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 12.41 sec
MapReduce Total cumulative CPU time: 12 seconds 410 msec
Ended Job = job_1385815888185_0116
Loading data to table default.q3_shipping_priority
Table default.q3_shipping_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 383, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 78  Reduce: 20   Cumulative CPU: 948.39 sec   HDFS Read: 20257241310 HDFS Write: 499838544 SUCCESS
Job 1: Map: 305  Reduce: 79   Cumulative CPU: 4230.72 sec   HDFS Read: 80082043932 HDFS Write: 47949847 SUCCESS
Job 2: Map: 10  Reduce: 1   Cumulative CPU: 47.14 sec   HDFS Read: 47968756 HDFS Write: 47942799 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 12.41 sec   HDFS Read: 47943165 HDFS Write: 383 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 27 minutes 18 seconds 660 msec
OK
Time taken: 616.471 seconds
Time:626.36
Running Hive query: tpch/q4_order_priority.hive
13/12/01 15:28:32 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 15:28:32 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 15:28:32 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 15:28:32 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 15:28:32 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 15:28:32 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 15:28:32 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.849 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.217 seconds
OK
Time taken: 0.135 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0117, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0117/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0117
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-01 15:34:48,756 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3007.51 sec
MapReduce Total cumulative CPU time: 50 minutes 7 seconds 510 msec
Ended Job = job_1385815888185_0117
Loading data to table default.q4_order_priority_tmp
Table default.q4_order_priority_tmp stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 1350015083, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 3007.51 sec   HDFS Read: 79582199808 HDFS Write: 1350015083 SUCCESS
Total MapReduce CPU Time Spent: 50 minutes 7 seconds 510 msec
OK
Time taken: 368.424 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0118, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0118/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0118
Hadoop job information for Stage-1: number of mappers: 79; number of reducers: 20
2013-12-01 15:37:58,387 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1201.91 sec
MapReduce Total cumulative CPU time: 20 minutes 1 seconds 910 msec
Ended Job = job_1385815888185_0118
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0119, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0119/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0119
Hadoop job information for Stage-2: number of mappers: 7; number of reducers: 1
2013-12-01 15:38:17,374 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.09 sec
MapReduce Total cumulative CPU time: 5 seconds 90 msec
Ended Job = job_1385815888185_0119
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0120, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0120/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0120
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-01 15:38:36,338 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.76 sec
MapReduce Total cumulative CPU time: 1 seconds 760 msec
Ended Job = job_1385815888185_0120
Loading data to table default.q4_order_priority
Table default.q4_order_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 87, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 79  Reduce: 20   Cumulative CPU: 1201.91 sec   HDFS Read: 19143698538 HDFS Write: 4860 SUCCESS
Job 1: Map: 7  Reduce: 1   Cumulative CPU: 5.09 sec   HDFS Read: 10315 HDFS Write: 248 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.76 sec   HDFS Read: 615 HDFS Write: 87 SUCCESS
Total MapReduce CPU Time Spent: 20 minutes 8 seconds 760 msec
OK
Time taken: 227.354 seconds
Time:605.83
Running Hive query: tpch/q5_local_supplier_volume.hive
13/12/01 15:38:38 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 15:38:38 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 15:38:38 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 15:38:38 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 15:38:38 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 15:38:38 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 15:38:38 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.804 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.112 seconds
OK
Time taken: 0.104 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.231 seconds
OK
Time taken: 0.211 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.058 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 15:38:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 15:38:55 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_15-38-47_225_695753903779705757-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 15:38:55 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_15-38-47_225_695753903779705757-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 15:38:55 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 15:38:55 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 15:38:55 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 15:38:55 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 15:38:55 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 15:38:55 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 15:38:55 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 03:38:55	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 03:38:56	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_15-38-47_225_695753903779705757-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile81--.hashtable
2013-12-01 03:38:56	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_15-38-47_225_695753903779705757-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile81--.hashtable
2013-12-01 03:38:56	End of local task; Time Taken: 1.024 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0121, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0121/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0121
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-12-01 15:39:10,388 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 0.97 sec
MapReduce Total cumulative CPU time: 970 msec
Ended Job = job_1385815888185_0121
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0122, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0122/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0122
Hadoop job information for Stage-7: number of mappers: 3; number of reducers: 1
2013-12-01 15:39:33,171 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 14.74 sec
MapReduce Total cumulative CPU time: 14 seconds 740 msec
Ended Job = job_1385815888185_0122
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0123, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0123/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0123
Hadoop job information for Stage-8: number of mappers: 298; number of reducers: 80
2013-12-01 15:50:33,418 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 6542.77 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 49 minutes 2 seconds 770 msec
Ended Job = job_1385815888185_0123
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 24
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0124, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0124/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0124
Hadoop job information for Stage-1: number of mappers: 87; number of reducers: 24
2013-12-01 15:53:50,540 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1797.64 sec
MapReduce Total cumulative CPU time: 29 minutes 57 seconds 640 msec
Ended Job = job_1385815888185_0124
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0125, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0125/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0125
Hadoop job information for Stage-2: number of mappers: 19; number of reducers: 4
2013-12-01 15:55:06,354 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 310.49 sec
MapReduce Total cumulative CPU time: 5 minutes 10 seconds 490 msec
Ended Job = job_1385815888185_0125
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0126, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0126/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0126
Hadoop job information for Stage-3: number of mappers: 3; number of reducers: 1
2013-12-01 15:55:24,728 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.67 sec
MapReduce Total cumulative CPU time: 2 seconds 670 msec
Ended Job = job_1385815888185_0126
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0127, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0127/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0127
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-01 15:55:43,240 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.42 sec
MapReduce Total cumulative CPU time: 1 seconds 420 msec
Ended Job = job_1385815888185_0127
Loading data to table default.q5_local_supplier_volume
Table default.q5_local_supplier_volume stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 136, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.97 sec   HDFS Read: 2424 HDFS Write: 222 SUCCESS
Job 1: Map: 3  Reduce: 1   Cumulative CPU: 14.74 sec   HDFS Read: 142874895 HDFS Write: 5881020 SUCCESS
Job 2: Map: 298  Reduce: 80   Cumulative CPU: 6542.77 sec   HDFS Read: 79588081194 HDFS Write: 5591627689 SUCCESS
Job 3: Map: 87  Reduce: 24   Cumulative CPU: 1797.64 sec   HDFS Read: 23385322800 HDFS Write: 830634778 SUCCESS
Job 4: Map: 19  Reduce: 4   Cumulative CPU: 310.49 sec   HDFS Read: 3294208021 HDFS Write: 1028 SUCCESS
Job 5: Map: 3  Reduce: 1   Cumulative CPU: 2.67 sec   HDFS Read: 2347 HDFS Write: 257 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.42 sec   HDFS Read: 623 HDFS Write: 136 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 24 minutes 30 seconds 700 msec
OK
Time taken: 1016.537 seconds
Time:1026.95
Running Hive query: tpch/q6_forecast_revenue_change.hive
13/12/01 15:55:45 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 15:55:45 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 15:55:45 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 15:55:45 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 15:55:45 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 15:55:45 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 15:55:45 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.934 seconds
OK
Time taken: 0.21 seconds
OK
Time taken: 0.217 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0128, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0128/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0128
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 1
2013-12-01 15:59:30,137 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2028.98 sec
MapReduce Total cumulative CPU time: 33 minutes 48 seconds 980 msec
Ended Job = job_1385815888185_0128
Loading data to table default.q6_forecast_revenue_change
Table default.q6_forecast_revenue_change stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 20, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 1   Cumulative CPU: 2028.98 sec   HDFS Read: 79582199808 HDFS Write: 20 SUCCESS
Total MapReduce CPU Time Spent: 33 minutes 48 seconds 980 msec
OK
Time taken: 217.249 seconds
Time:227.02
Running Hive query: tpch/q7_volume_shipping.hive
13/12/01 15:59:32 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 15:59:32 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 15:59:32 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 15:59:32 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 15:59:32 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 15:59:32 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 15:59:32 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.644 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.119 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.24 seconds
OK
Time taken: 0.296 seconds
OK
Time taken: 0.212 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.044 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 3
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 15:59:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 15:59:44 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_15-59-41_238_6438930909514562925-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 15:59:44 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_15-59-41_238_6438930909514562925-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 15:59:44 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 15:59:44 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 15:59:44 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 15:59:44 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 15:59:44 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 15:59:44 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 15:59:44 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 03:59:45	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 03:59:46	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_15-59-41_238_6438930909514562925-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-12-01 03:59:46	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_15-59-41_238_6438930909514562925-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-12-01 03:59:46	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_15-59-41_238_6438930909514562925-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-12-01 03:59:46	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_15-59-41_238_6438930909514562925-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-12-01 03:59:46	End of local task; Time Taken: 1.143 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0129, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0129/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0129
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 0
2013-12-01 16:00:01,490 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.54 sec
MapReduce Total cumulative CPU time: 1 seconds 540 msec
Ended Job = job_1385815888185_0129
Stage-5 is selected by condition resolver.
Stage-4 is filtered out by condition resolver.
Stage-6 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-12-01_15-59-41_238_6438930909514562925-1/-ext-10000
Loading data to table default.q7_volume_shipping_tmp
Table default.q7_volume_shipping_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 38, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.54 sec   HDFS Read: 2424 HDFS Write: 38 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 540 msec
OK
Time taken: 20.79 seconds
Total MapReduce jobs = 9
Stage-6 is selected by condition resolver.
Launching Job 1 out of 9
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0130, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0130/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0130
Hadoop job information for Stage-6: number of mappers: 364; number of reducers: 80
2013-12-01 16:10:05,361 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 4903.81 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 21 minutes 43 seconds 810 msec
Ended Job = job_1385815888185_0130
Stage-24 is filtered out by condition resolver.
Stage-25 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 9
Number of reduce tasks not specified. Estimated from input data size: 10
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0131, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0131/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0131
Hadoop job information for Stage-7: number of mappers: 41; number of reducers: 10
2013-12-01 16:15:13,239 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 2112.74 sec
MapReduce Total cumulative CPU time: 35 minutes 12 seconds 740 msec
Ended Job = job_1385815888185_0131
Stage-22 is filtered out by condition resolver.
Stage-23 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 9
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0132, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0132/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0132
Hadoop job information for Stage-1: number of mappers: 33; number of reducers: 8
2013-12-01 16:19:49,575 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2006.7 sec
MapReduce Total cumulative CPU time: 33 minutes 26 seconds 700 msec
Ended Job = job_1385815888185_0132
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 16:19:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 16:19:51 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_16-00-02_035_8417767594564327816-1/-local-10024/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 16:19:51 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_16-00-02_035_8417767594564327816-1/-local-10024/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 16:19:51 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 16:19:51 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 16:19:51 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 16:19:51 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 16:19:51 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 16:19:51 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 16:19:51 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 04:19:52	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 04:19:53	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_16-00-02_035_8417767594564327816-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-12-01 04:19:53	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_16-00-02_035_8417767594564327816-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-12-01 04:19:53	End of local task; Time Taken: 0.643 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 4 out of 9
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0133, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0133/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0133
Hadoop job information for Stage-3: number of mappers: 29; number of reducers: 7
2013-12-01 16:20:55,455 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 440.85 sec
MapReduce Total cumulative CPU time: 7 minutes 20 seconds 850 msec
Ended Job = job_1385815888185_0133
Launching Job 5 out of 9
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0134, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0134/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0134
Hadoop job information for Stage-4: number of mappers: 3; number of reducers: 1
2013-12-01 16:21:13,841 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 2.77 sec
MapReduce Total cumulative CPU time: 2 seconds 770 msec
Ended Job = job_1385815888185_0134
Loading data to table default.q7_volume_shipping
Table default.q7_volume_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 161, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 364  Reduce: 80   Cumulative CPU: 4903.81 sec   HDFS Read: 97375874339 HDFS Write: 9560339811 SUCCESS
Job 1: Map: 41  Reduce: 10   Cumulative CPU: 2112.74 sec   HDFS Read: 12024017736 HDFS Write: 9009669033 SUCCESS
Job 2: Map: 33  Reduce: 8   Cumulative CPU: 2006.7 sec   HDFS Read: 9152688342 HDFS Write: 8468719588 SUCCESS
Job 3: Map: 29  Reduce: 7   Cumulative CPU: 440.85 sec   HDFS Read: 8468877441 HDFS Write: 844 SUCCESS
Job 4: Map: 3  Reduce: 1   Cumulative CPU: 2.77 sec   HDFS Read: 2833 HDFS Write: 161 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 37 minutes 46 seconds 870 msec
OK
Time taken: 1272.248 seconds
Time:1303.48
Running Hive query: tpch/q8_national_market_share.hive
13/12/01 16:21:15 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 16:21:15 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 16:21:15 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 16:21:15 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 16:21:15 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 16:21:15 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 16:21:15 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.828 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.133 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.19 seconds
OK
Time taken: 0.202 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.035 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 18
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 16:21:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 16:21:34 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_16-21-24_875_3258674486938086265-1/-local-10035/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 16:21:34 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_16-21-24_875_3258674486938086265-1/-local-10035/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 16:21:34 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 16:21:34 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 16:21:34 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 16:21:34 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 16:21:34 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 16:21:34 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 16:21:34 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 04:21:35	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 04:21:36	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_16-21-24_875_3258674486938086265-1/-local-10032/HashTable-Stage-32/MapJoin-mapfile111--.hashtable
2013-12-01 04:21:36	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_16-21-24_875_3258674486938086265-1/-local-10032/HashTable-Stage-32/MapJoin-mapfile111--.hashtable
2013-12-01 04:21:36	End of local task; Time Taken: 0.933 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 18
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0135, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0135/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0135
Hadoop job information for Stage-32: number of mappers: 1; number of reducers: 0
2013-12-01 16:21:49,323 Stage-32 map = 100%,  reduce = 0%, Cumulative CPU 0.96 sec
MapReduce Total cumulative CPU time: 960 msec
Ended Job = job_1385815888185_0135
Stage-42 is filtered out by condition resolver.
Stage-43 is filtered out by condition resolver.
Stage-9 is selected by condition resolver.
Launching Job 2 out of 18
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0136, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0136/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0136
Hadoop job information for Stage-9: number of mappers: 12; number of reducers: 3
2013-12-01 16:22:30,559 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 122.66 sec
MapReduce Total cumulative CPU time: 2 minutes 2 seconds 660 msec
Ended Job = job_1385815888185_0136
Stage-40 is filtered out by condition resolver.
Stage-41 is filtered out by condition resolver.
Stage-10 is selected by condition resolver.
Launching Job 3 out of 18
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0137, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0137/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0137
Hadoop job information for Stage-10: number of mappers: 69; number of reducers: 18
2013-12-01 16:24:12,923 Stage-10 map = 100%,  reduce = 100%, Cumulative CPU 743.3 sec
MapReduce Total cumulative CPU time: 12 minutes 23 seconds 300 msec
Ended Job = job_1385815888185_0137
Stage-38 is filtered out by condition resolver.
Stage-39 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 18
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0138, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0138/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0138
Hadoop job information for Stage-1: number of mappers: 303; number of reducers: 80
2013-12-01 16:36:42,560 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6222.99 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 43 minutes 42 seconds 990 msec
Ended Job = job_1385815888185_0138
Stage-36 is filtered out by condition resolver.
Stage-37 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 18
Number of reduce tasks not specified. Estimated from input data size: 5
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0139, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0139/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0139
Hadoop job information for Stage-2: number of mappers: 22; number of reducers: 5
2013-12-01 16:38:12,415 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 432.43 sec
MapReduce Total cumulative CPU time: 7 minutes 12 seconds 430 msec
Ended Job = job_1385815888185_0139
Stage-34 is filtered out by condition resolver.
Stage-35 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 6 out of 18
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0140, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0140/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0140
Hadoop job information for Stage-3: number of mappers: 5; number of reducers: 1
2013-12-01 16:38:40,400 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 23.36 sec
MapReduce Total cumulative CPU time: 23 seconds 360 msec
Ended Job = job_1385815888185_0140
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 16:38:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 16:38:42 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_16-21-24_875_3258674486938086265-1/-local-10049/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 16:38:42 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_16-21-24_875_3258674486938086265-1/-local-10049/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 16:38:42 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 16:38:42 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 16:38:42 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 16:38:42 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 16:38:42 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 16:38:42 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 16:38:42 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 04:38:43	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 04:38:44	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_16-21-24_875_3258674486938086265-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-12-01 04:38:44	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_16-21-24_875_3258674486938086265-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-12-01 04:38:44	End of local task; Time Taken: 0.772 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 7 out of 18
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0141, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0141/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0141
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-12-01 16:39:07,558 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 7.44 sec
MapReduce Total cumulative CPU time: 7 seconds 440 msec
Ended Job = job_1385815888185_0141
Launching Job 8 out of 18
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0142, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0142/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0142
Ended Job = job_1385815888185_0142
Loading data to table default.q8_national_market_share
Table default.q8_national_market_share stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 49, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.96 sec   HDFS Read: 2424 HDFS Write: 186 SUCCESS
Job 1: Map: 12  Reduce: 3   Cumulative CPU: 122.66 sec   HDFS Read: 2463567332 HDFS Write: 63652437 SUCCESS
Job 2: Map: 69  Reduce: 18   Cumulative CPU: 743.3 sec   HDFS Read: 17857327924 HDFS Write: 303326332 SUCCESS
Job 3: Map: 303  Reduce: 80   Cumulative CPU: 6222.99 sec   HDFS Read: 79885531006 HDFS Write: 1917158445 SUCCESS
Job 4: Map: 22  Reduce: 5   Cumulative CPU: 432.43 sec   HDFS Read: 4370488314 HDFS Write: 11836897 SUCCESS
Job 5: Map: 5  Reduce: 1   Cumulative CPU: 23.36 sec   HDFS Read: 154712749 HDFS Write: 11111526 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 7.44 sec   HDFS Read: 11111893 HDFS Write: 152 SUCCESS
Job 7:  Cumulative CPU: 1.36 sec   HDFS Read: 519 HDFS Write: 49 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 5 minutes 54 seconds 500 msec
OK
Time taken: 1081.331 seconds
Time:1091.91
Running Hive query: tpch/q9_product_type_profit.hive
13/12/01 16:39:27 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 16:39:27 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 16:39:27 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 16:39:27 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 16:39:27 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 16:39:27 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 16:39:27 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.845 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.2 seconds
OK
Time taken: 0.218 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.051 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 16:39:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 16:39:44 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_16-39-36_660_878761296207528102-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 16:39:44 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_16-39-36_660_878761296207528102-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 16:39:44 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 16:39:44 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 16:39:44 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 16:39:44 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 16:39:44 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 16:39:44 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 16:39:44 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 04:39:45	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 04:39:46	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_16-39-36_660_878761296207528102-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-12-01 04:39:46	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_16-39-36_660_878761296207528102-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-12-01 04:39:46	End of local task; Time Taken: 0.758 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0143, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0143/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0143
Hadoop job information for Stage-25: number of mappers: 2; number of reducers: 0
2013-12-01 16:40:04,973 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 7.95 sec
MapReduce Total cumulative CPU time: 7 seconds 950 msec
Ended Job = job_1385815888185_0143
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 78
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0144, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0144/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0144
Hadoop job information for Stage-8: number of mappers: 298; number of reducers: 78
2013-12-01 17:09:49,050 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 9268.73 sec
MapReduce Total cumulative CPU time: 0 days 2 hours 34 minutes 28 seconds 730 msec
Ended Job = job_1385815888185_0144
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 49
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0145, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0145/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0145
Hadoop job information for Stage-1: number of mappers: 172; number of reducers: 49
2013-12-01 17:31:21,913 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8934.31 sec
MapReduce Total cumulative CPU time: 0 days 2 hours 28 minutes 54 seconds 310 msec
Ended Job = job_1385815888185_0145
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 42
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0146, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0146/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0146
Hadoop job information for Stage-2: number of mappers: 158; number of reducers: 42
2013-12-01 17:49:19,928 Stage-2 map = 98%,  reduce = 50%
Ended Job = job_1385815888185_0146 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1385815888185_0146_m_000020 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000017 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000019 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000023 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000033 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000041 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000061 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000091 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000093 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000083 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000110 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000126 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000141 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000137 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_r_000013 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_r_000008 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_r_000010 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_r_000016 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_r_000036 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_r_000024 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_r_000034 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000076 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000101 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000098 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_m_000070 (and more) from job job_1385815888185_0146
Examining task ID: task_1385815888185_0146_r_000037 (and more) from job job_1385815888185_0146

Task with the most failures(5): 
-----
Task ID:
  task_1385815888185_0146_r_000030

URL:
  http://hadoop11:8088/taskdetails.jsp?jobid=job_1385815888185_0146&tipid=task_1385815888185_0146_r_000030
-----
Diagnostic Messages for this Task:
Exception from container-launch: 
org.apache.hadoop.util.Shell$ExitCodeException: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:464)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:195)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:283)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:79)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)




FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Job 0: Map: 2   Cumulative CPU: 7.95 sec   HDFS Read: 142874307 HDFS Write: 29304980 SUCCESS
Job 1: Map: 298  Reduce: 78   Cumulative CPU: 9268.73 sec   HDFS Read: 79611505375 HDFS Write: 37657255397 SUCCESS
Job 2: Map: 172  Reduce: 49   Cumulative CPU: 8934.31 sec   HDFS Read: 49867433061 HDFS Write: 40120742271 SUCCESS
Job 3: Map: 158  Reduce: 42   FAIL
Total MapReduce CPU Time Spent: 0 days 5 hours 3 minutes 30 seconds 989 msec
Command exited with non-zero status 2
Time:4194.00
Running Hive query: tpch/q10_returned_item.hive
13/12/01 17:49:21 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 17:49:21 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 17:49:21 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 17:49:21 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 17:49:21 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 17:49:21 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 17:49:21 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.707 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.221 seconds
OK
Time taken: 0.222 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.059 seconds
Total MapReduce jobs = 7
Stage-1 is selected by condition resolver.
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0147, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0147/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0147
Hadoop job information for Stage-1: number of mappers: 78; number of reducers: 20
2013-12-01 17:55:32,544 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 879.04 sec
MapReduce Total cumulative CPU time: 14 minutes 39 seconds 40 msec
Ended Job = job_1385815888185_0147
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 17:55:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 17:55:35 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_17-49-30_194_6781416153183542127-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 17:55:35 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_17-49-30_194_6781416153183542127-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 17:55:35 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 17:55:35 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 17:55:35 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 17:55:35 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 17:55:35 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 17:55:35 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 17:55:35 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 05:55:35	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 05:55:36	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_17-49-30_194_6781416153183542127-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-12-01 05:55:36	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_17-49-30_194_6781416153183542127-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-12-01 05:55:36	End of local task; Time Taken: 0.652 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 7
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0148, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0148/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0148
Hadoop job information for Stage-13: number of mappers: 6; number of reducers: 0
2013-12-01 17:56:17,771 Stage-13 map = 100%,  reduce = 0%, Cumulative CPU 71.93 sec
MapReduce Total cumulative CPU time: 1 minutes 11 seconds 930 msec
Ended Job = job_1385815888185_0148
Stage-17 is filtered out by condition resolver.
Stage-18 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0149, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0149/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0149
Hadoop job information for Stage-3: number of mappers: 302; number of reducers: 79
2013-12-01 18:10:20,509 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 3571.52 sec
MapReduce Total cumulative CPU time: 59 minutes 31 seconds 520 msec
Ended Job = job_1385815888185_0149
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0150, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0150/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0150
Hadoop job information for Stage-4: number of mappers: 7; number of reducers: 1
2013-12-01 18:11:35,464 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 117.67 sec
MapReduce Total cumulative CPU time: 1 minutes 57 seconds 670 msec
Ended Job = job_1385815888185_0150
Launching Job 5 out of 7
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0151, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0151/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0151
Hadoop job information for Stage-5: number of mappers: 4; number of reducers: 1
2013-12-01 18:12:20,752 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 64.02 sec
MapReduce Total cumulative CPU time: 1 minutes 4 seconds 20 msec
Ended Job = job_1385815888185_0151
Loading data to table default.q10_returned_item
Table default.q10_returned_item stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3579, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 78  Reduce: 20   Cumulative CPU: 879.04 sec   HDFS Read: 20257241310 HDFS Write: 980369913 SUCCESS
Job 1: Map: 6   Cumulative CPU: 71.93 sec   HDFS Read: 980375223 HDFS Write: 1021319236 SUCCESS
Job 2: Map: 302  Reduce: 79   Cumulative CPU: 3571.52 sec   HDFS Read: 80603536145 HDFS Write: 889025471 SUCCESS
Job 3: Map: 7  Reduce: 1   Cumulative CPU: 117.67 sec   HDFS Read: 889044024 HDFS Write: 704132397 SUCCESS
Job 4: Map: 4  Reduce: 1   Cumulative CPU: 64.02 sec   HDFS Read: 704143440 HDFS Write: 3579 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 18 minutes 24 seconds 180 msec
OK
Time taken: 1371.047 seconds
Time:1381.01
Running Hive query: tpch/q11_important_stock.hive
13/12/01 18:12:22 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 18:12:22 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 18:12:22 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 18:12:22 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 18:12:22 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 18:12:22 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 18:12:22 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.758 seconds
OK
Time taken: 0.103 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.228 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.132 seconds
OK
Time taken: 0.199 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.067 seconds
Total MapReduce jobs = 5
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 18:12:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 18:12:35 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_18-12-31_434_7935700719209954480-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 18:12:35 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_18-12-31_434_7935700719209954480-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 18:12:35 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 18:12:35 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 18:12:35 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 18:12:35 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 18:12:35 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 18:12:35 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 18:12:35 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 06:12:36	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 06:12:37	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_18-12-31_434_7935700719209954480-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-12-01 06:12:37	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_18-12-31_434_7935700719209954480-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-12-01 06:12:37	End of local task; Time Taken: 1.058 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0152, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0152/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0152
Hadoop job information for Stage-10: number of mappers: 2; number of reducers: 0
2013-12-01 18:12:56,308 Stage-10 map = 100%,  reduce = 0%, Cumulative CPU 5.67 sec
MapReduce Total cumulative CPU time: 5 seconds 670 msec
Ended Job = job_1385815888185_0152
Stage-11 is filtered out by condition resolver.
Stage-12 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0153, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0153/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0153
Hadoop job information for Stage-2: number of mappers: 48; number of reducers: 13
2013-12-01 18:14:46,192 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 734.44 sec
MapReduce Total cumulative CPU time: 12 minutes 14 seconds 440 msec
Ended Job = job_1385815888185_0153
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0154, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0154/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0154
Hadoop job information for Stage-3: number of mappers: 4; number of reducers: 1
2013-12-01 18:15:22,617 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 38.6 sec
MapReduce Total cumulative CPU time: 38 seconds 600 msec
Ended Job = job_1385815888185_0154
Loading data to table default.q11_part_tmp
Table default.q11_part_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 63173852, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2   Cumulative CPU: 5.67 sec   HDFS Read: 142874307 HDFS Write: 846792 SUCCESS
Job 1: Map: 48  Reduce: 13   Cumulative CPU: 734.44 sec   HDFS Read: 12210439725 HDFS Write: 94289297 SUCCESS
Job 2: Map: 4  Reduce: 1   Cumulative CPU: 38.6 sec   HDFS Read: 94292763 HDFS Write: 63173852 SUCCESS
Total MapReduce CPU Time Spent: 12 minutes 58 seconds 710 msec
OK
Time taken: 171.687 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0155, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0155/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0155
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-12-01 18:15:45,026 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.91 sec
MapReduce Total cumulative CPU time: 5 seconds 910 msec
Ended Job = job_1385815888185_0155
Loading data to table default.q11_sum_tmp
Table default.q11_sum_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 5.91 sec   HDFS Read: 63174071 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 910 msec
OK
Time taken: 22.382 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 18:15:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 18:15:47 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_18-15-45_503_8143819689713356286-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 18:15:47 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_18-15-45_503_8143819689713356286-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 18:15:48 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 18:15:48 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 18:15:48 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 18:15:48 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 18:15:48 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 18:15:48 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 18:15:48 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 06:15:48	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 06:15:49	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_18-15-45_503_8143819689713356286-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-12-01 06:15:49	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_18-15-45_503_8143819689713356286-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-12-01 06:15:49	End of local task; Time Taken: 0.657 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0156, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0156/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0156
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-12-01 18:16:14,460 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.82 sec
MapReduce Total cumulative CPU time: 8 seconds 820 msec
Ended Job = job_1385815888185_0156
Loading data to table default.q11_important_stock
Table default.q11_important_stock stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 0, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 8.82 sec   HDFS Read: 63174071 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 820 msec
OK
Time taken: 29.448 seconds
Time:233.71
Running Hive query: tpch/q12_shipping.hive
13/12/01 18:16:16 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 18:16:16 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 18:16:16 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 18:16:16 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 18:16:16 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 18:16:16 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 18:16:16 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.742 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.237 seconds
OK
Time taken: 0.222 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.051 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0157, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0157/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0157
Hadoop job information for Stage-1: number of mappers: 364; number of reducers: 80
2013-12-01 18:24:57,770 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3602.23 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 0 minutes 2 seconds 230 msec
Ended Job = job_1385815888185_0157
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0158, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0158/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0158
Hadoop job information for Stage-2: number of mappers: 5; number of reducers: 1
2013-12-01 18:25:15,886 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.93 sec
MapReduce Total cumulative CPU time: 4 seconds 930 msec
Ended Job = job_1385815888185_0158
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0159, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0159/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0159
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-01 18:25:34,678 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.86 sec
MapReduce Total cumulative CPU time: 1 seconds 860 msec
Ended Job = job_1385815888185_0159
Loading data to table default.q12_shipping
Table default.q12_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 46, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 364  Reduce: 80   Cumulative CPU: 3602.23 sec   HDFS Read: 97375874339 HDFS Write: 9920 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 4.93 sec   HDFS Read: 28325 HDFS Write: 156 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.86 sec   HDFS Read: 522 HDFS Write: 46 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 0 minutes 9 seconds 20 msec
OK
Time taken: 550.496 seconds
Time:560.21
Running Hive query: tpch/q13_customer_distribution.hive
13/12/01 18:25:36 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 18:25:36 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 18:25:36 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 18:25:36 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 18:25:36 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 18:25:36 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 18:25:36 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.751 seconds
OK
Time taken: 0.141 seconds
OK
Time taken: 0.204 seconds
OK
Time taken: 0.196 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.048 seconds
Total MapReduce jobs = 4
Stage-1 is selected by condition resolver.
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 21
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0160, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0160/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0160
Hadoop job information for Stage-1: number of mappers: 77; number of reducers: 21
2013-12-01 18:29:07,270 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1703.51 sec
MapReduce Total cumulative CPU time: 28 minutes 23 seconds 510 msec
Ended Job = job_1385815888185_0160
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0161, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0161/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0161
Hadoop job information for Stage-2: number of mappers: 5; number of reducers: 1
2013-12-01 18:30:23,854 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 114.18 sec
MapReduce Total cumulative CPU time: 1 minutes 54 seconds 180 msec
Ended Job = job_1385815888185_0161
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0162, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0162/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0162
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-01 18:30:42,475 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.56 sec
MapReduce Total cumulative CPU time: 1 seconds 560 msec
Ended Job = job_1385815888185_0162
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0163, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0163/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0163
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-01 18:31:00,407 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.9 sec
MapReduce Total cumulative CPU time: 1 seconds 900 msec
Ended Job = job_1385815888185_0163
Loading data to table default.q13_customer_distribution
Table default.q13_customer_distribution stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 392, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 21   Cumulative CPU: 1703.51 sec   HDFS Read: 20257241173 HDFS Write: 333230859 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 114.18 sec   HDFS Read: 333236225 HDFS Write: 1054 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.56 sec   HDFS Read: 1420 HDFS Write: 1054 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 1.9 sec   HDFS Read: 1420 HDFS Write: 392 SUCCESS
Total MapReduce CPU Time Spent: 30 minutes 21 seconds 150 msec
OK
Time taken: 316.027 seconds
Time:325.74
Running Hive query: tpch/q14_promotion_effect.hive
13/12/01 18:31:02 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 18:31:02 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 18:31:02 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 18:31:02 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 18:31:02 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 18:31:02 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 18:31:02 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.947 seconds
OK
Time taken: 0.103 seconds
OK
Time taken: 0.227 seconds
OK
Time taken: 0.207 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.052 seconds
Total MapReduce jobs = 2
Stage-1 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0164, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0164/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0164
Hadoop job information for Stage-1: number of mappers: 307; number of reducers: 79
2013-12-01 18:38:18,734 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2565.12 sec
MapReduce Total cumulative CPU time: 42 minutes 45 seconds 120 msec
Ended Job = job_1385815888185_0164
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0165, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0165/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0165
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1
2013-12-01 18:38:40,538 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.13 sec
MapReduce Total cumulative CPU time: 6 seconds 130 msec
Ended Job = job_1385815888185_0165
Loading data to table default.q14_promotion_effect
Table default.q14_promotion_effect stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 18, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 307  Reduce: 79   Cumulative CPU: 2565.12 sec   HDFS Read: 82035510185 HDFS Write: 10191 SUCCESS
Job 1: Map: 6  Reduce: 1   Cumulative CPU: 6.13 sec   HDFS Read: 28599 HDFS Write: 18 SUCCESS
Total MapReduce CPU Time Spent: 42 minutes 51 seconds 250 msec
OK
Time taken: 450.188 seconds
Time:460.12
Running Hive query: tpch/q15_top_supplier.hive
13/12/01 18:38:42 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 18:38:42 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 18:38:42 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 18:38:42 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 18:38:42 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 18:38:42 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 18:38:42 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.904 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.221 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.141 seconds
OK
Time taken: 0.218 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.045 seconds
OK
Time taken: 0.046 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0166, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0166/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0166
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-01 18:45:37,972 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2600.96 sec
MapReduce Total cumulative CPU time: 43 minutes 20 seconds 960 msec
Ended Job = job_1385815888185_0166
Loading data to table default.revenue
Table default.revenue stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 22445546, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 2600.96 sec   HDFS Read: 79582199808 HDFS Write: 22445546 SUCCESS
Total MapReduce CPU Time Spent: 43 minutes 20 seconds 960 msec
OK
Time taken: 407.181 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0167, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0167/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0167
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1
2013-12-01 18:45:58,743 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 13.6 sec
MapReduce Total cumulative CPU time: 13 seconds 600 msec
Ended Job = job_1385815888185_0167
Loading data to table default.max_revenue
Table default.max_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 13, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 6  Reduce: 1   Cumulative CPU: 13.6 sec   HDFS Read: 22452528 HDFS Write: 13 SUCCESS
Total MapReduce CPU Time Spent: 13 seconds 600 msec
OK
Time taken: 20.69 seconds
Total MapReduce jobs = 3
Stage-12 is selected by condition resolver.
Stage-1 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 18:46:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 18:46:02 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_18-45-59_223_6241291122686087817-1/-local-10009/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 18:46:02 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_18-45-59_223_6241291122686087817-1/-local-10009/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 18:46:02 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 18:46:02 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 18:46:02 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 18:46:02 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 18:46:02 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 18:46:02 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 18:46:02 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 06:46:03	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 06:46:05	Processing rows:	200000	Hashtable size:	199999	Memory usage:	95827320	percentage:	0.201
2013-12-01 06:46:06	Processing rows:	300000	Hashtable size:	299999	Memory usage:	84552496	percentage:	0.177
2013-12-01 06:46:06	Processing rows:	400000	Hashtable size:	399999	Memory usage:	181845512	percentage:	0.381
2013-12-01 06:46:08	Processing rows:	500000	Hashtable size:	499999	Memory usage:	174617120	percentage:	0.366
2013-12-01 06:46:08	Processing rows:	600000	Hashtable size:	599999	Memory usage:	204547584	percentage:	0.429
2013-12-01 06:46:08	Processing rows:	700000	Hashtable size:	699999	Memory usage:	175483632	percentage:	0.368
2013-12-01 06:46:10	Processing rows:	800000	Hashtable size:	799999	Memory usage:	213153824	percentage:	0.447
2013-12-01 06:46:11	Processing rows:	900000	Hashtable size:	899999	Memory usage:	262416472	percentage:	0.55
2013-12-01 06:46:11	Processing rows:	1000000	Hashtable size:	999999	Memory usage:	266818168	percentage:	0.559
2013-12-01 06:46:11	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_18-45-59_223_6241291122686087817-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile11--.hashtable
2013-12-01 06:46:12	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_18-45-59_223_6241291122686087817-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile11--.hashtable
2013-12-01 06:46:12	End of local task; Time Taken: 8.827 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0168, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0168/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0168
Hadoop job information for Stage-8: number of mappers: 2; number of reducers: 0
2013-12-01 18:46:42,312 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 25.33 sec
MapReduce Total cumulative CPU time: 25 seconds 330 msec
Ended Job = job_1385815888185_0168
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 18:46:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 18:46:44 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_18-45-59_223_6241291122686087817-1/-local-10013/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 18:46:44 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_18-45-59_223_6241291122686087817-1/-local-10013/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 18:46:44 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 18:46:44 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 18:46:44 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 18:46:44 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 18:46:44 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 18:46:44 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 18:46:44 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 06:46:45	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 06:46:46	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_18-45-59_223_6241291122686087817-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-12-01 06:46:46	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_18-45-59_223_6241291122686087817-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-12-01 06:46:46	End of local task; Time Taken: 0.619 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0169, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0169/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0169
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
2013-12-01 18:47:09,029 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 7.25 sec
MapReduce Total cumulative CPU time: 7 seconds 250 msec
Ended Job = job_1385815888185_0169
Loading data to table default.q15_top_supplier
Table default.q15_top_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 77, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2   Cumulative CPU: 25.33 sec   HDFS Read: 142874307 HDFS Write: 90810975 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 7.25 sec   HDFS Read: 90811709 HDFS Write: 77 SUCCESS
Total MapReduce CPU Time Spent: 32 seconds 580 msec
OK
Time taken: 70.289 seconds
Time:508.47
Running Hive query: tpch/q16_parts_supplier_relationship.hive
13/12/01 18:47:10 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 18:47:10 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 18:47:10 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 18:47:10 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 18:47:10 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 18:47:10 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 18:47:10 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.734 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.203 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.225 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.052 seconds
OK
Time taken: 0.078 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0170, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0170/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0170
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 0
2013-12-01 18:47:41,068 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 10.42 sec
MapReduce Total cumulative CPU time: 10 seconds 420 msec
Ended Job = job_1385815888185_0170
Stage-4 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Stage-5 is filtered out by condition resolver.
Launching Job 3 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0171, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0171/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0171
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
2013-12-01 18:47:54,360 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 2.15 sec
MapReduce Total cumulative CPU time: 2 seconds 150 msec
Ended Job = job_1385815888185_0171
Loading data to table default.supplier_tmp
Table default.supplier_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6885604, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2   Cumulative CPU: 10.42 sec   HDFS Read: 142874307 HDFS Write: 6885604 SUCCESS
Job 1: Map: 1   Cumulative CPU: 2.15 sec   HDFS Read: 6885995 HDFS Write: 6885604 SUCCESS
Total MapReduce CPU Time Spent: 12 seconds 570 msec
OK
Time taken: 35.158 seconds
Total MapReduce jobs = 2
Stage-3 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 15
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0172, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0172/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0172
Hadoop job information for Stage-3: number of mappers: 56; number of reducers: 15
2013-12-01 18:50:13,724 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 980.08 sec
MapReduce Total cumulative CPU time: 16 minutes 20 seconds 80 msec
Ended Job = job_1385815888185_0172
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 18:50:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 18:50:16 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_18-47-54_980_5893717427408151632-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 18:50:16 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_18-47-54_980_5893717427408151632-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 18:50:16 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 18:50:16 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 18:50:16 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 18:50:16 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 18:50:16 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 18:50:16 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 18:50:16 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 06:50:17	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 06:50:18	Processing rows:	200000	Hashtable size:	199999	Memory usage:	37257408	percentage:	0.078
2013-12-01 06:50:18	Processing rows:	300000	Hashtable size:	299999	Memory usage:	48257240	percentage:	0.101
2013-12-01 06:50:18	Processing rows:	400000	Hashtable size:	399999	Memory usage:	57159928	percentage:	0.12
2013-12-01 06:50:18	Processing rows:	500000	Hashtable size:	499999	Memory usage:	66062600	percentage:	0.138
2013-12-01 06:50:18	Processing rows:	600000	Hashtable size:	599999	Memory usage:	79159592	percentage:	0.166
2013-12-01 06:50:18	Processing rows:	700000	Hashtable size:	699999	Memory usage:	88062296	percentage:	0.185
2013-12-01 06:50:18	Processing rows:	800000	Hashtable size:	799999	Memory usage:	96964984	percentage:	0.203
2013-12-01 06:50:18	Processing rows:	900000	Hashtable size:	899999	Memory usage:	105867656	percentage:	0.222
2013-12-01 06:50:18	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_18-47-54_980_5893717427408151632-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-12-01 06:50:20	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_18-47-54_980_5893717427408151632-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-12-01 06:50:20	End of local task; Time Taken: 2.818 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0173, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0173/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0173
Hadoop job information for Stage-5: number of mappers: 15; number of reducers: 0
2013-12-01 18:51:44,734 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 495.31 sec
MapReduce Total cumulative CPU time: 8 minutes 15 seconds 310 msec
Ended Job = job_1385815888185_0173
Loading data to table default.q16_tmp
Table default.q16_tmp stats: [num_partitions: 0, num_files: 15, num_rows: 0, total_size: 2989814109, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 56  Reduce: 15   Cumulative CPU: 980.08 sec   HDFS Read: 14662902576 HDFS Write: 3937268409 SUCCESS
Job 1: Map: 15   Cumulative CPU: 495.31 sec   HDFS Read: 3937302871 HDFS Write: 2989814109 SUCCESS
Total MapReduce CPU Time Spent: 24 minutes 35 seconds 390 msec
OK
Time taken: 230.262 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0174, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0174/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0174
Hadoop job information for Stage-1: number of mappers: 12; number of reducers: 3
2013-12-01 18:52:44,995 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 294.96 sec
MapReduce Total cumulative CPU time: 4 minutes 54 seconds 960 msec
Ended Job = job_1385815888185_0174
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0175, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0175/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0175
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-12-01 18:53:06,457 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.82 sec
MapReduce Total cumulative CPU time: 5 seconds 820 msec
Ended Job = job_1385815888185_0175
Loading data to table default.q16_parts_supplier_relationship
Table default.q16_parts_supplier_relationship stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 1039440, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 12  Reduce: 3   Cumulative CPU: 294.96 sec   HDFS Read: 2989879503 HDFS Write: 1450608 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 5.82 sec   HDFS Read: 1451419 HDFS Write: 1039440 SUCCESS
Total MapReduce CPU Time Spent: 5 minutes 0 seconds 780 msec
OK
Time taken: 81.683 seconds
Time:357.40
Running Hive query: tpch/q17_small_quantity_order_revenue.hive
13/12/01 18:53:08 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 18:53:08 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 18:53:08 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 18:53:08 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 18:53:08 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 18:53:08 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 18:53:08 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.779 seconds
OK
Time taken: 0.11 seconds
OK
Time taken: 0.238 seconds
OK
Time taken: 0.15 seconds
OK
Time taken: 0.212 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.051 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0176, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0176/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0176
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-01 19:06:23,845 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6582.52 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 49 minutes 42 seconds 520 msec
Ended Job = job_1385815888185_0176
Loading data to table default.lineitem_tmp
Table default.lineitem_tmp stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 494293140, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 6582.52 sec   HDFS Read: 79582199808 HDFS Write: 494293140 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 49 minutes 42 seconds 520 msec
OK
Time taken: 787.54 seconds
Total MapReduce jobs = 5
Stage-1 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 83
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0177, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0177/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0177
Hadoop job information for Stage-1: number of mappers: 307; number of reducers: 83
2013-12-01 19:19:12,972 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6527.5 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 48 minutes 47 seconds 500 msec
Ended Job = job_1385815888185_0177
Stage-13 is filtered out by condition resolver.
Stage-14 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0178, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0178/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0178
Hadoop job information for Stage-2: number of mappers: 10; number of reducers: 1
2013-12-01 19:20:43,043 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 159.73 sec
MapReduce Total cumulative CPU time: 2 minutes 39 seconds 730 msec
Ended Job = job_1385815888185_0178
Launching Job 3 out of 5
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0179, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0179/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0179
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-01 19:21:02,159 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.93 sec
MapReduce Total cumulative CPU time: 1 seconds 930 msec
Ended Job = job_1385815888185_0179
Loading data to table default.q17_small_quantity_order_revenue
Table default.q17_small_quantity_order_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 20, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 307  Reduce: 83   Cumulative CPU: 6527.5 sec   HDFS Read: 82035510185 HDFS Write: 22558316 SUCCESS
Job 1: Map: 10  Reduce: 1   Cumulative CPU: 159.73 sec   HDFS Read: 516877852 HDFS Write: 121 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.93 sec   HDFS Read: 488 HDFS Write: 20 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 51 minutes 29 seconds 160 msec
OK
Time taken: 878.164 seconds
Time:1675.68
Running Hive query: tpch/q18_large_volume_customer.hive
13/12/01 19:21:03 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 19:21:03 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 19:21:03 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 19:21:03 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 19:21:03 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 19:21:03 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 19:21:03 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.576 seconds
OK
Time taken: 0.181 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.206 seconds
OK
Time taken: 0.129 seconds
OK
Time taken: 0.19 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.047 seconds
OK
Time taken: 0.052 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 69
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0180, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0180/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0180
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 69
2013-12-01 19:29:26,315 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3716.21 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 1 minutes 56 seconds 210 msec
Ended Job = job_1385815888185_0180
Loading data to table default.q18_tmp
Table default.q18_tmp stats: [num_partitions: 0, num_files: 69, num_rows: 0, total_size: 2291717657, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 69   Cumulative CPU: 3716.21 sec   HDFS Read: 79582199808 HDFS Write: 2291717657 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 1 minutes 56 seconds 210 msec
OK
Time taken: 494.438 seconds
Total MapReduce jobs = 5
Stage-5 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0181, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0181/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0181
Hadoop job information for Stage-5: number of mappers: 77; number of reducers: 18
2013-12-01 19:35:04,482 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 1953.82 sec
MapReduce Total cumulative CPU time: 32 minutes 33 seconds 820 msec
Ended Job = job_1385815888185_0181
Stage-15 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0182, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0182/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0182
Hadoop job information for Stage-1: number of mappers: 344; number of reducers: 79
2013-12-01 19:49:27,158 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6839.22 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 53 minutes 59 seconds 220 msec
Ended Job = job_1385815888185_0182
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0183, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0183/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0183
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1
2013-12-01 19:49:59,990 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 9.24 sec
MapReduce Total cumulative CPU time: 9 seconds 240 msec
Ended Job = job_1385815888185_0183
Launching Job 4 out of 5
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0184, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0184/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0184
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-01 19:50:18,911 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.93 sec
MapReduce Total cumulative CPU time: 2 seconds 930 msec
Ended Job = job_1385815888185_0184
Loading data to table default.q18_large_volume_customer
Table default.q18_large_volume_customer stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6391, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 18   Cumulative CPU: 1953.82 sec   HDFS Read: 20257241173 HDFS Write: 9688881598 SUCCESS
Job 1: Map: 344  Reduce: 79   Cumulative CPU: 6839.22 sec   HDFS Read: 91562986099 HDFS Write: 471842 SUCCESS
Job 2: Map: 6  Reduce: 1   Cumulative CPU: 9.24 sec   HDFS Read: 490250 HDFS Write: 465114 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 2.93 sec   HDFS Read: 465481 HDFS Write: 6391 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 26 minutes 45 seconds 210 msec
OK
Time taken: 1252.426 seconds
Time:1756.76
Running Hive query: tpch/q19_discounted_revenue.hive
13/12/01 19:50:20 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 19:50:20 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 19:50:20 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 19:50:20 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 19:50:20 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 19:50:20 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 19:50:20 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.748 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.213 seconds
OK
Time taken: 0.229 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.058 seconds
Total MapReduce jobs = 2
Stage-1 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0185, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0185/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0185
Hadoop job information for Stage-1: number of mappers: 307; number of reducers: 79
2013-12-01 20:09:05,643 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9264.21 sec
MapReduce Total cumulative CPU time: 0 days 2 hours 34 minutes 24 seconds 210 msec
Ended Job = job_1385815888185_0185
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0186, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0186/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0186
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1
2013-12-01 20:09:30,557 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.72 sec
MapReduce Total cumulative CPU time: 5 seconds 720 msec
Ended Job = job_1385815888185_0186
Loading data to table default.q19_discounted_revenue
Table default.q19_discounted_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 22, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 307  Reduce: 79   Cumulative CPU: 9264.21 sec   HDFS Read: 82035510185 HDFS Write: 9559 SUCCESS
Job 1: Map: 6  Reduce: 1   Cumulative CPU: 5.72 sec   HDFS Read: 27967 HDFS Write: 22 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 34 minutes 29 seconds 930 msec
OK
Time taken: 1141.912 seconds
Time:1151.65
Running Hive query: tpch/q20_potential_part_promotion.hive
13/12/01 20:09:32 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 20:09:32 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 20:09:32 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 20:09:32 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 20:09:32 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 20:09:32 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 20:09:32 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.615 seconds
OK
Time taken: 0.151 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.2 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.223 seconds
OK
Time taken: 0.038 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.083 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0187, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0187/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0187
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 3
2013-12-01 20:10:19,850 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 74.99 sec
MapReduce Total cumulative CPU time: 1 minutes 14 seconds 990 msec
Ended Job = job_1385815888185_0187
Loading data to table default.q20_tmp1
Table default.q20_tmp1 stats: [num_partitions: 0, num_files: 3, num_rows: 0, total_size: 1834396, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10  Reduce: 3   Cumulative CPU: 74.99 sec   HDFS Read: 2453310377 HDFS Write: 1834396 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 14 seconds 990 msec
OK
Time taken: 38.831 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0188, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0188/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0188
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-01 20:18:04,505 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3135.81 sec
MapReduce Total cumulative CPU time: 52 minutes 15 seconds 810 msec
Ended Job = job_1385815888185_0188
Loading data to table default.q20_tmp2
Table default.q20_tmp2 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 1096808654, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 3135.81 sec   HDFS Read: 79582199808 HDFS Write: 1096808654 SUCCESS
Total MapReduce CPU Time Spent: 52 minutes 15 seconds 810 msec
OK
Time taken: 464.596 seconds
Total MapReduce jobs = 4
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 20:18:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 20:18:08 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_20-18-05_021_596124152762645459-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 20:18:08 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_20-18-05_021_596124152762645459-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 20:18:08 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 20:18:08 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 20:18:08 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 20:18:08 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 20:18:08 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 20:18:08 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 20:18:08 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 08:18:09	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 08:18:10	Processing rows:	200000	Hashtable size:	199999	Memory usage:	41666816	percentage:	0.087
2013-12-01 08:18:10	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_20-18-05_021_596124152762645459-1/-local-10007/HashTable-Stage-8/MapJoin-mapfile21--.hashtable
2013-12-01 08:18:10	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_20-18-05_021_596124152762645459-1/-local-10007/HashTable-Stage-8/MapJoin-mapfile21--.hashtable
2013-12-01 08:18:10	End of local task; Time Taken: 1.355 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 4
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0189, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0189/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0189
Hadoop job information for Stage-8: number of mappers: 46; number of reducers: 0
2013-12-01 20:19:05,065 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 323.14 sec
MapReduce Total cumulative CPU time: 5 minutes 23 seconds 140 msec
Ended Job = job_1385815888185_0189
Stage-9 is filtered out by condition resolver.
Stage-10 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0190, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0190/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0190
Hadoop job information for Stage-1: number of mappers: 12; number of reducers: 2
2013-12-01 20:21:47,738 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 426.86 sec
MapReduce Total cumulative CPU time: 7 minutes 6 seconds 860 msec
Ended Job = job_1385815888185_0190
Loading data to table default.q20_tmp3
Table default.q20_tmp3 stats: [num_partitions: 0, num_files: 2, num_rows: 0, total_size: 9821178, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 46   Cumulative CPU: 323.14 sec   HDFS Read: 12209592199 HDFS Write: 24620633 SUCCESS
Job 1: Map: 12  Reduce: 2   Cumulative CPU: 426.86 sec   HDFS Read: 1121447377 HDFS Write: 9821178 SUCCESS
Total MapReduce CPU Time Spent: 12 minutes 30 seconds 0 msec
OK
Time taken: 223.18 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0191, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0191/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0191
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-12-01 20:22:11,939 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9.24 sec
MapReduce Total cumulative CPU time: 9 seconds 240 msec
Ended Job = job_1385815888185_0191
Loading data to table default.q20_tmp4
Table default.q20_tmp4 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3082950, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 9.24 sec   HDFS Read: 9821471 HDFS Write: 3082950 SUCCESS
Total MapReduce CPU Time Spent: 9 seconds 240 msec
OK
Time taken: 24.182 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 20:22:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 20:22:14 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_20-22-12_383_4816824503329568688-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 20:22:14 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_20-22-12_383_4816824503329568688-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 20:22:14 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 20:22:14 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 20:22:14 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 20:22:14 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 20:22:14 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 20:22:14 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 20:22:14 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 08:22:15	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 08:22:16	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_20-22-12_383_4816824503329568688-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile41--.hashtable
2013-12-01 08:22:16	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_20-22-12_383_4816824503329568688-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile41--.hashtable
2013-12-01 08:22:16	Processing rows:	200000	Hashtable size:	199999	Memory usage:	81705816	percentage:	0.171
2013-12-01 08:22:17	Processing rows:	300000	Hashtable size:	299999	Memory usage:	90670032	percentage:	0.19
2013-12-01 08:22:17	Processing rows:	400000	Hashtable size:	399999	Memory usage:	103828592	percentage:	0.218
2013-12-01 08:22:17	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_20-22-12_383_4816824503329568688-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-12-01 08:22:17	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_20-22-12_383_4816824503329568688-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-12-01 08:22:17	End of local task; Time Taken: 1.879 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0192, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0192/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0192
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
2013-12-01 20:22:43,727 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 13.59 sec
MapReduce Total cumulative CPU time: 13 seconds 590 msec
Ended Job = job_1385815888185_0192
Loading data to table default.q20_potential_part_promotion
Table default.q20_potential_part_promotion stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 810032, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 13.59 sec   HDFS Read: 142874307 HDFS Write: 810032 SUCCESS
Total MapReduce CPU Time Spent: 13 seconds 590 msec
OK
Time taken: 31.791 seconds
Time:793.13
Running Hive query: tpch/q21_suppliers_who_kept_orders_waiting.hive
13/12/01 20:22:45 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 20:22:45 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 20:22:45 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 20:22:45 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 20:22:45 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 20:22:45 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 20:22:45 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.838 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.208 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.132 seconds
OK
Time taken: 0.387 seconds
OK
Time taken: 0.055 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.084 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0193, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0193/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0193
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-01 20:36:36,010 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6452.62 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 47 minutes 32 seconds 620 msec
Ended Job = job_1385815888185_0193
Loading data to table default.q21_tmp1
Table default.q21_tmp1 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 2819600047, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 6452.62 sec   HDFS Read: 79582199808 HDFS Write: 2819600047 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 47 minutes 32 seconds 620 msec
OK
Time taken: 821.865 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0194, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0194/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0194
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-12-01 20:45:58,652 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4707.69 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 18 minutes 27 seconds 690 msec
Ended Job = job_1385815888185_0194
Loading data to table default.q21_tmp2
Table default.q21_tmp2 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 2583844107, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 4707.69 sec   HDFS Read: 79582199808 HDFS Write: 2583844107 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 18 minutes 27 seconds 690 msec
OK
Time taken: 562.462 seconds
Total MapReduce jobs = 14
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 20:46:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 20:46:05 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_20-45-59_162_106011097629871616-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 20:46:05 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_20-45-59_162_106011097629871616-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 20:46:05 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 20:46:05 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 20:46:05 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 20:46:05 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 20:46:05 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 20:46:05 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 20:46:05 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 08:46:06	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 08:46:07	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_20-45-59_162_106011097629871616-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-12-01 08:46:07	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_20-45-59_162_106011097629871616-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-12-01 08:46:07	End of local task; Time Taken: 1.033 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 14
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0195, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0195/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0195
Hadoop job information for Stage-24: number of mappers: 2; number of reducers: 0
2013-12-01 20:46:28,993 Stage-24 map = 100%,  reduce = 0%, Cumulative CPU 5.5 sec
MapReduce Total cumulative CPU time: 5 seconds 500 msec
Ended Job = job_1385815888185_0195
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 14
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0196, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0196/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0196
Hadoop job information for Stage-8: number of mappers: 299; number of reducers: 80
2013-12-01 20:56:04,781 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 4062.5 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 7 minutes 42 seconds 500 msec
Ended Job = job_1385815888185_0196
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 14
Number of reduce tasks not specified. Estimated from input data size: 19
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0197, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0197/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0197
Hadoop job information for Stage-1: number of mappers: 72; number of reducers: 19
2013-12-01 20:58:22,808 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 892.11 sec
MapReduce Total cumulative CPU time: 14 minutes 52 seconds 110 msec
Ended Job = job_1385815888185_0197
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 14
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0198, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0198/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0198
Hadoop job information for Stage-2: number of mappers: 16; number of reducers: 4
2013-12-01 21:01:20,630 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 998.74 sec
MapReduce Total cumulative CPU time: 16 minutes 38 seconds 740 msec
Ended Job = job_1385815888185_0198
Stage-25 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 5 out of 14
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0199, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0199/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0199
Hadoop job information for Stage-3: number of mappers: 14; number of reducers: 3
2013-12-01 21:04:40,868 Stage-3 map = 100%,  reduce = 0%
Ended Job = job_1385815888185_0199 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1385815888185_0199_m_000013 (and more) from job job_1385815888185_0199
Examining task ID: task_1385815888185_0199_m_000007 (and more) from job job_1385815888185_0199
Examining task ID: task_1385815888185_0199_r_000000 (and more) from job job_1385815888185_0199

Task with the most failures(4): 
-----
Task ID:
  task_1385815888185_0199_r_000000

URL:
  http://hadoop11:8088/taskdetails.jsp?jobid=job_1385815888185_0199&tipid=task_1385815888185_0199_r_000000
-----
Diagnostic Messages for this Task:
Exception from container-launch: 
org.apache.hadoop.util.Shell$ExitCodeException: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:464)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:195)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:283)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:79)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)




FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Job 0: Map: 2   Cumulative CPU: 5.5 sec   HDFS Read: 142874307 HDFS Write: 1611586 SUCCESS
Job 1: Map: 299  Reduce: 80   Cumulative CPU: 4062.5 sec   HDFS Read: 79583812126 HDFS Write: 686901797 SUCCESS
Job 2: Map: 72  Reduce: 19   Cumulative CPU: 892.11 sec   HDFS Read: 18480594733 HDFS Write: 331675044 SUCCESS
Job 3: Map: 16  Reduce: 4   Cumulative CPU: 998.74 sec   HDFS Read: 3151287762 HDFS Write: 319626840 SUCCESS
Job 4: Map: 14  Reduce: 3   FAIL
Total MapReduce CPU Time Spent: 0 days 1 hours 39 minutes 18 seconds 849 msec
Command exited with non-zero status 2
Time:2516.82
Running Hive query: tpch/q22_global_sales_opportunity.hive
13/12/01 21:04:42 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 21:04:42 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 21:04:42 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 21:04:42 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 21:04:42 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 21:04:42 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 21:04:42 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.673 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.231 seconds
OK
Time taken: 0.129 seconds
OK
Time taken: 0.125 seconds
OK
Time taken: 0.132 seconds
OK
Time taken: 0.208 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.07 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0200, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0200/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0200
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 0
2013-12-01 21:05:28,161 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 90.15 sec
MapReduce Total cumulative CPU time: 1 minutes 30 seconds 150 msec
Ended Job = job_1385815888185_0200
Stage-4 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Stage-5 is filtered out by condition resolver.
Launching Job 3 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0201, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0201/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0201
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
2013-12-01 21:05:54,381 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 9.53 sec
MapReduce Total cumulative CPU time: 9 seconds 530 msec
Ended Job = job_1385815888185_0201
Loading data to table default.q22_customer_tmp
Table default.q22_customer_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 80028920, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10   Cumulative CPU: 90.15 sec   HDFS Read: 2463566642 HDFS Write: 80028920 SUCCESS
Job 1: Map: 1   Cumulative CPU: 9.53 sec   HDFS Read: 80030327 HDFS Write: 80028920 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 39 seconds 680 msec
OK
Time taken: 63.774 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0202, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0202/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0202
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-12-01 21:06:19,322 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.46 sec
MapReduce Total cumulative CPU time: 7 seconds 460 msec
Ended Job = job_1385815888185_0202
Loading data to table default.q22_customer_tmp1
Table default.q22_customer_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 18, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 7.46 sec   HDFS Read: 80029143 HDFS Write: 18 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 460 msec
OK
Time taken: 24.855 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0203, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0203/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0203
Hadoop job information for Stage-1: number of mappers: 67; number of reducers: 18
2013-12-01 21:08:40,698 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1024.75 sec
MapReduce Total cumulative CPU time: 17 minutes 4 seconds 750 msec
Ended Job = job_1385815888185_0203
Loading data to table default.q22_orders_tmp
Table default.q22_orders_tmp stats: [num_partitions: 0, num_files: 18, num_rows: 0, total_size: 82591212, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 67  Reduce: 18   Cumulative CPU: 1024.75 sec   HDFS Read: 17793674531 HDFS Write: 82591212 SUCCESS
Total MapReduce CPU Time Spent: 17 minutes 4 seconds 750 msec
OK
Time taken: 141.43 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0204, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0204/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0204
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 1
2013-12-01 21:09:54,946 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 113.5 sec
MapReduce Total cumulative CPU time: 1 minutes 53 seconds 500 msec
Ended Job = job_1385815888185_0204
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/01 21:09:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/12/01 21:09:57 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_21-08-41_233_8431672344439492356-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/12/01 21:09:57 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-12-01_21-08-41_233_8431672344439492356-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/12/01 21:09:57 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/12/01 21:09:57 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/12/01 21:09:57 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/12/01 21:09:57 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/12/01 21:09:57 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/12/01 21:09:57 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/12/01 21:09:57 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-12-01 09:09:58	Starting to launch local task to process map join;	maximum memory = 477102080
2013-12-01 09:09:58	Dump the side-table into file: file:/tmp/hadoop/hive_2013-12-01_21-08-41_233_8431672344439492356-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-12-01 09:09:58	Upload 1 File to: file:/tmp/hadoop/hive_2013-12-01_21-08-41_233_8431672344439492356-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-12-01 09:09:58	End of local task; Time Taken: 0.646 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0205, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0205/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0205
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-12-01 21:10:22,353 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 8.16 sec
MapReduce Total cumulative CPU time: 8 seconds 160 msec
Ended Job = job_1385815888185_0205
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0206, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0206/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0206
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-12-01 21:10:40,976 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.86 sec
MapReduce Total cumulative CPU time: 1 seconds 860 msec
Ended Job = job_1385815888185_0206
Loading data to table default.q22_global_sales_opportunity
Table default.q22_global_sales_opportunity stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 202, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 7  Reduce: 1   Cumulative CPU: 113.5 sec   HDFS Read: 162622689 HDFS Write: 39621076 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 8.16 sec   HDFS Read: 39621443 HDFS Write: 320 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.86 sec   HDFS Read: 687 HDFS Write: 202 SUCCESS
Total MapReduce CPU Time Spent: 2 minutes 3 seconds 520 msec
OK
Time taken: 120.214 seconds
Time:360.43
