Running Hive from /opt/hive-0.12.0
Running Hadoop from 
Running Hive query: tpch/q1_pricing_summary_report.hive
13/11/27 01:50:20 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 01:50:20 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 01:50:20 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 01:50:20 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 01:50:20 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 01:50:20 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 01:50:20 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 8.102 seconds
OK
Time taken: 0.155 seconds
OK
Time taken: 0.249 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0001, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0001/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0001
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-27 01:51:06,622 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 44.54 sec
MapReduce Total cumulative CPU time: 44 seconds 540 msec
Ended Job = job_1385527530902_0001
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0002, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0002/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0002
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-27 01:51:30,259 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.06 sec
MapReduce Total cumulative CPU time: 2 seconds 60 msec
Ended Job = job_1385527530902_0002
Loading data to table default.q1_pricing_summary_report
Table default.q1_pricing_summary_report stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 574, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 44.54 sec   HDFS Read: 759884717 HDFS Write: 423 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 2.06 sec   HDFS Read: 790 HDFS Write: 574 SUCCESS
Total MapReduce CPU Time Spent: 46 seconds 600 msec
OK
Time taken: 60.925 seconds
Time:72.51
Running Hive query: tpch/q2_minimum_cost_supplier.hive
13/11/27 01:51:32 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 01:51:32 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 01:51:32 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 01:51:32 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 01:51:32 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 01:51:32 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 01:51:32 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.959 seconds
OK
Time taken: 0.087 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.207 seconds
OK
Time taken: 0.102 seconds
OK
Time taken: 0.095 seconds
OK
Time taken: 0.198 seconds
OK
Time taken: 0.047 seconds
OK
Time taken: 0.025 seconds
OK
Time taken: 0.035 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.092 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.042 seconds
Total MapReduce jobs = 7
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 01:51:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 01:51:45 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_01-51-40_341_6156060775570542879-1/-local-10016/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 01:51:45 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_01-51-40_341_6156060775570542879-1/-local-10016/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 01:51:45 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 01:51:45 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 01:51:45 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 01:51:45 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 01:51:45 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 01:51:45 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 01:51:45 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 01:51:46	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 01:51:47	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_01-51-40_341_6156060775570542879-1/-local-10013/HashTable-Stage-15/MapJoin-mapfile41--.hashtable
2013-11-27 01:51:47	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_01-51-40_341_6156060775570542879-1/-local-10013/HashTable-Stage-15/MapJoin-mapfile41--.hashtable
2013-11-27 01:51:47	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_01-51-40_341_6156060775570542879-1/-local-10013/HashTable-Stage-15/MapJoin-mapfile51--.hashtable
2013-11-27 01:51:47	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_01-51-40_341_6156060775570542879-1/-local-10013/HashTable-Stage-15/MapJoin-mapfile51--.hashtable
2013-11-27 01:51:47	End of local task; Time Taken: 1.511 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 7
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0003, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0003/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0003
Hadoop job information for Stage-15: number of mappers: 1; number of reducers: 0
2013-11-27 01:52:00,646 Stage-15 map = 100%,  reduce = 0%, Cumulative CPU 1.44 sec
MapReduce Total cumulative CPU time: 1 seconds 440 msec
Ended Job = job_1385527530902_0003
Stage-19 is filtered out by condition resolver.
Stage-20 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0004, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0004/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0004
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-27 01:52:26,175 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 12.53 sec
MapReduce Total cumulative CPU time: 12 seconds 530 msec
Ended Job = job_1385527530902_0004
Stage-17 is filtered out by condition resolver.
Stage-18 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0005, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0005/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0005
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
2013-11-27 01:52:51,001 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 10.56 sec
MapReduce Total cumulative CPU time: 10 seconds 560 msec
Ended Job = job_1385527530902_0005
Loading data to table default.q2_minimum_cost_supplier_tmp1
Table default.q2_minimum_cost_supplier_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 109507, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.44 sec   HDFS Read: 2424 HDFS Write: 324185 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 12.53 sec   HDFS Read: 119309372 HDFS Write: 27325124 SUCCESS
Job 2: Map: 2  Reduce: 1   Cumulative CPU: 10.56 sec   HDFS Read: 51460812 HDFS Write: 109507 SUCCESS
Total MapReduce CPU Time Spent: 24 seconds 530 msec
OK
Time taken: 71.352 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0006, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0006/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-27 01:53:09,272 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.76 sec
MapReduce Total cumulative CPU time: 1 seconds 760 msec
Ended Job = job_1385527530902_0006
Loading data to table default.q2_minimum_cost_supplier_tmp2
Table default.q2_minimum_cost_supplier_tmp2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6083, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 1.76 sec   HDFS Read: 109743 HDFS Write: 6083 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 760 msec
OK
Time taken: 18.05 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 01:53:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 01:53:11 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_01-53-09_743_5923590253259262704-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 01:53:11 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_01-53-09_743_5923590253259262704-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 01:53:11 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 01:53:11 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 01:53:11 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 01:53:11 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 01:53:11 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 01:53:11 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 01:53:11 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 01:53:12	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 01:53:13	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_01-53-09_743_5923590253259262704-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile61--.hashtable
2013-11-27 01:53:13	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_01-53-09_743_5923590253259262704-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile61--.hashtable
2013-11-27 01:53:13	End of local task; Time Taken: 0.584 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0007, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0007/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0007
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-27 01:53:33,349 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.12 sec
MapReduce Total cumulative CPU time: 2 seconds 120 msec
Ended Job = job_1385527530902_0007
Loading data to table default.q2_minimum_cost_supplier
Table default.q2_minimum_cost_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16329, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.12 sec   HDFS Read: 109743 HDFS Write: 16329 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 120 msec
OK
Time taken: 24.082 seconds
Time:122.92
Running Hive query: tpch/q3_shipping_priority.hive
13/11/27 01:53:34 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 01:53:34 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 01:53:34 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 01:53:34 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 01:53:34 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 01:53:34 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 01:53:34 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.013 seconds
OK
Time taken: 0.133 seconds
OK
Time taken: 0.089 seconds
OK
Time taken: 0.242 seconds
OK
Time taken: 0.225 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.068 seconds
OK
Time taken: 0.057 seconds
Total MapReduce jobs = 7
Stage-16 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0008, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0008/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0008
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-11-27 01:54:09,325 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.23 sec
MapReduce Total cumulative CPU time: 17 seconds 230 msec
Ended Job = job_1385527530902_0008
Stage-14 is filtered out by condition resolver.
Stage-15 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0009, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0009/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0009
Hadoop job information for Stage-2: number of mappers: 5; number of reducers: 1
2013-11-27 01:54:42,681 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 47.17 sec
MapReduce Total cumulative CPU time: 47 seconds 170 msec
Ended Job = job_1385527530902_0009
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0010, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0010/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0010
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-27 01:55:01,798 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 5.05 sec
MapReduce Total cumulative CPU time: 5 seconds 50 msec
Ended Job = job_1385527530902_0010
Launching Job 4 out of 7
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0011, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0011/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0011
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-27 01:55:22,472 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.24 sec
MapReduce Total cumulative CPU time: 4 seconds 240 msec
Ended Job = job_1385527530902_0011
Loading data to table default.q3_shipping_priority
Table default.q3_shipping_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 334, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 3  Reduce: 1   Cumulative CPU: 17.23 sec   HDFS Read: 196303005 HDFS Write: 4901873 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 47.17 sec   HDFS Read: 764786957 HDFS Write: 481131 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 5.05 sec   HDFS Read: 481498 HDFS Write: 481131 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 4.24 sec   HDFS Read: 481498 HDFS Write: 334 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 13 seconds 690 msec
OK
Time taken: 100.032 seconds
Time:109.11
Running Hive query: tpch/q4_order_priority.hive
13/11/27 01:55:24 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 01:55:24 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 01:55:24 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 01:55:24 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 01:55:24 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 01:55:24 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 01:55:24 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.002 seconds
OK
Time taken: 0.096 seconds
OK
Time taken: 0.219 seconds
OK
Time taken: 0.103 seconds
OK
Time taken: 0.221 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.048 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0012, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0012/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0012
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-27 01:56:01,345 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 34.45 sec
MapReduce Total cumulative CPU time: 34 seconds 450 msec
Ended Job = job_1385527530902_0012
Loading data to table default.q4_order_priority_tmp
Table default.q4_order_priority_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 10748076, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 34.45 sec   HDFS Read: 759884717 HDFS Write: 10748076 SUCCESS
Total MapReduce CPU Time Spent: 34 seconds 450 msec
OK
Time taken: 29.984 seconds
Total MapReduce jobs = 4
Stage-9 is selected by condition resolver.
Stage-1 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 01:56:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 01:56:05 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_01-56-01_842_5404314152110579113-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 01:56:05 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_01-56-01_842_5404314152110579113-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 01:56:05 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 01:56:05 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 01:56:05 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 01:56:05 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 01:56:05 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 01:56:05 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 01:56:05 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 01:56:05	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 01:56:06	Processing rows:	200000	Hashtable size:	199999	Memory usage:	38469976	percentage:	0.081
2013-11-27 01:56:06	Processing rows:	300000	Hashtable size:	299999	Memory usage:	49469368	percentage:	0.104
2013-11-27 01:56:06	Processing rows:	400000	Hashtable size:	399999	Memory usage:	62565952	percentage:	0.131
2013-11-27 01:56:06	Processing rows:	500000	Hashtable size:	499999	Memory usage:	71468184	percentage:	0.15
2013-11-27 01:56:06	Processing rows:	600000	Hashtable size:	599999	Memory usage:	80370424	percentage:	0.168
2013-11-27 01:56:06	Processing rows:	700000	Hashtable size:	699999	Memory usage:	89272624	percentage:	0.187
2013-11-27 01:56:06	Processing rows:	800000	Hashtable size:	799999	Memory usage:	106563480	percentage:	0.223
2013-11-27 01:56:07	Processing rows:	900000	Hashtable size:	899999	Memory usage:	93072952	percentage:	0.195
2013-11-27 01:56:07	Processing rows:	1000000	Hashtable size:	999999	Memory usage:	102181216	percentage:	0.214
2013-11-27 01:56:07	Processing rows:	1100000	Hashtable size:	1099999	Memory usage:	111289528	percentage:	0.233
2013-11-27 01:56:08	Processing rows:	1200000	Hashtable size:	1199999	Memory usage:	118879784	percentage:	0.249
2013-11-27 01:56:08	Processing rows:	1300000	Hashtable size:	1299999	Memory usage:	127988072	percentage:	0.268
2013-11-27 01:56:08	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_01-56-01_842_5404314152110579113-1/-local-10004/HashTable-Stage-6/MapJoin-mapfile01--.hashtable
2013-11-27 01:56:08	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_01-56-01_842_5404314152110579113-1/-local-10004/HashTable-Stage-6/MapJoin-mapfile01--.hashtable
2013-11-27 01:56:08	End of local task; Time Taken: 2.833 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 4
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0013, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0013/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0013
Hadoop job information for Stage-6: number of mappers: 2; number of reducers: 0
2013-11-27 01:59:55,169 Stage-6 map = 0%,  reduce = 0%
Ended Job = job_1385527530902_0013 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1385527530902_0013_m_000001 (and more) from job job_1385527530902_0013

Task with the most failures(4): 
-----
Task ID:
  task_1385527530902_0013_m_000000

URL:
  http://hadoop11:8088/taskdetails.jsp?jobid=job_1385527530902_0013&tipid=task_1385527530902_0013_m_000000
-----
Diagnostic Messages for this Task:
Error: GC overhead limit exceeded

FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Job 0: Map: 2   FAIL
Total MapReduce CPU Time Spent: -1 msec
Command exited with non-zero status 2
Time:272.36
Running Hive query: tpch/q5_local_supplier_volume.hive
13/11/27 01:59:56 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 01:59:56 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 01:59:56 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 01:59:56 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 01:59:56 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 01:59:56 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 01:59:56 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.923 seconds
OK
Time taken: 0.131 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.093 seconds
OK
Time taken: 0.2 seconds
OK
Time taken: 0.209 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.083 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 12
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:00:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:00:10 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-00-04_664_5702442531271668899-1/-local-10023/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:00:10 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-00-04_664_5702442531271668899-1/-local-10023/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:00:10 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:00:10 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:00:10 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:00:10 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:00:10 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:00:10 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:00:10 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:00:11	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:00:12	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-00-04_664_5702442531271668899-1/-local-10020/HashTable-Stage-22/MapJoin-mapfile60--.hashtable
2013-11-27 02:00:12	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-00-04_664_5702442531271668899-1/-local-10020/HashTable-Stage-22/MapJoin-mapfile60--.hashtable
2013-11-27 02:00:12	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-00-04_664_5702442531271668899-1/-local-10020/HashTable-Stage-22/MapJoin-mapfile71--.hashtable
2013-11-27 02:00:12	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-00-04_664_5702442531271668899-1/-local-10020/HashTable-Stage-22/MapJoin-mapfile71--.hashtable
2013-11-27 02:00:12	End of local task; Time Taken: 1.256 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 12
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0014, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0014/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0014
Hadoop job information for Stage-22: number of mappers: 1; number of reducers: 0
2013-11-27 02:00:25,835 Stage-22 map = 100%,  reduce = 0%, Cumulative CPU 1.39 sec
MapReduce Total cumulative CPU time: 1 seconds 390 msec
Ended Job = job_1385527530902_0014
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 12
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0015, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0015/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0015
Hadoop job information for Stage-8: number of mappers: 5; number of reducers: 1
2013-11-27 02:01:02,502 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 55.54 sec
MapReduce Total cumulative CPU time: 55 seconds 540 msec
Ended Job = job_1385527530902_0015
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 12
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0016, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0016/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0016
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-11-27 02:01:29,014 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 23.96 sec
MapReduce Total cumulative CPU time: 23 seconds 960 msec
Ended Job = job_1385527530902_0016
Stage-24 is filtered out by condition resolver.
Stage-25 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 12
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0017, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0017/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0017
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-27 02:01:52,239 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 10.83 sec
MapReduce Total cumulative CPU time: 10 seconds 830 msec
Ended Job = job_1385527530902_0017
Launching Job 5 out of 12
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0018, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0018/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0018
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-27 02:02:39,582 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.32 sec
MapReduce Total cumulative CPU time: 1 seconds 320 msec
Ended Job = job_1385527530902_0018
Launching Job 6 out of 12
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0019, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0019/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0019
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-27 02:02:57,154 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.4 sec
MapReduce Total cumulative CPU time: 1 seconds 400 msec
Ended Job = job_1385527530902_0019
Loading data to table default.q5_local_supplier_volume
Table default.q5_local_supplier_volume stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 137, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.39 sec   HDFS Read: 2424 HDFS Write: 57058 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 55.54 sec   HDFS Read: 759942142 HDFS Write: 54824341 SUCCESS
Job 2: Map: 3  Reduce: 1   Cumulative CPU: 23.96 sec   HDFS Read: 226781365 HDFS Write: 8237498 SUCCESS
Job 3: Map: 2  Reduce: 1   Cumulative CPU: 10.83 sec   HDFS Read: 32584213 HDFS Write: 257 SUCCESS
Job 4: Map: 1  Reduce: 1   Cumulative CPU: 1.32 sec   HDFS Read: 624 HDFS Write: 257 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 1.4 sec   HDFS Read: 624 HDFS Write: 137 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 34 seconds 440 msec
OK
Time taken: 172.958 seconds
Time:182.31
Running Hive query: tpch/q6_forecast_revenue_change.hive
13/11/27 02:02:58 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:02:58 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:02:58 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:02:58 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:02:58 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:02:58 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:02:58 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.01 seconds
OK
Time taken: 0.173 seconds
OK
Time taken: 0.227 seconds
OK
Time taken: 0.048 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0020, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0020/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0020
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-27 02:03:31,292 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 26.66 sec
MapReduce Total cumulative CPU time: 26 seconds 660 msec
Ended Job = job_1385527530902_0020
Loading data to table default.q6_forecast_revenue_change
Table default.q6_forecast_revenue_change stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 20, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 26.66 sec   HDFS Read: 759884717 HDFS Write: 20 SUCCESS
Total MapReduce CPU Time Spent: 26 seconds 660 msec
OK
Time taken: 25.677 seconds
Time:34.23
Running Hive query: tpch/q7_volume_shipping.hive
13/11/27 02:03:33 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:03:33 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:03:33 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:03:33 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:03:33 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:03:33 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:03:33 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.0 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.163 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.184 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.024 seconds
OK
Time taken: 0.084 seconds
OK
Time taken: 0.042 seconds
Total MapReduce jobs = 3
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:03:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:03:43 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-03-41_138_858998337987838999-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:03:44 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-03-41_138_858998337987838999-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:03:44 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:03:44 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:03:44 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:03:44 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:03:44 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:03:44 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:03:44 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:03:44	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:03:45	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-03-41_138_858998337987838999-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-11-27 02:03:45	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-03-41_138_858998337987838999-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-11-27 02:03:45	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-03-41_138_858998337987838999-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-11-27 02:03:45	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-03-41_138_858998337987838999-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-11-27 02:03:45	End of local task; Time Taken: 0.924 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0021, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0021/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0021
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 0
2013-11-27 02:03:58,822 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.42 sec
MapReduce Total cumulative CPU time: 1 seconds 420 msec
Ended Job = job_1385527530902_0021
Stage-5 is selected by condition resolver.
Stage-4 is filtered out by condition resolver.
Stage-6 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-11-27_02-03-41_138_858998337987838999-1/-ext-10000
Loading data to table default.q7_volume_shipping_tmp
Table default.q7_volume_shipping_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 38, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.42 sec   HDFS Read: 2424 HDFS Write: 38 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 420 msec
OK
Time taken: 18.241 seconds
Total MapReduce jobs = 6
Stage-6 is selected by condition resolver.
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0022, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0022/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0022
Hadoop job information for Stage-6: number of mappers: 6; number of reducers: 1
2013-11-27 02:04:36,599 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 54.67 sec
MapReduce Total cumulative CPU time: 54 seconds 670 msec
Ended Job = job_1385527530902_0022
Stage-20 is filtered out by condition resolver.
Stage-21 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0023, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0023/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0023
Hadoop job information for Stage-7: number of mappers: 2; number of reducers: 1
2013-11-27 02:05:09,341 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 25.65 sec
MapReduce Total cumulative CPU time: 25 seconds 650 msec
Ended Job = job_1385527530902_0023
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:05:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:05:11 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-03-59_381_1565185882795491002-1/-local-10018/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:05:11 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-03-59_381_1565185882795491002-1/-local-10018/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:05:11 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:05:11 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:05:11 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:05:11 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:05:11 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:05:11 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:05:11 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:05:12	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:05:13	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-03-59_381_1565185882795491002-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile30--.hashtable
2013-11-27 02:05:13	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-03-59_381_1565185882795491002-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile30--.hashtable
2013-11-27 02:05:13	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-03-59_381_1565185882795491002-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-11-27 02:05:13	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-03-59_381_1565185882795491002-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-11-27 02:05:13	End of local task; Time Taken: 1.099 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0024, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0024/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0024
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-27 02:05:40,006 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 12.23 sec
MapReduce Total cumulative CPU time: 12 seconds 230 msec
Ended Job = job_1385527530902_0024
Launching Job 4 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0025, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0025/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0025
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-27 02:05:57,261 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.42 sec
MapReduce Total cumulative CPU time: 1 seconds 420 msec
Ended Job = job_1385527530902_0025
Loading data to table default.q7_volume_shipping
Table default.q7_volume_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 160, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 6  Reduce: 1   Cumulative CPU: 54.67 sec   HDFS Read: 931841374 HDFS Write: 93291795 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 25.65 sec   HDFS Read: 117638510 HDFS Write: 88566860 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 12.23 sec   HDFS Read: 88567227 HDFS Write: 268 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 1.42 sec   HDFS Read: 635 HDFS Write: 160 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 33 seconds 970 msec
OK
Time taken: 118.329 seconds
Time:145.84
Running Hive query: tpch/q8_national_market_share.hive
13/11/27 02:05:58 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:05:58 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:05:58 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:05:58 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:05:58 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:05:58 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:05:58 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.019 seconds
OK
Time taken: 0.098 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.124 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.084 seconds
OK
Time taken: 0.144 seconds
OK
Time taken: 0.191 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.067 seconds
OK
Time taken: 0.025 seconds
OK
Time taken: 0.042 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:06:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:06:14 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-06-07_076_4272481895488143493-1/-local-10031/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:06:14 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-06-07_076_4272481895488143493-1/-local-10031/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:06:14 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:06:14 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:06:14 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:06:14 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:06:14 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:06:14 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:06:14 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:06:15	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:06:16	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-06-07_076_4272481895488143493-1/-local-10028/HashTable-Stage-30/MapJoin-mapfile101--.hashtable
2013-11-27 02:06:16	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-06-07_076_4272481895488143493-1/-local-10028/HashTable-Stage-30/MapJoin-mapfile101--.hashtable
2013-11-27 02:06:16	End of local task; Time Taken: 0.78 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0026, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0026/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0026
Hadoop job information for Stage-30: number of mappers: 1; number of reducers: 0
2013-11-27 02:06:28,415 Stage-30 map = 100%,  reduce = 0%, Cumulative CPU 0.96 sec
MapReduce Total cumulative CPU time: 960 msec
Ended Job = job_1385527530902_0026
Stage-38 is filtered out by condition resolver.
Stage-39 is filtered out by condition resolver.
Stage-9 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0027, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0027/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0027
Hadoop job information for Stage-9: number of mappers: 2; number of reducers: 1
2013-11-27 02:06:48,517 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 6.01 sec
MapReduce Total cumulative CPU time: 6 seconds 10 msec
Ended Job = job_1385527530902_0027
Stage-36 is filtered out by condition resolver.
Stage-37 is filtered out by condition resolver.
Stage-10 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0028, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0028/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0028
Hadoop job information for Stage-10: number of mappers: 3; number of reducers: 1
2013-11-27 02:07:10,053 Stage-10 map = 100%,  reduce = 100%, Cumulative CPU 16.6 sec
MapReduce Total cumulative CPU time: 16 seconds 600 msec
Ended Job = job_1385527530902_0028
Stage-34 is filtered out by condition resolver.
Stage-35 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0029, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0029/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0029
Hadoop job information for Stage-1: number of mappers: 5; number of reducers: 1
2013-11-27 02:07:49,483 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 58.2 sec
MapReduce Total cumulative CPU time: 58 seconds 200 msec
Ended Job = job_1385527530902_0029
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0030, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0030/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0030
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-27 02:08:12,560 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 11.96 sec
MapReduce Total cumulative CPU time: 11 seconds 960 msec
Ended Job = job_1385527530902_0030
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:08:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:08:14 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-06-07_076_4272481895488143493-1/-local-10043/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:08:14 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-06-07_076_4272481895488143493-1/-local-10043/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:08:14 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:08:14 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:08:14 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:08:14 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:08:14 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:08:14 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:08:14 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:08:15	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:08:16	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-06-07_076_4272481895488143493-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-11-27 02:08:16	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-06-07_076_4272481895488143493-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-11-27 02:08:16	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-06-07_076_4272481895488143493-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile10--.hashtable
2013-11-27 02:08:16	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-06-07_076_4272481895488143493-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile10--.hashtable
2013-11-27 02:08:16	End of local task; Time Taken: 1.003 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0031, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0031/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0031
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-27 02:08:35,411 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 3.9 sec
MapReduce Total cumulative CPU time: 3 seconds 900 msec
Ended Job = job_1385527530902_0031
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0032, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0032/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0032
Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 1
2013-11-27 02:08:52,903 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 1.35 sec
MapReduce Total cumulative CPU time: 1 seconds 350 msec
Ended Job = job_1385527530902_0032
Loading data to table default.q8_national_market_share
Table default.q8_national_market_share stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 50, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.96 sec   HDFS Read: 2424 HDFS Write: 186 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 6.01 sec   HDFS Read: 24346901 HDFS Write: 621992 SUCCESS
Job 2: Map: 3  Reduce: 1   Cumulative CPU: 16.6 sec   HDFS Read: 172579016 HDFS Write: 2941437 SUCCESS
Job 3: Map: 5  Reduce: 1   Cumulative CPU: 58.2 sec   HDFS Read: 762826521 HDFS Write: 18639423 SUCCESS
Job 4: Map: 2  Reduce: 1   Cumulative CPU: 11.96 sec   HDFS Read: 42775111 HDFS Write: 123284 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 3.9 sec   HDFS Read: 123651 HDFS Write: 152 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.35 sec   HDFS Read: 519 HDFS Write: 50 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 38 seconds 980 msec
OK
Time taken: 166.257 seconds
Time:175.61
Running Hive query: tpch/q9_product_type_profit.hive
13/11/27 02:08:54 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:08:54 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:08:54 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:08:54 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:08:54 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:08:54 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:08:54 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.989 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.112 seconds
OK
Time taken: 0.128 seconds
OK
Time taken: 0.104 seconds
OK
Time taken: 0.083 seconds
OK
Time taken: 0.14 seconds
OK
Time taken: 0.193 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.074 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:09:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:09:09 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-09-02_588_1015802334663082024-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:09:09 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-09-02_588_1015802334663082024-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:09:09 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:09:09 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:09:09 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:09:09 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:09:09 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:09:09 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:09:09 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:09:10	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:09:10	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-09-02_588_1015802334663082024-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-11-27 02:09:10	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-09-02_588_1015802334663082024-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-11-27 02:09:10	End of local task; Time Taken: 0.584 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0033, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0033/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0033
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-11-27 02:09:23,338 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 1.92 sec
MapReduce Total cumulative CPU time: 1 seconds 920 msec
Ended Job = job_1385527530902_0033
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0034, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0034/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0034
Hadoop job information for Stage-8: number of mappers: 5; number of reducers: 1
2013-11-27 02:10:22,249 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 79.56 sec
MapReduce Total cumulative CPU time: 1 minutes 19 seconds 560 msec
Ended Job = job_1385527530902_0034
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0035, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0035/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0035
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-27 02:11:25,249 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 87.68 sec
MapReduce Total cumulative CPU time: 1 minutes 27 seconds 680 msec
Ended Job = job_1385527530902_0035
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0036, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0036/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0036
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 1
2013-11-27 02:12:11,639 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 70.14 sec
MapReduce Total cumulative CPU time: 1 minutes 10 seconds 140 msec
Ended Job = job_1385527530902_0036
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0037, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0037/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0037
Hadoop job information for Stage-3: number of mappers: 3; number of reducers: 1
2013-11-27 02:12:37,829 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 22.61 sec
MapReduce Total cumulative CPU time: 22 seconds 610 msec
Ended Job = job_1385527530902_0037
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0038, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0038/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0038
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-27 02:12:55,242 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.64 sec
MapReduce Total cumulative CPU time: 1 seconds 640 msec
Ended Job = job_1385527530902_0038
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0039, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0039/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0039
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-27 02:13:12,565 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 1.37 sec
MapReduce Total cumulative CPU time: 1 seconds 370 msec
Ended Job = job_1385527530902_0039
Loading data to table default.q9_product_type_profit
Table default.q9_product_type_profit stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 5817, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.92 sec   HDFS Read: 1409388 HDFS Write: 283161 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 79.56 sec   HDFS Read: 760168245 HDFS Write: 361738849 SUCCESS
Job 2: Map: 4  Reduce: 1   Cumulative CPU: 87.68 sec   HDFS Read: 480731297 HDFS Write: 392272628 SUCCESS
Job 3: Map: 4  Reduce: 1   Cumulative CPU: 70.14 sec   HDFS Read: 416415230 HDFS Write: 19699656 SUCCESS
Job 4: Map: 3  Reduce: 1   Cumulative CPU: 22.61 sec   HDFS Read: 191656680 HDFS Write: 6470 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 1.64 sec   HDFS Read: 6837 HDFS Write: 6470 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.37 sec   HDFS Read: 6837 HDFS Write: 5817 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 24 seconds 920 msec
OK
Time taken: 250.481 seconds
Time:259.72
Running Hive query: tpch/q10_returned_item.hive
13/11/27 02:13:14 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:13:14 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:13:14 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:13:14 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:13:14 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:13:14 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:13:14 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.057 seconds
OK
Time taken: 0.086 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.194 seconds
OK
Time taken: 0.205 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.024 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 8
Stage-20 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 1 out of 8
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0040, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0040/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0040
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-11-27 02:13:47,773 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 16.16 sec
MapReduce Total cumulative CPU time: 16 seconds 160 msec
Ended Job = job_1385527530902_0040
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:13:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:13:49 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-13-22_143_5764317856636847204-1/-local-10017/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:13:49 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-13-22_143_5764317856636847204-1/-local-10017/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:13:49 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:13:49 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:13:49 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:13:49 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:13:49 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:13:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:13:49 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:13:50	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:13:51	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-13-22_143_5764317856636847204-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-11-27 02:13:51	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-13-22_143_5764317856636847204-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-11-27 02:13:51	End of local task; Time Taken: 0.561 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 8
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0041, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0041/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0041
Hadoop job information for Stage-13: number of mappers: 1; number of reducers: 0
2013-11-27 02:14:04,213 Stage-13 map = 100%,  reduce = 0%, Cumulative CPU 3.3 sec
MapReduce Total cumulative CPU time: 3 seconds 300 msec
Ended Job = job_1385527530902_0041
Stage-17 is filtered out by condition resolver.
Stage-18 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 3 out of 8
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0042, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0042/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0042
Hadoop job information for Stage-3: number of mappers: 5; number of reducers: 1
2013-11-27 02:14:30,292 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 36.74 sec
MapReduce Total cumulative CPU time: 36 seconds 740 msec
Ended Job = job_1385527530902_0042
Launching Job 4 out of 8
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0043, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0043/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0043
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-27 02:14:49,759 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 6.18 sec
MapReduce Total cumulative CPU time: 6 seconds 180 msec
Ended Job = job_1385527530902_0043
Launching Job 5 out of 8
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0044, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0044/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0044
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-27 02:15:08,605 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 4.51 sec
MapReduce Total cumulative CPU time: 4 seconds 510 msec
Ended Job = job_1385527530902_0044
Loading data to table default.q10_returned_item
Table default.q10_returned_item stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3322, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 3  Reduce: 1   Cumulative CPU: 16.16 sec   HDFS Read: 196303005 HDFS Write: 9679083 SUCCESS
Job 1: Map: 1   Cumulative CPU: 3.3 sec   HDFS Read: 9679450 HDFS Write: 10085522 SUCCESS
Job 2: Map: 5  Reduce: 1   Cumulative CPU: 36.74 sec   HDFS Read: 769970606 HDFS Write: 6865977 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 6.18 sec   HDFS Read: 6866344 HDFS Write: 6865957 SUCCESS
Job 4: Map: 1  Reduce: 1   Cumulative CPU: 4.51 sec   HDFS Read: 6866324 HDFS Write: 3322 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 6 seconds 890 msec
OK
Time taken: 106.956 seconds
Time:116.02
Running Hive query: tpch/q11_important_stock.hive
13/11/27 02:15:10 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:15:10 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:15:10 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:15:10 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:15:10 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:15:10 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:15:10 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.007 seconds
OK
Time taken: 0.086 seconds
OK
Time taken: 0.092 seconds
OK
Time taken: 0.148 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.084 seconds
OK
Time taken: 0.229 seconds
OK
Time taken: 0.025 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.083 seconds
Total MapReduce jobs = 5
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:15:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:15:22 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-15-18_304_3882142351513380730-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:15:22 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-15-18_304_3882142351513380730-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:15:22 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:15:22 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:15:22 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:15:22 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:15:22 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:15:22 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:15:22 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:15:23	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:15:23	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-15-18_304_3882142351513380730-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-11-27 02:15:23	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-15-18_304_3882142351513380730-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-11-27 02:15:23	End of local task; Time Taken: 0.801 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0045, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0045/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0045
Hadoop job information for Stage-10: number of mappers: 1; number of reducers: 0
2013-11-27 02:15:35,977 Stage-10 map = 100%,  reduce = 0%, Cumulative CPU 1.41 sec
MapReduce Total cumulative CPU time: 1 seconds 410 msec
Ended Job = job_1385527530902_0045
Stage-11 is filtered out by condition resolver.
Stage-12 is selected by condition resolver.
Stage-2 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:15:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:15:38 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-15-18_304_3882142351513380730-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:15:38 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-15-18_304_3882142351513380730-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:15:38 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:15:38 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:15:38 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:15:38 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:15:38 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:15:38 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:15:38 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:15:39	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:15:39	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-15-18_304_3882142351513380730-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile10--.hashtable
2013-11-27 02:15:39	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-15-18_304_3882142351513380730-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile10--.hashtable
2013-11-27 02:15:39	End of local task; Time Taken: 0.564 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 3 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0046, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0046/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0046
Hadoop job information for Stage-8: number of mappers: 1; number of reducers: 0
2013-11-27 02:15:54,343 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 4.87 sec
MapReduce Total cumulative CPU time: 4 seconds 870 msec
Ended Job = job_1385527530902_0046
Launching Job 4 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0047, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0047/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0047
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-27 02:16:13,693 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 5.46 sec
MapReduce Total cumulative CPU time: 5 seconds 460 msec
Ended Job = job_1385527530902_0047
Loading data to table default.q11_part_tmp
Table default.q11_part_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 563609, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.41 sec   HDFS Read: 1409388 HDFS Write: 8056 SUCCESS
Job 1: Map: 1   Cumulative CPU: 4.87 sec   HDFS Read: 118984820 HDFS Write: 863493 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 5.46 sec   HDFS Read: 863860 HDFS Write: 563609 SUCCESS
Total MapReduce CPU Time Spent: 11 seconds 740 msec
OK
Time taken: 55.902 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0048, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0048/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0048
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-27 02:16:32,658 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.5 sec
MapReduce Total cumulative CPU time: 2 seconds 500 msec
Ended Job = job_1385527530902_0048
Loading data to table default.q11_sum_tmp
Table default.q11_sum_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.5 sec   HDFS Read: 563828 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 500 msec
OK
Time taken: 18.932 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:16:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:16:35 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-16-33_139_2072112191885930870-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:16:35 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-16-33_139_2072112191885930870-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:16:35 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:16:35 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:16:35 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:16:35 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:16:35 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:16:35 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:16:35 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:16:36	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:16:36	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-16-33_139_2072112191885930870-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-11-27 02:16:36	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-16-33_139_2072112191885930870-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-11-27 02:16:36	End of local task; Time Taken: 0.569 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0049, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0049/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0049
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-27 02:16:55,551 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.52 sec
MapReduce Total cumulative CPU time: 3 seconds 520 msec
Ended Job = job_1385527530902_0049
Loading data to table default.q11_important_stock
Table default.q11_important_stock stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 20102, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.52 sec   HDFS Read: 563828 HDFS Write: 20102 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 520 msec
OK
Time taken: 22.85 seconds
Time:106.89
Running Hive query: tpch/q12_shipping.hive
13/11/27 02:16:57 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:16:57 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:16:57 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:16:57 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:16:57 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:16:57 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:16:57 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.003 seconds
OK
Time taken: 0.096 seconds
OK
Time taken: 0.152 seconds
OK
Time taken: 0.235 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.058 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0050, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0050/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0050
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1
2013-11-27 02:17:34,801 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 41.08 sec
MapReduce Total cumulative CPU time: 41 seconds 80 msec
Ended Job = job_1385527530902_0050
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0051, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0051/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0051
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-27 02:17:51,924 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 1.51 sec
MapReduce Total cumulative CPU time: 1 seconds 510 msec
Ended Job = job_1385527530902_0051
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0052, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0052/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0052
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-27 02:18:09,413 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.92 sec
MapReduce Total cumulative CPU time: 1 seconds 920 msec
Ended Job = job_1385527530902_0052
Loading data to table default.q12_shipping
Table default.q12_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 38, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 6  Reduce: 1   Cumulative CPU: 41.08 sec   HDFS Read: 931841374 HDFS Write: 152 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 1.51 sec   HDFS Read: 519 HDFS Write: 152 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.92 sec   HDFS Read: 519 HDFS Write: 38 SUCCESS
Total MapReduce CPU Time Spent: 44 seconds 510 msec
OK
Time taken: 65.188 seconds
Time:73.90
Running Hive query: tpch/q13_customer_distribution.hive
13/11/27 02:18:11 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:18:11 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:18:11 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:18:11 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:18:11 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:18:11 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:18:11 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.988 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.208 seconds
OK
Time taken: 0.224 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.042 seconds
Total MapReduce jobs = 4
Stage-1 is selected by condition resolver.
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0053, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0053/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0053
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-11-27 02:18:48,740 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 25.12 sec
MapReduce Total cumulative CPU time: 25 seconds 120 msec
Ended Job = job_1385527530902_0053
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0054, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0054/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0054
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-27 02:19:08,962 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.66 sec
MapReduce Total cumulative CPU time: 5 seconds 660 msec
Ended Job = job_1385527530902_0054
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0055, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0055/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0055
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-27 02:19:26,343 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.36 sec
MapReduce Total cumulative CPU time: 1 seconds 360 msec
Ended Job = job_1385527530902_0055
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0056, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0056/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0056
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-27 02:19:43,865 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.82 sec
MapReduce Total cumulative CPU time: 1 seconds 820 msec
Ended Job = job_1385527530902_0056
Loading data to table default.q13_customer_distribution
Table default.q13_customer_distribution stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 295, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 3  Reduce: 1   Cumulative CPU: 25.12 sec   HDFS Read: 196303005 HDFS Write: 3266379 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 5.66 sec   HDFS Read: 3266746 HDFS Write: 955 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.36 sec   HDFS Read: 1322 HDFS Write: 955 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 1.82 sec   HDFS Read: 1322 HDFS Write: 295 SUCCESS
Total MapReduce CPU Time Spent: 33 seconds 960 msec
OK
Time taken: 85.677 seconds
Time:94.46
Running Hive query: tpch/q14_promotion_effect.hive
13/11/27 02:19:45 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:19:45 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:19:45 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:19:45 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:19:45 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:19:45 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:19:45 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.09 seconds
OK
Time taken: 0.089 seconds
OK
Time taken: 0.18 seconds
OK
Time taken: 0.214 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.057 seconds
Total MapReduce jobs = 3
Stage-8 is selected by condition resolver.
Stage-1 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:19:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:19:57 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-19-53_236_7037872187100960770-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:19:57 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-19-53_236_7037872187100960770-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:19:57 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:19:57 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:19:57 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:19:57 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:19:57 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:19:57 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:19:57 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:19:57	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:19:59	Processing rows:	200000	Hashtable size:	199999	Memory usage:	68630840	percentage:	0.144
2013-11-27 02:19:59	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-19-53_236_7037872187100960770-1/-local-10003/HashTable-Stage-6/MapJoin-mapfile10--.hashtable
2013-11-27 02:19:59	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-19-53_236_7037872187100960770-1/-local-10003/HashTable-Stage-6/MapJoin-mapfile10--.hashtable
2013-11-27 02:19:59	End of local task; Time Taken: 2.172 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0057, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0057/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0057
Hadoop job information for Stage-6: number of mappers: 4; number of reducers: 0
2013-11-27 02:20:22,313 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 29.41 sec
MapReduce Total cumulative CPU time: 29 seconds 410 msec
Ended Job = job_1385527530902_0057
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0058, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0058/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0058
Hadoop job information for Stage-2: number of mappers: 3; number of reducers: 1
2013-11-27 02:20:42,101 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.96 sec
MapReduce Total cumulative CPU time: 2 seconds 960 msec
Ended Job = job_1385527530902_0058
Loading data to table default.q14_promotion_effect
Table default.q14_promotion_effect stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 19, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4   Cumulative CPU: 29.41 sec   HDFS Read: 759884717 HDFS Write: 516 SUCCESS
Job 1: Map: 3  Reduce: 1   Cumulative CPU: 2.96 sec   HDFS Read: 1839 HDFS Write: 19 SUCCESS
Total MapReduce CPU Time Spent: 32 seconds 370 msec
OK
Time taken: 49.356 seconds
Time:58.23
Running Hive query: tpch/q15_top_supplier.hive
13/11/27 02:20:43 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:20:43 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:20:43 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:20:43 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:20:43 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:20:43 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:20:43 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.082 seconds
OK
Time taken: 0.087 seconds
OK
Time taken: 0.164 seconds
OK
Time taken: 0.096 seconds
OK
Time taken: 0.102 seconds
OK
Time taken: 0.207 seconds
OK
Time taken: 0.03 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.035 seconds
OK
Time taken: 0.058 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0059, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0059/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0059
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-27 02:21:17,319 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.55 sec
MapReduce Total cumulative CPU time: 27 seconds 550 msec
Ended Job = job_1385527530902_0059
Loading data to table default.revenue
Table default.revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 193625, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 27.55 sec   HDFS Read: 759884717 HDFS Write: 193625 SUCCESS
Total MapReduce CPU Time Spent: 27 seconds 550 msec
OK
Time taken: 26.174 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0060, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0060/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0060
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-27 02:21:35,100 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.31 sec
MapReduce Total cumulative CPU time: 2 seconds 310 msec
Ended Job = job_1385527530902_0060
Loading data to table default.max_revenue
Table default.max_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 19, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.31 sec   HDFS Read: 193839 HDFS Write: 19 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 310 msec
OK
Time taken: 17.675 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:21:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:21:37 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-21-35_541_5390156150235917020-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:21:37 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-21-35_541_5390156150235917020-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:21:37 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:21:37 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:21:37 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:21:37 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:21:37 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:21:37 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:21:37 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:21:38	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:21:39	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-21-35_541_5390156150235917020-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile11--.hashtable
2013-11-27 02:21:39	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-21-35_541_5390156150235917020-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile11--.hashtable
2013-11-27 02:21:39	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-21-35_541_5390156150235917020-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-11-27 02:21:39	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-21-35_541_5390156150235917020-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-11-27 02:21:39	End of local task; Time Taken: 1.154 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0061, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0061/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0061
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-27 02:21:58,877 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 3.14 sec
MapReduce Total cumulative CPU time: 3 seconds 140 msec
Ended Job = job_1385527530902_0061
Loading data to table default.q15_top_supplier
Table default.q15_top_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 77, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.14 sec   HDFS Read: 1409388 HDFS Write: 77 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 140 msec
OK
Time taken: 23.816 seconds
Time:76.75
Running Hive query: tpch/q16_parts_supplier_relationship.hive
13/11/27 02:22:00 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:22:00 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:22:00 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:22:00 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:22:00 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:22:00 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:22:00 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.077 seconds
OK
Time taken: 0.096 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.162 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.13 seconds
OK
Time taken: 0.232 seconds
OK
Time taken: 0.036 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.063 seconds
OK
Time taken: 0.077 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0062, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0062/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0062
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2013-11-27 02:22:22,050 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.19 sec
MapReduce Total cumulative CPU time: 2 seconds 190 msec
Ended Job = job_1385527530902_0062
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-11-27_02-22-08_695_4702620632657984823-1/-ext-10000
Loading data to table default.supplier_tmp
Table default.supplier_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 48875, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 2.19 sec   HDFS Read: 1409388 HDFS Write: 48875 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 190 msec
OK
Time taken: 13.928 seconds
Total MapReduce jobs = 3
Stage-10 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:22:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:22:25 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-22-22_624_2877139154806709875-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:22:25 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-22-22_624_2877139154806709875-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:22:25 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:22:25 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:22:25 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:22:25 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:22:25 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:22:25 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:22:25 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:22:26	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:22:28	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-22-22_624_2877139154806709875-1/-local-10005/HashTable-Stage-6/MapJoin-mapfile11--.hashtable
2013-11-27 02:22:28	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-22-22_624_2877139154806709875-1/-local-10005/HashTable-Stage-6/MapJoin-mapfile11--.hashtable
2013-11-27 02:22:28	End of local task; Time Taken: 1.98 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0063, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0063/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0063
Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 0
2013-11-27 02:22:45,944 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 7.75 sec
MapReduce Total cumulative CPU time: 7 seconds 750 msec
Ended Job = job_1385527530902_0063
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:22:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:22:47 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-22-22_624_2877139154806709875-1/-local-10012/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:22:47 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-22-22_624_2877139154806709875-1/-local-10012/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:22:48 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:22:48 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:22:48 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:22:48 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:22:48 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:22:48 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:22:48 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:22:48	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:22:49	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-22-22_624_2877139154806709875-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-27 02:22:49	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-22-22_624_2877139154806709875-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-27 02:22:49	End of local task; Time Taken: 0.802 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 3 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0064, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0064/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0064
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 0
2013-11-27 02:23:05,969 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 6.9 sec
MapReduce Total cumulative CPU time: 6 seconds 900 msec
Ended Job = job_1385527530902_0064
Loading data to table default.q16_tmp
Table default.q16_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 28413398, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 7.75 sec   HDFS Read: 118984820 HDFS Write: 38640426 SUCCESS
Job 1: Map: 1   Cumulative CPU: 6.9 sec   HDFS Read: 38640793 HDFS Write: 28413398 SUCCESS
Total MapReduce CPU Time Spent: 14 seconds 650 msec
OK
Time taken: 43.807 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0065, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0065/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0065
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-27 02:23:28,043 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.1 sec
MapReduce Total cumulative CPU time: 8 seconds 100 msec
Ended Job = job_1385527530902_0065
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0066, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0066/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0066
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-27 02:23:47,318 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.5 sec
MapReduce Total cumulative CPU time: 5 seconds 500 msec
Ended Job = job_1385527530902_0066
Loading data to table default.q16_parts_supplier_relationship
Table default.q16_parts_supplier_relationship stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 649661, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 8.1 sec   HDFS Read: 28413612 HDFS Write: 917247 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 5.5 sec   HDFS Read: 917614 HDFS Write: 649661 SUCCESS
Total MapReduce CPU Time Spent: 13 seconds 600 msec
OK
Time taken: 41.306 seconds
Time:108.38
Running Hive query: tpch/q17_small_quantity_order_revenue.hive
13/11/27 02:23:48 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:23:48 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:23:48 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:23:48 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:23:48 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:23:48 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:23:48 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.002 seconds
OK
Time taken: 0.086 seconds
OK
Time taken: 0.161 seconds
OK
Time taken: 0.134 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.047 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0067, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0067/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0067
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-27 02:24:36,894 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 57.22 sec
MapReduce Total cumulative CPU time: 57 seconds 220 msec
Ended Job = job_1385527530902_0067
Loading data to table default.lineitem_tmp
Table default.lineitem_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 4543028, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 57.22 sec   HDFS Read: 759884717 HDFS Write: 4543028 SUCCESS
Total MapReduce CPU Time Spent: 57 seconds 220 msec
OK
Time taken: 40.777 seconds
Total MapReduce jobs = 3
Stage-12 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0068, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0068/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0068
Hadoop job information for Stage-1: number of mappers: 5; number of reducers: 1
2013-11-27 02:25:15,160 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 59.87 sec
MapReduce Total cumulative CPU time: 59 seconds 870 msec
Ended Job = job_1385527530902_0068
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:25:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:25:17 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-24-37_416_613525783061705189-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:25:17 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-24-37_416_613525783061705189-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:25:17 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:25:17 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:25:17 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:25:17 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:25:17 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:25:17 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:25:17 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:25:17	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:25:19	Processing rows:	200000	Hashtable size:	199999	Memory usage:	85803072	percentage:	0.18
2013-11-27 02:25:19	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-24-37_416_613525783061705189-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-11-27 02:25:19	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-24-37_416_613525783061705189-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-11-27 02:25:19	End of local task; Time Taken: 1.76 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0069, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0069/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0069
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-27 02:25:40,288 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 4.64 sec
MapReduce Total cumulative CPU time: 4 seconds 640 msec
Ended Job = job_1385527530902_0069
Loading data to table default.q17_small_quantity_order_revenue
Table default.q17_small_quantity_order_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 18, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 5  Reduce: 1   Cumulative CPU: 59.87 sec   HDFS Read: 784020038 HDFS Write: 225252 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 4.64 sec   HDFS Read: 225618 HDFS Write: 18 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 4 seconds 510 msec
OK
Time taken: 63.313 seconds
Time:112.98
Running Hive query: tpch/q18_large_volume_customer.hive
13/11/27 02:25:41 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:25:41 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:25:41 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:25:41 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:25:41 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:25:41 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:25:41 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.918 seconds
OK
Time taken: 0.271 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.006 seconds
OK
Time taken: 0.31 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.042 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0070, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0070/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0070
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-27 02:26:20,745 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 37.89 sec
MapReduce Total cumulative CPU time: 37 seconds 890 msec
Ended Job = job_1385527530902_0070
Loading data to table default.q18_tmp
Table default.q18_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 19917655, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 37.89 sec   HDFS Read: 759884717 HDFS Write: 19917655 SUCCESS
Total MapReduce CPU Time Spent: 37 seconds 890 msec
OK
Time taken: 31.589 seconds
Total MapReduce jobs = 7
Stage-17 is filtered out by condition resolver.
Stage-5 is selected by condition resolver.
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0071, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0071/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0071
Hadoop job information for Stage-5: number of mappers: 3; number of reducers: 1
2013-11-27 02:26:52,675 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 23.37 sec
MapReduce Total cumulative CPU time: 23 seconds 370 msec
Ended Job = job_1385527530902_0071
Stage-15 is filtered out by condition resolver.
Stage-16 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0072, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0072/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0072
Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1
2013-11-27 02:27:31,734 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 68.0 sec
MapReduce Total cumulative CPU time: 1 minutes 8 seconds 0 msec
Ended Job = job_1385527530902_0072
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0073, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0073/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0073
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-27 02:27:49,074 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 1.36 sec
MapReduce Total cumulative CPU time: 1 seconds 360 msec
Ended Job = job_1385527530902_0073
Launching Job 4 out of 7
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0074, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0074/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0074
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-27 02:28:06,172 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.44 sec
MapReduce Total cumulative CPU time: 1 seconds 440 msec
Ended Job = job_1385527530902_0074
Loading data to table default.q18_large_volume_customer
Table default.q18_large_volume_customer stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3427, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 3  Reduce: 1   Cumulative CPU: 23.37 sec   HDFS Read: 196303005 HDFS Write: 94749328 SUCCESS
Job 1: Map: 6  Reduce: 1   Cumulative CPU: 68.0 sec   HDFS Read: 874552281 HDFS Write: 4159 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.36 sec   HDFS Read: 4526 HDFS Write: 4139 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 1.44 sec   HDFS Read: 4506 HDFS Write: 3427 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 34 seconds 170 msec
OK
Time taken: 105.322 seconds
Time:145.89
Running Hive query: tpch/q19_discounted_revenue.hive
13/11/27 02:28:07 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:28:07 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:28:07 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:28:07 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:28:07 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:28:07 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:28:07 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.042 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.173 seconds
OK
Time taken: 0.2 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 3
Stage-8 is selected by condition resolver.
Stage-1 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:28:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:28:19 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-28-15_399_4642823734772407803-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:28:19 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-28-15_399_4642823734772407803-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:28:19 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:28:19 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:28:19 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:28:19 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:28:19 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:28:19 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:28:19 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:28:20	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:28:21	Processing rows:	200000	Hashtable size:	199999	Memory usage:	108704960	percentage:	0.228
2013-11-27 02:28:21	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-28-15_399_4642823734772407803-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-27 02:28:21	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-28-15_399_4642823734772407803-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-27 02:28:21	End of local task; Time Taken: 1.686 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0075, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0075/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0075
Hadoop job information for Stage-5: number of mappers: 4; number of reducers: 0
2013-11-27 02:28:54,940 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 61.95 sec
MapReduce Total cumulative CPU time: 1 minutes 1 seconds 950 msec
Ended Job = job_1385527530902_0075
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0076, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0076/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0076
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 1
2013-11-27 02:29:14,351 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.21 sec
MapReduce Total cumulative CPU time: 3 seconds 210 msec
Ended Job = job_1385527530902_0076
Loading data to table default.q19_discounted_revenue
Table default.q19_discounted_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4   Cumulative CPU: 61.95 sec   HDFS Read: 759884717 HDFS Write: 484 SUCCESS
Job 1: Map: 4  Reduce: 1   Cumulative CPU: 3.21 sec   HDFS Read: 1952 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 5 seconds 160 msec
OK
Time taken: 59.458 seconds
Time:68.22
Running Hive query: tpch/q20_potential_part_promotion.hive
13/11/27 02:29:16 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:29:16 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:29:16 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:29:16 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:29:16 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:29:16 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:29:16 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.969 seconds
OK
Time taken: 0.097 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.151 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.104 seconds
OK
Time taken: 0.096 seconds
OK
Time taken: 0.102 seconds
OK
Time taken: 0.206 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.084 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0077, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0077/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0077
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-27 02:29:44,065 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.78 sec
MapReduce Total cumulative CPU time: 3 seconds 780 msec
Ended Job = job_1385527530902_0077
Loading data to table default.q20_tmp1
Table default.q20_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 13746, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.78 sec   HDFS Read: 24135321 HDFS Write: 13746 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 780 msec
OK
Time taken: 20.248 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0078, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0078/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0078
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-27 02:30:12,410 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 35.52 sec
MapReduce Total cumulative CPU time: 35 seconds 520 msec
Ended Job = job_1385527530902_0078
Loading data to table default.q20_tmp2
Table default.q20_tmp2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 8750143, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 35.52 sec   HDFS Read: 759884717 HDFS Write: 8750143 SUCCESS
Total MapReduce CPU Time Spent: 35 seconds 520 msec
OK
Time taken: 28.244 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:30:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:30:15 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-30-12_865_385444329940511480-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:30:15 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-30-12_865_385444329940511480-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:30:15 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:30:15 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:30:15 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:30:15 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:30:15 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:30:15 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:30:15 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:30:15	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:30:17	Processing rows:	200000	Hashtable size:	199999	Memory usage:	69474464	percentage:	0.146
2013-11-27 02:30:17	Processing rows:	300000	Hashtable size:	299999	Memory usage:	124959136	percentage:	0.262
2013-11-27 02:30:17	Processing rows:	400000	Hashtable size:	399999	Memory usage:	127970256	percentage:	0.268
2013-11-27 02:30:17	Processing rows:	500000	Hashtable size:	499999	Memory usage:	183066672	percentage:	0.384
2013-11-27 02:30:19	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-30-12_865_385444329940511480-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-27 02:30:19	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-30-12_865_385444329940511480-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-27 02:30:19	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-30-12_865_385444329940511480-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile11--.hashtable
2013-11-27 02:30:19	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-30-12_865_385444329940511480-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile11--.hashtable
2013-11-27 02:30:19	End of local task; Time Taken: 3.879 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0079, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0079/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0079
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 0
2013-11-27 02:30:40,908 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 14.25 sec
MapReduce Total cumulative CPU time: 14 seconds 250 msec
Ended Job = job_1385527530902_0079
Loading data to table default.q20_tmp3
Table default.q20_tmp3 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 85072, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 14.25 sec   HDFS Read: 118984820 HDFS Write: 85072 SUCCESS
Total MapReduce CPU Time Spent: 14 seconds 250 msec
OK
Time taken: 28.507 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0080, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0080/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0080
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-27 02:30:59,967 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.2 sec
MapReduce Total cumulative CPU time: 3 seconds 200 msec
Ended Job = job_1385527530902_0080
Loading data to table default.q20_tmp4
Table default.q20_tmp4 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21481, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.2 sec   HDFS Read: 85287 HDFS Write: 21481 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 200 msec
OK
Time taken: 19.04 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:31:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:31:02 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-31-00_414_671380806946562416-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:31:02 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-31-00_414_671380806946562416-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:31:02 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:31:02 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:31:02 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:31:02 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:31:02 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:31:02 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:31:02 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:31:03	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:31:04	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-31-00_414_671380806946562416-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-11-27 02:31:04	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-31-00_414_671380806946562416-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-11-27 02:31:04	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-31-00_414_671380806946562416-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile21--.hashtable
2013-11-27 02:31:04	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-31-00_414_671380806946562416-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile21--.hashtable
2013-11-27 02:31:04	End of local task; Time Taken: 0.997 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0081, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0081/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0081
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-27 02:31:22,157 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.5 sec
MapReduce Total cumulative CPU time: 2 seconds 500 msec
Ended Job = job_1385527530902_0081
Loading data to table default.q20_potential_part_promotion
Table default.q20_potential_part_promotion stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 8381, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.5 sec   HDFS Read: 1409388 HDFS Write: 8381 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 500 msec
OK
Time taken: 22.174 seconds
Time:127.73
Running Hive query: tpch/q21_suppliers_who_kept_orders_waiting.hive
13/11/27 02:31:23 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:31:23 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:31:23 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:31:23 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:31:23 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:31:23 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:31:23 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 5.978 seconds
OK
Time taken: 0.104 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.098 seconds
OK
Time taken: 0.195 seconds
OK
Time taken: 0.089 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.215 seconds
OK
Time taken: 0.037 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.042 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0082, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0082/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0082
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-27 02:32:14,358 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 56.61 sec
MapReduce Total cumulative CPU time: 56 seconds 610 msec
Ended Job = job_1385527530902_0082
Loading data to table default.q21_tmp1
Table default.q21_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 22196331, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 56.61 sec   HDFS Read: 759884717 HDFS Write: 22196331 SUCCESS
Total MapReduce CPU Time Spent: 56 seconds 610 msec
OK
Time taken: 42.943 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0083, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0083/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0083
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-27 02:32:53,091 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 52.43 sec
MapReduce Total cumulative CPU time: 52 seconds 430 msec
Ended Job = job_1385527530902_0083
Loading data to table default.q21_tmp2
Table default.q21_tmp2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 20334949, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 52.43 sec   HDFS Read: 759884717 HDFS Write: 20334949 SUCCESS
Total MapReduce CPU Time Spent: 52 seconds 430 msec
OK
Time taken: 38.731 seconds
Total MapReduce jobs = 14
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:32:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:32:59 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-32-53_597_3798144450405692929-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:32:59 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-32-53_597_3798144450405692929-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:32:59 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:32:59 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:32:59 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:32:59 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:32:59 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:32:59 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:32:59 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:33:00	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:33:01	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-32-53_597_3798144450405692929-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-11-27 02:33:01	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-32-53_597_3798144450405692929-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-11-27 02:33:01	End of local task; Time Taken: 0.79 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 14
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0084, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0084/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0084
Hadoop job information for Stage-24: number of mappers: 1; number of reducers: 0
2013-11-27 02:33:12,793 Stage-24 map = 100%,  reduce = 0%, Cumulative CPU 1.47 sec
MapReduce Total cumulative CPU time: 1 seconds 470 msec
Ended Job = job_1385527530902_0084
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0085, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0085/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0085
Hadoop job information for Stage-8: number of mappers: 5; number of reducers: 1
2013-11-27 02:33:42,480 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 39.16 sec
MapReduce Total cumulative CPU time: 39 seconds 160 msec
Ended Job = job_1385527530902_0085
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0086, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0086/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0086
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-11-27 02:34:04,437 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.12 sec
MapReduce Total cumulative CPU time: 17 seconds 120 msec
Ended Job = job_1385527530902_0086
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0087, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0087/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0087
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-27 02:34:30,831 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 15.1 sec
MapReduce Total cumulative CPU time: 15 seconds 100 msec
Ended Job = job_1385527530902_0087
Stage-25 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 5 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0088, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0088/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0088
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
2013-11-27 02:34:57,476 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 15.34 sec
MapReduce Total cumulative CPU time: 15 seconds 340 msec
Ended Job = job_1385527530902_0088
Launching Job 6 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0089, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0089/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0089
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-27 02:35:14,498 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.47 sec
MapReduce Total cumulative CPU time: 1 seconds 470 msec
Ended Job = job_1385527530902_0089
Launching Job 7 out of 14
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0090, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0090/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0090
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-27 02:35:31,801 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 1.95 sec
MapReduce Total cumulative CPU time: 1 seconds 950 msec
Ended Job = job_1385527530902_0090
Loading data to table default.q21_suppliers_who_kept_orders_waiting
Table default.q21_suppliers_who_kept_orders_waiting stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 2200, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.47 sec   HDFS Read: 1409388 HDFS Write: 16258 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 39.16 sec   HDFS Read: 759901342 HDFS Write: 6802110 SUCCESS
Job 2: Map: 3  Reduce: 1   Cumulative CPU: 17.12 sec   HDFS Read: 178759134 HDFS Write: 3292611 SUCCESS
Job 3: Map: 2  Reduce: 1   Cumulative CPU: 15.1 sec   HDFS Read: 25489524 HDFS Write: 3171898 SUCCESS
Job 4: Map: 2  Reduce: 1   Cumulative CPU: 15.34 sec   HDFS Read: 23507429 HDFS Write: 15443 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 1.47 sec   HDFS Read: 15810 HDFS Write: 15443 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.95 sec   HDFS Read: 15810 HDFS Write: 2200 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 31 seconds 610 msec
OK
Time taken: 158.646 seconds
Time:249.65
Running Hive query: tpch/q22_global_sales_opportunity.hive
13/11/27 02:35:33 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:35:33 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:35:33 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:35:33 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:35:33 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:35:33 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:35:33 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.405 seconds
OK
Time taken: 0.112 seconds
OK
Time taken: 0.174 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.218 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.084 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385527530902_0091, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0091/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0091
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2013-11-27 02:35:56,483 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.29 sec
MapReduce Total cumulative CPU time: 3 seconds 290 msec
Ended Job = job_1385527530902_0091
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-11-27_02-35-42_066_820630165239025497-1/-ext-10000
Loading data to table default.q22_customer_tmp
Table default.q22_customer_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 716626, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 3.29 sec   HDFS Read: 24346348 HDFS Write: 716626 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 290 msec
OK
Time taken: 15.05 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0092, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0092/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0092
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-27 02:36:15,933 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.36 sec
MapReduce Total cumulative CPU time: 3 seconds 360 msec
Ended Job = job_1385527530902_0092
Loading data to table default.q22_customer_tmp1
Table default.q22_customer_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 19, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.36 sec   HDFS Read: 716849 HDFS Write: 19 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 360 msec
OK
Time taken: 19.307 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0093, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0093/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0093
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2013-11-27 02:36:41,702 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 14.24 sec
MapReduce Total cumulative CPU time: 14 seconds 240 msec
Ended Job = job_1385527530902_0093
Loading data to table default.q22_orders_tmp
Table default.q22_orders_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 625906, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 14.24 sec   HDFS Read: 171956657 HDFS Write: 625906 SUCCESS
Total MapReduce CPU Time Spent: 14 seconds 240 msec
OK
Time taken: 25.758 seconds
Total MapReduce jobs = 2
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/27 02:36:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/27 02:36:44 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-36-42_184_3179400772805226528-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/27 02:36:44 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-27_02-36-42_184_3179400772805226528-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/27 02:36:44 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/27 02:36:44 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/27 02:36:44 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/27 02:36:44 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/27 02:36:44 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/27 02:36:44 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/27 02:36:44 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-27 02:36:45	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-27 02:36:46	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-36-42_184_3179400772805226528-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile10--.hashtable
2013-11-27 02:36:46	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-36-42_184_3179400772805226528-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile10--.hashtable
2013-11-27 02:36:46	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-27_02-36-42_184_3179400772805226528-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-11-27 02:36:46	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-27_02-36-42_184_3179400772805226528-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-11-27 02:36:46	End of local task; Time Taken: 1.199 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0094, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0094/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0094
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-27 02:37:06,545 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 4.86 sec
MapReduce Total cumulative CPU time: 4 seconds 860 msec
Ended Job = job_1385527530902_0094
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385527530902_0095, Tracking URL = http://10.6.40.110/proxy/application_1385527530902_0095/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385527530902_0095
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-27 02:37:23,890 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.8 sec
MapReduce Total cumulative CPU time: 1 seconds 800 msec
Ended Job = job_1385527530902_0095
Loading data to table default.q22_global_sales_opportunity
Table default.q22_global_sales_opportunity stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 168, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 4.86 sec   HDFS Read: 716849 HDFS Write: 313 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 1.8 sec   HDFS Read: 680 HDFS Write: 168 SUCCESS
Total MapReduce CPU Time Spent: 6 seconds 660 msec
OK
Time taken: 42.157 seconds
Time:112.08
