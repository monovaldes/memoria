Running Hive from /opt/hive-0.12.0
Running Hadoop from /usr/lib/hadoop
Running Hive query: tpch/q1_pricing_summary_report.hive
13/11/29 11:47:51 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 11:47:51 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 11:47:51 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 11:47:51 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 11:47:51 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 11:47:51 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 11:47:51 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.643 seconds
OK
Time taken: 0.213 seconds
OK
Time taken: 0.199 seconds
OK
Time taken: 0.06 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 24
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0301, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0301/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0301
Hadoop job information for Stage-1: number of mappers: 89; number of reducers: 24
2013-11-29 11:50:25,298 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1125.35 sec
MapReduce Total cumulative CPU time: 18 minutes 45 seconds 350 msec
Ended Job = job_1385675857984_0301
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0302, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0302/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0302
Hadoop job information for Stage-2: number of mappers: 8; number of reducers: 1
2013-11-29 11:50:46,392 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.41 sec
MapReduce Total cumulative CPU time: 7 seconds 410 msec
Ended Job = job_1385675857984_0302
Loading data to table default.q1_pricing_summary_report
Table default.q1_pricing_summary_report stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 593, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 89  Reduce: 24   Cumulative CPU: 1125.35 sec   HDFS Read: 23635245029 HDFS Write: 2635 SUCCESS
Job 1: Map: 8  Reduce: 1   Cumulative CPU: 7.41 sec   HDFS Read: 9123 HDFS Write: 593 SUCCESS
Total MapReduce CPU Time Spent: 18 minutes 52 seconds 760 msec
OK
Time taken: 167.332 seconds
Time:176.79
Running Hive query: tpch/q2_minimum_cost_supplier.hive
13/11/29 11:50:48 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 11:50:48 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 11:50:48 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 11:50:48 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 11:50:48 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 11:50:48 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 11:50:48 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.757 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.168 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.226 seconds
OK
Time taken: 0.113 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.21 seconds
OK
Time taken: 0.038 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.036 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 10
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 11:51:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 11:51:03 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_11-50-57_485_1683190286272202185-1/-local-10020/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 11:51:03 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_11-50-57_485_1683190286272202185-1/-local-10020/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 11:51:03 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 11:51:03 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 11:51:03 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 11:51:03 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 11:51:03 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 11:51:03 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 11:51:03 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 11:51:04	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 11:51:05	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_11-50-57_485_1683190286272202185-1/-local-10017/HashTable-Stage-18/MapJoin-mapfile61--.hashtable
2013-11-29 11:51:05	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_11-50-57_485_1683190286272202185-1/-local-10017/HashTable-Stage-18/MapJoin-mapfile61--.hashtable
2013-11-29 11:51:05	End of local task; Time Taken: 0.934 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0303, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0303/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0303
Hadoop job information for Stage-18: number of mappers: 1; number of reducers: 0
2013-11-29 11:51:19,131 Stage-18 map = 100%,  reduce = 0%, Cumulative CPU 1.01 sec
MapReduce Total cumulative CPU time: 1 seconds 10 msec
Ended Job = job_1385675857984_0303
Stage-23 is filtered out by condition resolver.
Stage-24 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0304, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0304/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0304
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2013-11-29 11:51:42,846 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9.46 sec
MapReduce Total cumulative CPU time: 9 seconds 460 msec
Ended Job = job_1385675857984_0304
Stage-21 is filtered out by condition resolver.
Stage-22 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 3 out of 10
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0305, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0305/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0305
Hadoop job information for Stage-2: number of mappers: 16; number of reducers: 4
2013-11-29 11:52:37,827 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 231.05 sec
MapReduce Total cumulative CPU time: 3 minutes 51 seconds 50 msec
Ended Job = job_1385675857984_0305
Stage-19 is filtered out by condition resolver.
Stage-20 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 4 out of 10
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0306, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0306/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0306
Hadoop job information for Stage-3: number of mappers: 8; number of reducers: 2
2013-11-29 11:53:23,618 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 109.44 sec
MapReduce Total cumulative CPU time: 1 minutes 49 seconds 440 msec
Ended Job = job_1385675857984_0306
Loading data to table default.q2_minimum_cost_supplier_tmp1
Table default.q2_minimum_cost_supplier_tmp1 stats: [num_partitions: 0, num_files: 2, num_rows: 0, total_size: 3258368, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.01 sec   HDFS Read: 2424 HDFS Write: 231 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 9.46 sec   HDFS Read: 42773253 HDFS Write: 9846347 SUCCESS
Job 2: Map: 16  Reduce: 4   Cumulative CPU: 231.05 sec   HDFS Read: 3651124245 HDFS Write: 827556525 SUCCESS
Job 3: Map: 8  Reduce: 2   Cumulative CPU: 109.44 sec   HDFS Read: 1559795188 HDFS Write: 3258368 SUCCESS
Total MapReduce CPU Time Spent: 5 minutes 50 seconds 960 msec
OK
Time taken: 146.653 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0307, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0307/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0307
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2013-11-29 11:53:46,360 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.33 sec
MapReduce Total cumulative CPU time: 6 seconds 330 msec
Ended Job = job_1385675857984_0307
Loading data to table default.q2_minimum_cost_supplier_tmp2
Table default.q2_minimum_cost_supplier_tmp2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 206270, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 6.33 sec   HDFS Read: 3258840 HDFS Write: 206270 SUCCESS
Total MapReduce CPU Time Spent: 6 seconds 330 msec
OK
Time taken: 22.724 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 11:53:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 11:53:49 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_11-53-46_862_1665971807378487291-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 11:53:49 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_11-53-46_862_1665971807378487291-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 11:53:49 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 11:53:49 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 11:53:49 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 11:53:49 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 11:53:49 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 11:53:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 11:53:49 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 11:53:50	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 11:53:50	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_11-53-46_862_1665971807378487291-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile71--.hashtable
2013-11-29 11:53:51	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_11-53-46_862_1665971807378487291-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile71--.hashtable
2013-11-29 11:53:51	End of local task; Time Taken: 1.04 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0308, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0308/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0308
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2013-11-29 11:54:15,778 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.0 sec
MapReduce Total cumulative CPU time: 7 seconds 0 msec
Ended Job = job_1385675857984_0308
Loading data to table default.q2_minimum_cost_supplier
Table default.q2_minimum_cost_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16388, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 7.0 sec   HDFS Read: 3258840 HDFS Write: 16388 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 0 msec
OK
Time taken: 29.407 seconds
Time:209.33
Running Hive query: tpch/q3_shipping_priority.hive
13/11/29 11:54:17 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 11:54:17 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 11:54:17 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 11:54:17 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 11:54:17 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 11:54:17 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 11:54:17 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.803 seconds
OK
Time taken: 0.132 seconds
OK
Time taken: 0.128 seconds
OK
Time taken: 0.204 seconds
OK
Time taken: 0.206 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.058 seconds
Total MapReduce jobs = 6
Stage-1 is selected by condition resolver.
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0309, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0309/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0309
Hadoop job information for Stage-1: number of mappers: 24; number of reducers: 6
2013-11-29 11:55:28,252 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 287.53 sec
MapReduce Total cumulative CPU time: 4 minutes 47 seconds 530 msec
Ended Job = job_1385675857984_0309
Stage-14 is filtered out by condition resolver.
Stage-15 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 24
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0310, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0310/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0310
Hadoop job information for Stage-2: number of mappers: 93; number of reducers: 24
2013-11-29 11:58:04,497 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 1307.64 sec
MapReduce Total cumulative CPU time: 21 minutes 47 seconds 640 msec
Ended Job = job_1385675857984_0310
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0311, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0311/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0311
Hadoop job information for Stage-3: number of mappers: 7; number of reducers: 1
2013-11-29 11:58:26,636 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 29.12 sec
MapReduce Total cumulative CPU time: 29 seconds 120 msec
Ended Job = job_1385675857984_0311
Launching Job 4 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0312, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0312/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0312
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-29 11:58:49,600 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 6.88 sec
MapReduce Total cumulative CPU time: 6 seconds 880 msec
Ended Job = job_1385675857984_0312
Loading data to table default.q3_shipping_priority
Table default.q3_shipping_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 393, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 24  Reduce: 6   Cumulative CPU: 287.53 sec   HDFS Read: 6032620514 HDFS Write: 149968977 SUCCESS
Job 1: Map: 93  Reduce: 24   Cumulative CPU: 1307.64 sec   HDFS Read: 23785215918 HDFS Write: 14378815 SUCCESS
Job 2: Map: 7  Reduce: 1   Cumulative CPU: 29.12 sec   HDFS Read: 14385158 HDFS Write: 14376567 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 6.88 sec   HDFS Read: 14376934 HDFS Write: 393 SUCCESS
Total MapReduce CPU Time Spent: 27 minutes 11 seconds 170 msec
OK
Time taken: 263.857 seconds
Time:273.84
Running Hive query: tpch/q4_order_priority.hive
13/11/29 11:58:51 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 11:58:51 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 11:58:51 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 11:58:51 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 11:58:51 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 11:58:51 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 11:58:51 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.779 seconds
OK
Time taken: 0.112 seconds
OK
Time taken: 0.229 seconds
OK
Time taken: 0.137 seconds
OK
Time taken: 0.385 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.047 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 24
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0313, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0313/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0313
Hadoop job information for Stage-1: number of mappers: 89; number of reducers: 24
2013-11-29 12:00:59,538 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 933.96 sec
MapReduce Total cumulative CPU time: 15 minutes 33 seconds 960 msec
Ended Job = job_1385675857984_0313
Loading data to table default.q4_order_priority_tmp
Table default.q4_order_priority_tmp stats: [num_partitions: 0, num_files: 24, num_rows: 0, total_size: 387150873, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 89  Reduce: 24   Cumulative CPU: 933.96 sec   HDFS Read: 23635245029 HDFS Write: 387150873 SUCCESS
Total MapReduce CPU Time Spent: 15 minutes 33 seconds 960 msec
OK
Time taken: 119.95 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0314, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0314/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0314
Hadoop job information for Stage-1: number of mappers: 29; number of reducers: 6
2013-11-29 12:02:18,066 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 383.25 sec
MapReduce Total cumulative CPU time: 6 minutes 23 seconds 250 msec
Ended Job = job_1385675857984_0314
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0315, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0315/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0315
Hadoop job information for Stage-2: number of mappers: 5; number of reducers: 1
2013-11-29 12:02:36,880 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.86 sec
MapReduce Total cumulative CPU time: 3 seconds 860 msec
Ended Job = job_1385675857984_0315
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0316, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0316/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0316
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-29 12:03:26,757 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.05 sec
MapReduce Total cumulative CPU time: 2 seconds 50 msec
Ended Job = job_1385675857984_0316
Loading data to table default.q4_order_priority
Table default.q4_order_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 82, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 29  Reduce: 6   Cumulative CPU: 383.25 sec   HDFS Read: 5682935796 HDFS Write: 1458 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 3.86 sec   HDFS Read: 3509 HDFS Write: 248 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 2.05 sec   HDFS Read: 614 HDFS Write: 82 SUCCESS
Total MapReduce CPU Time Spent: 6 minutes 29 seconds 160 msec
OK
Time taken: 147.061 seconds
Time:277.11
Running Hive query: tpch/q5_local_supplier_volume.hive
13/11/29 12:03:28 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 12:03:28 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 12:03:28 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 12:03:28 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 12:03:28 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 12:03:28 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 12:03:28 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.755 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.126 seconds
OK
Time taken: 0.122 seconds
OK
Time taken: 0.118 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.186 seconds
OK
Time taken: 0.208 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 12:03:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 12:03:45 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_12-03-37_568_9054796268492598894-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 12:03:45 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_12-03-37_568_9054796268492598894-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 12:03:45 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 12:03:45 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 12:03:45 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 12:03:45 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 12:03:45 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 12:03:45 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 12:03:45 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:03:46	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:03:47	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_12-03-37_568_9054796268492598894-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile81--.hashtable
2013-11-29 12:03:47	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_12-03-37_568_9054796268492598894-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile81--.hashtable
2013-11-29 12:03:47	End of local task; Time Taken: 0.937 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0317, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0317/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0317
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-11-29 12:04:01,106 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 1.09 sec
MapReduce Total cumulative CPU time: 1 seconds 90 msec
Ended Job = job_1385675857984_0317
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0318, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0318/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0318
Hadoop job information for Stage-7: number of mappers: 2; number of reducers: 1
2013-11-29 12:04:22,630 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 7.65 sec
MapReduce Total cumulative CPU time: 7 seconds 650 msec
Ended Job = job_1385675857984_0318
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 24
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0319, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0319/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0319
Hadoop job information for Stage-8: number of mappers: 90; number of reducers: 24
2013-11-29 12:07:34,225 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 1867.0 sec
MapReduce Total cumulative CPU time: 31 minutes 7 seconds 0 msec
Ended Job = job_1385675857984_0319
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0320, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0320/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0320
Hadoop job information for Stage-1: number of mappers: 30; number of reducers: 7
2013-11-29 12:09:05,532 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 543.06 sec
MapReduce Total cumulative CPU time: 9 minutes 3 seconds 60 msec
Ended Job = job_1385675857984_0320
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0321, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0321/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0321
Hadoop job information for Stage-2: number of mappers: 9; number of reducers: 1
2013-11-29 12:09:55,557 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 97.75 sec
MapReduce Total cumulative CPU time: 1 minutes 37 seconds 750 msec
Ended Job = job_1385675857984_0321
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0322, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0322/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0322
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-29 12:10:15,724 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.53 sec
MapReduce Total cumulative CPU time: 1 seconds 530 msec
Ended Job = job_1385675857984_0322
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0323, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0323/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0323
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-29 12:10:34,521 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.52 sec
MapReduce Total cumulative CPU time: 1 seconds 520 msec
Ended Job = job_1385675857984_0323
Loading data to table default.q5_local_supplier_volume
Table default.q5_local_supplier_volume stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 141, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.09 sec   HDFS Read: 2424 HDFS Write: 222 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 7.65 sec   HDFS Read: 42773244 HDFS Write: 1761160 SUCCESS
Job 2: Map: 90  Reduce: 24   Cumulative CPU: 1867.0 sec   HDFS Read: 23637006556 HDFS Write: 1680987946 SUCCESS
Job 3: Map: 30  Reduce: 7   Cumulative CPU: 543.06 sec   HDFS Read: 6976776230 HDFS Write: 249858345 SUCCESS
Job 4: Map: 9  Reduce: 1   Cumulative CPU: 97.75 sec   HDFS Read: 986699632 HDFS Write: 257 SUCCESS
Job 5: Map: 1  Reduce: 1   Cumulative CPU: 1.53 sec   HDFS Read: 624 HDFS Write: 257 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.52 sec   HDFS Read: 624 HDFS Write: 141 SUCCESS
Total MapReduce CPU Time Spent: 41 minutes 59 seconds 600 msec
OK
Time taken: 417.533 seconds
Time:427.85
Running Hive query: tpch/q6_forecast_revenue_change.hive
13/11/29 12:10:36 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 12:10:36 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 12:10:36 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 12:10:36 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 12:10:36 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 12:10:36 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 12:10:36 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.634 seconds
OK
Time taken: 0.208 seconds
OK
Time taken: 0.224 seconds
OK
Time taken: 0.047 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0324, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0324/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0324
Hadoop job information for Stage-1: number of mappers: 89; number of reducers: 1
2013-11-29 12:12:02,579 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 610.86 sec
MapReduce Total cumulative CPU time: 10 minutes 10 seconds 860 msec
Ended Job = job_1385675857984_0324
Loading data to table default.q6_forecast_revenue_change
Table default.q6_forecast_revenue_change stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 89  Reduce: 1   Cumulative CPU: 610.86 sec   HDFS Read: 23635245029 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 10 minutes 10 seconds 860 msec
OK
Time taken: 78.623 seconds
Time:88.13
Running Hive query: tpch/q7_volume_shipping.hive
13/11/29 12:12:04 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 12:12:04 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 12:12:04 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 12:12:04 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 12:12:04 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 12:12:04 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 12:12:04 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.792 seconds
OK
Time taken: 0.104 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.186 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.216 seconds
OK
Time taken: 0.068 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 3
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 12:12:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 12:12:16 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_12-12-13_669_7412725451364563311-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 12:12:16 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_12-12-13_669_7412725451364563311-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 12:12:17 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 12:12:17 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 12:12:17 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 12:12:17 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 12:12:17 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 12:12:17 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 12:12:17 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:12:17	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:12:18	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_12-12-13_669_7412725451364563311-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-11-29 12:12:18	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_12-12-13_669_7412725451364563311-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-11-29 12:12:18	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_12-12-13_669_7412725451364563311-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-11-29 12:12:18	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_12-12-13_669_7412725451364563311-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-11-29 12:12:18	End of local task; Time Taken: 0.997 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0325, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0325/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0325
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 0
2013-11-29 12:12:35,000 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.5 sec
MapReduce Total cumulative CPU time: 1 seconds 500 msec
Ended Job = job_1385675857984_0325
Stage-5 is selected by condition resolver.
Stage-4 is filtered out by condition resolver.
Stage-6 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-11-29_12-12-13_669_7412725451364563311-1/-ext-10000
Loading data to table default.q7_volume_shipping_tmp
Table default.q7_volume_shipping_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 38, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.5 sec   HDFS Read: 2424 HDFS Write: 38 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 500 msec
OK
Time taken: 21.877 seconds
Total MapReduce jobs = 9
Stage-6 is selected by condition resolver.
Launching Job 1 out of 9
Number of reduce tasks not specified. Estimated from input data size: 24
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0326, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0326/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0326
Hadoop job information for Stage-6: number of mappers: 109; number of reducers: 24
2013-11-29 12:15:20,540 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 1505.38 sec
MapReduce Total cumulative CPU time: 25 minutes 5 seconds 380 msec
Ended Job = job_1385675857984_0326
Stage-24 is filtered out by condition resolver.
Stage-25 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 9
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0327, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0327/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0327
Hadoop job information for Stage-7: number of mappers: 13; number of reducers: 3
2013-11-29 12:19:19,520 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 630.68 sec
MapReduce Total cumulative CPU time: 10 minutes 30 seconds 680 msec
Ended Job = job_1385675857984_0327
Stage-22 is filtered out by condition resolver.
Stage-23 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 9
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0328, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0328/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0328
Hadoop job information for Stage-1: number of mappers: 13; number of reducers: 3
2013-11-29 12:21:14,192 Stage-1 map = 100%,  reduce = 0%
Ended Job = job_1385675857984_0328 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1385675857984_0328_m_000012 (and more) from job job_1385675857984_0328
Examining task ID: task_1385675857984_0328_m_000002 (and more) from job job_1385675857984_0328
Examining task ID: task_1385675857984_0328_r_000000 (and more) from job job_1385675857984_0328

Task with the most failures(4): 
-----
Task ID:
  task_1385675857984_0328_r_000001

URL:
  http://hadoop11:8088/taskdetails.jsp?jobid=job_1385675857984_0328&tipid=task_1385675857984_0328_r_000001
-----
Diagnostic Messages for this Task:
Exception from container-launch: 
org.apache.hadoop.util.Shell$ExitCodeException: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:464)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:195)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:283)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:79)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)




FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Job 0: Map: 109  Reduce: 24   Cumulative CPU: 1505.38 sec   HDFS Read: 28931026535 HDFS Write: 2859122481 SUCCESS
Job 1: Map: 13  Reduce: 3   Cumulative CPU: 630.68 sec   HDFS Read: 3595968122 HDFS Write: 2694217269 SUCCESS
Job 2: Map: 13  Reduce: 3   FAIL
Total MapReduce CPU Time Spent: 35 minutes 36 seconds 59 msec
Command exited with non-zero status 2
Time:551.12
Running Hive query: tpch/q8_national_market_share.hive
13/11/29 12:21:15 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 12:21:15 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 12:21:15 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 12:21:15 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 12:21:15 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 12:21:15 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 12:21:15 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.819 seconds
OK
Time taken: 0.104 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.205 seconds
OK
Time taken: 0.197 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.058 seconds
Total MapReduce jobs = 18
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 12:21:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 12:21:34 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_12-21-24_904_4580149555807057120-1/-local-10035/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 12:21:34 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_12-21-24_904_4580149555807057120-1/-local-10035/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 12:21:34 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 12:21:34 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 12:21:34 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 12:21:34 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 12:21:34 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 12:21:34 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 12:21:34 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:21:35	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:21:36	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_12-21-24_904_4580149555807057120-1/-local-10032/HashTable-Stage-32/MapJoin-mapfile111--.hashtable
2013-11-29 12:21:36	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_12-21-24_904_4580149555807057120-1/-local-10032/HashTable-Stage-32/MapJoin-mapfile111--.hashtable
2013-11-29 12:21:36	End of local task; Time Taken: 0.966 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 18
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0329, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0329/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0329
Hadoop job information for Stage-32: number of mappers: 1; number of reducers: 0
2013-11-29 12:21:53,972 Stage-32 map = 100%,  reduce = 0%, Cumulative CPU 1.01 sec
MapReduce Total cumulative CPU time: 1 seconds 10 msec
Ended Job = job_1385675857984_0329
Stage-42 is filtered out by condition resolver.
Stage-43 is filtered out by condition resolver.
Stage-9 is selected by condition resolver.
Launching Job 2 out of 18
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0330, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0330/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0330
Hadoop job information for Stage-9: number of mappers: 5; number of reducers: 1
2013-11-29 12:22:31,921 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 39.07 sec
MapReduce Total cumulative CPU time: 39 seconds 70 msec
Ended Job = job_1385675857984_0330
Stage-40 is filtered out by condition resolver.
Stage-41 is filtered out by condition resolver.
Stage-10 is selected by condition resolver.
Launching Job 3 out of 18
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0331, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0331/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0331
Hadoop job information for Stage-10: number of mappers: 21; number of reducers: 6
2013-11-29 12:23:20,791 Stage-10 map = 100%,  reduce = 100%, Cumulative CPU 216.99 sec
MapReduce Total cumulative CPU time: 3 minutes 36 seconds 990 msec
Ended Job = job_1385675857984_0331
Stage-38 is filtered out by condition resolver.
Stage-39 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 18
Number of reduce tasks not specified. Estimated from input data size: 24
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0332, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0332/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0332
Hadoop job information for Stage-1: number of mappers: 94; number of reducers: 24
2013-11-29 12:26:29,938 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1864.76 sec
MapReduce Total cumulative CPU time: 31 minutes 4 seconds 760 msec
Ended Job = job_1385675857984_0332
Stage-36 is filtered out by condition resolver.
Stage-37 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 18
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0333, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0333/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0333
Hadoop job information for Stage-2: number of mappers: 13; number of reducers: 2
2013-11-29 12:27:29,144 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 153.1 sec
MapReduce Total cumulative CPU time: 2 minutes 33 seconds 100 msec
Ended Job = job_1385675857984_0333
Stage-34 is filtered out by condition resolver.
Stage-35 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 6 out of 18
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0334, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0334/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0334
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
2013-11-29 12:28:00,094 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 10.69 sec
MapReduce Total cumulative CPU time: 10 seconds 690 msec
Ended Job = job_1385675857984_0334
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 12:28:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 12:28:02 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_12-21-24_904_4580149555807057120-1/-local-10049/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 12:28:02 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_12-21-24_904_4580149555807057120-1/-local-10049/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 12:28:02 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 12:28:02 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 12:28:02 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 12:28:02 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 12:28:02 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 12:28:02 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 12:28:02 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:28:03	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:28:03	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_12-21-24_904_4580149555807057120-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-11-29 12:28:03	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_12-21-24_904_4580149555807057120-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-11-29 12:28:03	End of local task; Time Taken: 0.649 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 7 out of 18
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0335, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0335/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0335
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-29 12:28:31,258 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 6.26 sec
MapReduce Total cumulative CPU time: 6 seconds 260 msec
Ended Job = job_1385675857984_0335
Launching Job 8 out of 18
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0336, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0336/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0336
Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 1
2013-11-29 12:28:53,278 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 1.48 sec
MapReduce Total cumulative CPU time: 1 seconds 480 msec
Ended Job = job_1385675857984_0336
Loading data to table default.q8_national_market_share
Table default.q8_national_market_share stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 51, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.01 sec   HDFS Read: 2424 HDFS Write: 186 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 39.07 sec   HDFS Read: 736839561 HDFS Write: 19073310 SUCCESS
Job 2: Map: 21  Reduce: 6   Cumulative CPU: 216.99 sec   HDFS Read: 5314855183 HDFS Write: 90707122 SUCCESS
Job 3: Map: 94  Reduce: 24   Cumulative CPU: 1864.76 sec   HDFS Read: 23725954208 HDFS Write: 570897952 SUCCESS
Job 4: Map: 13  Reduce: 2   Cumulative CPU: 153.1 sec   HDFS Read: 1303127730 HDFS Write: 3554068 SUCCESS
Job 5: Map: 2  Reduce: 1   Cumulative CPU: 10.69 sec   HDFS Read: 46327312 HDFS Write: 3347021 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 6.26 sec   HDFS Read: 3347388 HDFS Write: 152 SUCCESS
Job 7: Map: 1  Reduce: 1   Cumulative CPU: 1.48 sec   HDFS Read: 519 HDFS Write: 51 SUCCESS
Total MapReduce CPU Time Spent: 38 minutes 13 seconds 360 msec
OK
Time taken: 448.865 seconds
Time:459.39
Running Hive query: tpch/q9_product_type_profit.hive
13/11/29 12:28:55 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 12:28:55 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 12:28:55 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 12:28:55 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 12:28:55 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 12:28:55 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 12:28:55 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.761 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.11 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.214 seconds
OK
Time taken: 0.228 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.067 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/29 12:29:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/29 12:29:11 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_12-29-04_162_4299833631133605590-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/29 12:29:12 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-29_12-29-04_162_4299833631133605590-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/29 12:29:12 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/29 12:29:12 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/29 12:29:12 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/29 12:29:12 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/29 12:29:12 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/29 12:29:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/29 12:29:12 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-29 12:29:12	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-29 12:29:13	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-29_12-29-04_162_4299833631133605590-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-11-29 12:29:13	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-29_12-29-04_162_4299833631133605590-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-11-29 12:29:13	End of local task; Time Taken: 0.801 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385675857984_0337, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0337/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0337
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-11-29 12:29:28,133 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 3.53 sec
MapReduce Total cumulative CPU time: 3 seconds 530 msec
Ended Job = job_1385675857984_0337
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 24
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385675857984_0338, Tracking URL = http://10.6.40.110/proxy/application_1385675857984_0338/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385675857984_0338
Running Hive query: tpch/q10_returned_item.hive
Running Hive query: tpch/q11_important_stock.hive
Running Hive query: tpch/q12_shipping.hive
Running Hive query: tpch/q13_customer_distribution.hive
Running Hive query: tpch/q14_promotion_effect.hive
Running Hive query: tpch/q15_top_supplier.hive
Running Hive query: tpch/q16_parts_supplier_relationship.hive
Running Hive query: tpch/q17_small_quantity_order_revenue.hive
Running Hive query: tpch/q18_large_volume_customer.hive
Running Hive query: tpch/q19_discounted_revenue.hive
Running Hive query: tpch/q20_potential_part_promotion.hive
Running Hive query: tpch/q21_suppliers_who_kept_orders_waiting.hive
Running Hive query: tpch/q22_global_sales_opportunity.hive
