Running Hive from /opt/hive-0.12.0
Running Hadoop from 
Running Hive query: tpch/q1_pricing_summary_report.hive
13/11/30 12:06:12 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 12:06:12 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 12:06:12 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 12:06:12 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 12:06:12 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 12:06:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 12:06:12 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.929 seconds
OK
Time taken: 0.187 seconds
OK
Time taken: 0.237 seconds
OK
Time taken: 0.063 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0001, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0001/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0001
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-11-30 12:13:08,491 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3540.17 sec
MapReduce Total cumulative CPU time: 59 minutes 0 seconds 170 msec
Ended Job = job_1385815888185_0001
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0002, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0002/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0002
Hadoop job information for Stage-2: number of mappers: 10; number of reducers: 1
2013-11-30 12:13:29,867 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.69 sec
MapReduce Total cumulative CPU time: 8 seconds 690 msec
Ended Job = job_1385815888185_0002
Loading data to table default.q1_pricing_summary_report
Table default.q1_pricing_summary_report stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 591, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 3540.17 sec   HDFS Read: 79582199808 HDFS Write: 8011 SUCCESS
Job 1: Map: 10  Reduce: 1   Cumulative CPU: 8.69 sec   HDFS Read: 27221 HDFS Write: 591 SUCCESS
Total MapReduce CPU Time Spent: 59 minutes 8 seconds 860 msec
OK
Time taken: 429.433 seconds
Time:439.32
Running Hive query: tpch/q2_minimum_cost_supplier.hive
13/11/30 12:13:31 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 12:13:31 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 12:13:31 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 12:13:31 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 12:13:31 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 12:13:31 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 12:13:31 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.714 seconds
OK
Time taken: 0.13 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.149 seconds
OK
Time taken: 0.142 seconds
OK
Time taken: 0.217 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.136 seconds
OK
Time taken: 0.239 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.059 seconds
Total MapReduce jobs = 10
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 12:13:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 12:13:47 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_12-13-41_090_3160616828303056973-1/-local-10020/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 12:13:47 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_12-13-41_090_3160616828303056973-1/-local-10020/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 12:13:47 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 12:13:47 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 12:13:47 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 12:13:47 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 12:13:47 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 12:13:47 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 12:13:47 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 12:13:48	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 12:13:49	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_12-13-41_090_3160616828303056973-1/-local-10017/HashTable-Stage-18/MapJoin-mapfile61--.hashtable
2013-11-30 12:13:49	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_12-13-41_090_3160616828303056973-1/-local-10017/HashTable-Stage-18/MapJoin-mapfile61--.hashtable
2013-11-30 12:13:49	End of local task; Time Taken: 0.998 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0003, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0003/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0003
Hadoop job information for Stage-18: number of mappers: 1; number of reducers: 0
2013-11-30 12:14:06,220 Stage-18 map = 100%,  reduce = 0%, Cumulative CPU 1.02 sec
MapReduce Total cumulative CPU time: 1 seconds 20 msec
Ended Job = job_1385815888185_0003
Stage-23 is filtered out by condition resolver.
Stage-24 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0004, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0004/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0004
Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 1
2013-11-30 12:14:33,611 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 19.57 sec
MapReduce Total cumulative CPU time: 19 seconds 570 msec
Ended Job = job_1385815888185_0004
Stage-21 is filtered out by condition resolver.
Stage-22 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 3 out of 10
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0005, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0005/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0005
Hadoop job information for Stage-2: number of mappers: 47; number of reducers: 13
2013-11-30 12:16:22,008 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 777.55 sec
MapReduce Total cumulative CPU time: 12 minutes 57 seconds 550 msec
Ended Job = job_1385815888185_0005
Stage-19 is filtered out by condition resolver.
Stage-20 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 4 out of 10
Number of reduce tasks not specified. Estimated from input data size: 6
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0006, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0006/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0006
Hadoop job information for Stage-3: number of mappers: 22; number of reducers: 6
2013-11-30 12:17:22,298 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 356.1 sec
MapReduce Total cumulative CPU time: 5 minutes 56 seconds 100 msec
Ended Job = job_1385815888185_0006
Loading data to table default.q2_minimum_cost_supplier_tmp1
Table default.q2_minimum_cost_supplier_tmp1 stats: [num_partitions: 0, num_files: 6, num_rows: 0, total_size: 10939639, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.02 sec   HDFS Read: 2424 HDFS Write: 231 SUCCESS
Job 1: Map: 3  Reduce: 1   Cumulative CPU: 19.57 sec   HDFS Read: 142874905 HDFS Write: 32875244 SUCCESS
Job 2: Map: 47  Reduce: 13   Cumulative CPU: 777.55 sec   HDFS Read: 12242467810 HDFS Write: 2763176376 SUCCESS
Job 3: Map: 22  Reduce: 6   Cumulative CPU: 356.1 sec   HDFS Read: 5216534622 HDFS Write: 10939639 SUCCESS
Total MapReduce CPU Time Spent: 19 minutes 14 seconds 240 msec
OK
Time taken: 221.805 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0007, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0007/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0007
Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 1
2013-11-30 12:17:47,007 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.48 sec
MapReduce Total cumulative CPU time: 12 seconds 480 msec
Ended Job = job_1385815888185_0007
Loading data to table default.q2_minimum_cost_supplier_tmp2
Table default.q2_minimum_cost_supplier_tmp2 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 715857, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 12.48 sec   HDFS Read: 10940781 HDFS Write: 715857 SUCCESS
Total MapReduce CPU Time Spent: 12 seconds 480 msec
OK
Time taken: 24.573 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 12:17:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 12:17:49 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_12-17-47_469_5113812416971220400-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 12:17:49 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_12-17-47_469_5113812416971220400-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 12:17:49 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 12:17:49 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 12:17:49 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 12:17:49 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 12:17:49 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 12:17:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 12:17:50 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 12:17:50	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 12:17:51	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_12-17-47_469_5113812416971220400-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile71--.hashtable
2013-11-30 12:17:51	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_12-17-47_469_5113812416971220400-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile71--.hashtable
2013-11-30 12:17:51	End of local task; Time Taken: 1.176 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0008, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0008/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0008
Hadoop job information for Stage-2: number of mappers: 4; number of reducers: 1
2013-11-30 12:18:16,677 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 15.21 sec
MapReduce Total cumulative CPU time: 15 seconds 210 msec
Ended Job = job_1385815888185_0008
Loading data to table default.q2_minimum_cost_supplier
Table default.q2_minimum_cost_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16261, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 4  Reduce: 1   Cumulative CPU: 15.21 sec   HDFS Read: 10940781 HDFS Write: 16261 SUCCESS
Total MapReduce CPU Time Spent: 15 seconds 210 msec
OK
Time taken: 29.673 seconds
Time:286.70
Running Hive query: tpch/q3_shipping_priority.hive
13/11/30 12:18:18 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 12:18:18 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 12:18:18 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 12:18:18 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 12:18:18 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 12:18:18 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 12:18:18 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.796 seconds
OK
Time taken: 0.119 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.191 seconds
OK
Time taken: 0.269 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.056 seconds
Total MapReduce jobs = 6
Stage-1 is selected by condition resolver.
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0009, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0009/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0009
Hadoop job information for Stage-1: number of mappers: 78; number of reducers: 20
2013-11-30 12:20:18,680 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 947.72 sec
MapReduce Total cumulative CPU time: 15 minutes 47 seconds 720 msec
Ended Job = job_1385815888185_0009
Stage-14 is filtered out by condition resolver.
Stage-15 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0010, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0010/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0010
Hadoop job information for Stage-2: number of mappers: 302; number of reducers: 79
2013-11-30 12:28:22,552 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4275.49 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 11 minutes 15 seconds 490 msec
Ended Job = job_1385815888185_0010
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0011, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0011/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0011
Hadoop job information for Stage-3: number of mappers: 9; number of reducers: 1
2013-11-30 12:28:49,392 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 43.32 sec
MapReduce Total cumulative CPU time: 43 seconds 320 msec
Ended Job = job_1385815888185_0011
Launching Job 4 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0012, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0012/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0012
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-30 12:29:23,080 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 12.29 sec
MapReduce Total cumulative CPU time: 12 seconds 290 msec
Ended Job = job_1385815888185_0012
Loading data to table default.q3_shipping_priority
Table default.q3_shipping_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 381, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 78  Reduce: 20   Cumulative CPU: 947.72 sec   HDFS Read: 20257241310 HDFS Write: 499838524 SUCCESS
Job 1: Map: 302  Reduce: 79   Cumulative CPU: 4275.49 sec   HDFS Read: 80082043497 HDFS Write: 47949847 SUCCESS
Job 2: Map: 9  Reduce: 1   Cumulative CPU: 43.32 sec   HDFS Read: 47968690 HDFS Write: 47942799 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 12.29 sec   HDFS Read: 47943166 HDFS Write: 381 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 27 minutes 58 seconds 820 msec
OK
Time taken: 656.433 seconds
Time:666.44
Running Hive query: tpch/q4_order_priority.hive
13/11/30 12:29:24 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 12:29:24 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 12:29:24 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 12:29:24 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 12:29:24 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 12:29:24 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 12:29:24 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.87 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.166 seconds
OK
Time taken: 0.12 seconds
OK
Time taken: 0.238 seconds
OK
Time taken: 0.06 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.048 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0013, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0013/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0013
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-11-30 12:35:14,805 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2978.43 sec
MapReduce Total cumulative CPU time: 49 minutes 38 seconds 430 msec
Ended Job = job_1385815888185_0013
Loading data to table default.q4_order_priority_tmp
Table default.q4_order_priority_tmp stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 1350015083, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 2978.43 sec   HDFS Read: 79582199808 HDFS Write: 1350015083 SUCCESS
Total MapReduce CPU Time Spent: 49 minutes 38 seconds 430 msec
OK
Time taken: 341.818 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0014, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0014/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0014
Hadoop job information for Stage-1: number of mappers: 76; number of reducers: 20
2013-11-30 12:37:54,431 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1193.26 sec
MapReduce Total cumulative CPU time: 19 minutes 53 seconds 260 msec
Ended Job = job_1385815888185_0014
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0015, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0015/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0015
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1
2013-11-30 12:38:19,743 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.74 sec
MapReduce Total cumulative CPU time: 4 seconds 740 msec
Ended Job = job_1385815888185_0015
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0016, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0016/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0016
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-30 12:38:38,651 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.77 sec
MapReduce Total cumulative CPU time: 1 seconds 770 msec
Ended Job = job_1385815888185_0016
Loading data to table default.q4_order_priority
Table default.q4_order_priority stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 87, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 76  Reduce: 20   Cumulative CPU: 1193.26 sec   HDFS Read: 19143698127 HDFS Write: 4860 SUCCESS
Job 1: Map: 6  Reduce: 1   Cumulative CPU: 4.74 sec   HDFS Read: 10170 HDFS Write: 248 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.77 sec   HDFS Read: 615 HDFS Write: 87 SUCCESS
Total MapReduce CPU Time Spent: 19 minutes 59 seconds 770 msec
OK
Time taken: 203.63 seconds
Time:555.53
Running Hive query: tpch/q5_local_supplier_volume.hive
13/11/30 12:38:40 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 12:38:40 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 12:38:40 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 12:38:40 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 12:38:40 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 12:38:40 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 12:38:40 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.673 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.119 seconds
OK
Time taken: 0.097 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.18 seconds
OK
Time taken: 0.222 seconds
OK
Time taken: 0.047 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.085 seconds
OK
Time taken: 0.056 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 12:38:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 12:38:57 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_12-38-49_457_968676985159126483-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 12:38:57 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_12-38-49_457_968676985159126483-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 12:38:57 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 12:38:57 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 12:38:57 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 12:38:57 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 12:38:57 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 12:38:57 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 12:38:57 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 12:38:57	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 12:38:58	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_12-38-49_457_968676985159126483-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile81--.hashtable
2013-11-30 12:38:58	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_12-38-49_457_968676985159126483-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile81--.hashtable
2013-11-30 12:38:58	End of local task; Time Taken: 0.958 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0017, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0017/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0017
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 0
2013-11-30 12:39:11,432 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 0.97 sec
MapReduce Total cumulative CPU time: 970 msec
Ended Job = job_1385815888185_0017
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0018, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0018/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0018
Hadoop job information for Stage-7: number of mappers: 3; number of reducers: 1
2013-11-30 12:39:36,004 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 13.92 sec
MapReduce Total cumulative CPU time: 13 seconds 920 msec
Ended Job = job_1385815888185_0018
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0019, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0019/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0019
Hadoop job information for Stage-8: number of mappers: 298; number of reducers: 80
2013-11-30 12:50:22,360 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 6486.19 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 48 minutes 6 seconds 190 msec
Ended Job = job_1385815888185_0019
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 24
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0020, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0020/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0020
Hadoop job information for Stage-1: number of mappers: 87; number of reducers: 24
2013-11-30 12:53:37,297 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1784.07 sec
MapReduce Total cumulative CPU time: 29 minutes 44 seconds 70 msec
Ended Job = job_1385815888185_0020
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 15
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0021, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0021/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0021
Hadoop job information for Stage-2: number of mappers: 18; number of reducers: 4
2013-11-30 12:54:48,649 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 306.74 sec
MapReduce Total cumulative CPU time: 5 minutes 6 seconds 740 msec
Ended Job = job_1385815888185_0021
Launching Job 6 out of 15
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0022, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0022/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0022
Hadoop job information for Stage-3: number of mappers: 4; number of reducers: 1
2013-11-30 12:55:08,538 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 3.1 sec
MapReduce Total cumulative CPU time: 3 seconds 100 msec
Ended Job = job_1385815888185_0022
Launching Job 7 out of 15
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0023, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0023/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0023
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-30 12:55:25,943 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.47 sec
MapReduce Total cumulative CPU time: 1 seconds 470 msec
Ended Job = job_1385815888185_0023
Loading data to table default.q5_local_supplier_volume
Table default.q5_local_supplier_volume stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 136, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.97 sec   HDFS Read: 2424 HDFS Write: 222 SUCCESS
Job 1: Map: 3  Reduce: 1   Cumulative CPU: 13.92 sec   HDFS Read: 142874895 HDFS Write: 5881020 SUCCESS
Job 2: Map: 298  Reduce: 80   Cumulative CPU: 6486.19 sec   HDFS Read: 79588081194 HDFS Write: 5591627749 SUCCESS
Job 3: Map: 87  Reduce: 24   Cumulative CPU: 1784.07 sec   HDFS Read: 23385322860 HDFS Write: 830634698 SUCCESS
Job 4: Map: 18  Reduce: 4   Cumulative CPU: 306.74 sec   HDFS Read: 3294207796 HDFS Write: 1028 SUCCESS
Job 5: Map: 4  Reduce: 1   Cumulative CPU: 3.1 sec   HDFS Read: 2492 HDFS Write: 257 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 1.47 sec   HDFS Read: 623 HDFS Write: 136 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 23 minutes 16 seconds 460 msec
OK
Time taken: 997.075 seconds
Time:1007.40
Running Hive query: tpch/q6_forecast_revenue_change.hive
13/11/30 12:55:27 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 12:55:27 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 12:55:27 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 12:55:27 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 12:55:27 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 12:55:27 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 12:55:27 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.662 seconds
OK
Time taken: 0.198 seconds
OK
Time taken: 0.235 seconds
OK
Time taken: 0.048 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0024, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0024/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0024
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 1
2013-11-30 12:59:23,832 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2022.0 sec
MapReduce Total cumulative CPU time: 33 minutes 42 seconds 0 msec
Ended Job = job_1385815888185_0024
Loading data to table default.q6_forecast_revenue_change
Table default.q6_forecast_revenue_change stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 22, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 1   Cumulative CPU: 2022.0 sec   HDFS Read: 79582199808 HDFS Write: 22 SUCCESS
Total MapReduce CPU Time Spent: 33 minutes 42 seconds 0 msec
OK
Time taken: 228.366 seconds
Time:237.85
Running Hive query: tpch/q7_volume_shipping.hive
13/11/30 12:59:25 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 12:59:25 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 12:59:25 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 12:59:25 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 12:59:25 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 12:59:25 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 12:59:25 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.695 seconds
OK
Time taken: 0.104 seconds
OK
Time taken: 0.135 seconds
OK
Time taken: 0.132 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.182 seconds
OK
Time taken: 0.11 seconds
OK
Time taken: 0.232 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 3
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 12:59:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 12:59:38 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_12-59-34_775_2251709225601441898-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 12:59:38 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_12-59-34_775_2251709225601441898-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 12:59:38 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 12:59:38 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 12:59:38 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 12:59:38 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 12:59:38 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 12:59:38 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 12:59:38 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 12:59:39	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 12:59:39	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_12-59-34_775_2251709225601441898-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-11-30 12:59:39	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_12-59-34_775_2251709225601441898-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile01--.hashtable
2013-11-30 12:59:39	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_12-59-34_775_2251709225601441898-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-11-30 12:59:39	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_12-59-34_775_2251709225601441898-1/-local-10005/HashTable-Stage-2/MapJoin-mapfile10--.hashtable
2013-11-30 12:59:39	End of local task; Time Taken: 0.952 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0025, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0025/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0025
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 0
2013-11-30 12:59:55,503 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.5 sec
MapReduce Total cumulative CPU time: 1 seconds 500 msec
Ended Job = job_1385815888185_0025
Stage-5 is selected by condition resolver.
Stage-4 is filtered out by condition resolver.
Stage-6 is filtered out by condition resolver.
Moving data to: hdfs://10.6.40.110:9000/tmp/hive-hadoop/hive_2013-11-30_12-59-34_775_2251709225601441898-1/-ext-10000
Loading data to table default.q7_volume_shipping_tmp
Table default.q7_volume_shipping_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 38, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.5 sec   HDFS Read: 2424 HDFS Write: 38 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 500 msec
OK
Time taken: 21.266 seconds
Total MapReduce jobs = 9
Stage-6 is selected by condition resolver.
Launching Job 1 out of 9
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0026, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0026/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0026
Hadoop job information for Stage-6: number of mappers: 364; number of reducers: 80
2013-11-30 13:09:28,716 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 4901.73 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 21 minutes 41 seconds 730 msec
Ended Job = job_1385815888185_0026
Stage-24 is filtered out by condition resolver.
Stage-25 is filtered out by condition resolver.
Stage-7 is selected by condition resolver.
Launching Job 2 out of 9
Number of reduce tasks not specified. Estimated from input data size: 10
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0027, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0027/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0027
Hadoop job information for Stage-7: number of mappers: 40; number of reducers: 10
2013-11-30 13:13:53,172 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 2126.9 sec
MapReduce Total cumulative CPU time: 35 minutes 26 seconds 900 msec
Ended Job = job_1385815888185_0027
Stage-22 is filtered out by condition resolver.
Stage-23 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 9
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0028, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0028/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0028
Hadoop job information for Stage-1: number of mappers: 33; number of reducers: 8
2013-11-30 13:20:19,761 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2032.7 sec
MapReduce Total cumulative CPU time: 33 minutes 52 seconds 700 msec
Ended Job = job_1385815888185_0028
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 13:20:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 13:20:22 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_12-59-56_042_2065417207938674762-1/-local-10024/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 13:20:22 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_12-59-56_042_2065417207938674762-1/-local-10024/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 13:20:22 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 13:20:22 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 13:20:22 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 13:20:22 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 13:20:22 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 13:20:22 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 13:20:22 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 01:20:22	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 01:20:23	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_12-59-56_042_2065417207938674762-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-11-30 01:20:23	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_12-59-56_042_2065417207938674762-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2013-11-30 01:20:23	End of local task; Time Taken: 0.635 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 4 out of 9
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0029, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0029/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0029
Hadoop job information for Stage-3: number of mappers: 29; number of reducers: 7
2013-11-30 13:21:32,651 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 437.05 sec
MapReduce Total cumulative CPU time: 7 minutes 17 seconds 50 msec
Ended Job = job_1385815888185_0029
Launching Job 5 out of 9
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0030, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0030/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0030
Hadoop job information for Stage-4: number of mappers: 4; number of reducers: 1
2013-11-30 13:21:53,901 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.65 sec
MapReduce Total cumulative CPU time: 3 seconds 650 msec
Ended Job = job_1385815888185_0030
Loading data to table default.q7_volume_shipping
Table default.q7_volume_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 160, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 364  Reduce: 80   Cumulative CPU: 4901.73 sec   HDFS Read: 97375874339 HDFS Write: 9560339811 SUCCESS
Job 1: Map: 40  Reduce: 10   Cumulative CPU: 2126.9 sec   HDFS Read: 12024017589 HDFS Write: 9009669093 SUCCESS
Job 2: Map: 33  Reduce: 8   Cumulative CPU: 2032.7 sec   HDFS Read: 9152698744 HDFS Write: 8468719588 SUCCESS
Job 3: Map: 29  Reduce: 7   Cumulative CPU: 437.05 sec   HDFS Read: 8468877441 HDFS Write: 844 SUCCESS
Job 4: Map: 4  Reduce: 1   Cumulative CPU: 3.65 sec   HDFS Read: 2978 HDFS Write: 160 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 38 minutes 22 seconds 30 msec
OK
Time taken: 1318.336 seconds
Time:1349.97
Running Hive query: tpch/q8_national_market_share.hive
13/11/30 13:21:55 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 13:21:55 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 13:21:55 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 13:21:55 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 13:21:55 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 13:21:55 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 13:21:55 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.751 seconds
OK
Time taken: 0.104 seconds
OK
Time taken: 0.135 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.107 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.118 seconds
OK
Time taken: 0.178 seconds
OK
Time taken: 0.23 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.072 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.056 seconds
OK
Time taken: 0.084 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.048 seconds
Total MapReduce jobs = 18
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 13:22:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 13:22:14 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_13-22-05_008_3712090069085467773-1/-local-10035/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 13:22:14 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_13-22-05_008_3712090069085467773-1/-local-10035/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 13:22:14 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 13:22:14 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 13:22:14 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 13:22:14 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 13:22:14 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 13:22:14 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 13:22:14 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 01:22:15	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 01:22:16	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_13-22-05_008_3712090069085467773-1/-local-10032/HashTable-Stage-32/MapJoin-mapfile111--.hashtable
2013-11-30 01:22:16	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_13-22-05_008_3712090069085467773-1/-local-10032/HashTable-Stage-32/MapJoin-mapfile111--.hashtable
2013-11-30 01:22:16	End of local task; Time Taken: 0.937 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 18
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0031, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0031/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0031
Hadoop job information for Stage-32: number of mappers: 1; number of reducers: 0
2013-11-30 13:22:29,569 Stage-32 map = 100%,  reduce = 0%, Cumulative CPU 1.03 sec
MapReduce Total cumulative CPU time: 1 seconds 30 msec
Ended Job = job_1385815888185_0031
Stage-42 is filtered out by condition resolver.
Stage-43 is filtered out by condition resolver.
Stage-9 is selected by condition resolver.
Launching Job 2 out of 18
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0032, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0032/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0032
Hadoop job information for Stage-9: number of mappers: 12; number of reducers: 3
2013-11-30 13:23:14,134 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 121.71 sec
MapReduce Total cumulative CPU time: 2 minutes 1 seconds 710 msec
Ended Job = job_1385815888185_0032
Stage-40 is filtered out by condition resolver.
Stage-41 is filtered out by condition resolver.
Stage-10 is selected by condition resolver.
Launching Job 3 out of 18
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0033, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0033/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0033
Hadoop job information for Stage-10: number of mappers: 70; number of reducers: 18
2013-11-30 13:24:51,675 Stage-10 map = 100%,  reduce = 100%, Cumulative CPU 742.08 sec
MapReduce Total cumulative CPU time: 12 minutes 22 seconds 80 msec
Ended Job = job_1385815888185_0033
Stage-38 is filtered out by condition resolver.
Stage-39 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 4 out of 18
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0034, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0034/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0034
Hadoop job information for Stage-1: number of mappers: 305; number of reducers: 80
2013-11-30 13:37:25,453 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6254.71 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 44 minutes 14 seconds 710 msec
Ended Job = job_1385815888185_0034
Stage-36 is filtered out by condition resolver.
Stage-37 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 5 out of 18
Number of reduce tasks not specified. Estimated from input data size: 5
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0035, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0035/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0035
Hadoop job information for Stage-2: number of mappers: 24; number of reducers: 5
2013-11-30 13:38:58,344 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 436.9 sec
MapReduce Total cumulative CPU time: 7 minutes 16 seconds 900 msec
Ended Job = job_1385815888185_0035
Stage-34 is filtered out by condition resolver.
Stage-35 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 6 out of 18
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0036, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0036/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0036
Hadoop job information for Stage-3: number of mappers: 6; number of reducers: 1
2013-11-30 13:39:24,233 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 27.54 sec
MapReduce Total cumulative CPU time: 27 seconds 540 msec
Ended Job = job_1385815888185_0036
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 13:39:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 13:39:26 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_13-22-05_008_3712090069085467773-1/-local-10049/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 13:39:26 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_13-22-05_008_3712090069085467773-1/-local-10049/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 13:39:26 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 13:39:26 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 13:39:26 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 13:39:26 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 13:39:26 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 13:39:26 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 13:39:26 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 01:39:27	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 01:39:28	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_13-22-05_008_3712090069085467773-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-11-30 01:39:28	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_13-22-05_008_3712090069085467773-1/-local-10010/HashTable-Stage-5/MapJoin-mapfile00--.hashtable
2013-11-30 01:39:28	End of local task; Time Taken: 0.686 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 7 out of 18
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0037, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0037/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0037
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-30 13:39:52,009 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 7.53 sec
MapReduce Total cumulative CPU time: 7 seconds 530 msec
Ended Job = job_1385815888185_0037
Launching Job 8 out of 18
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0038, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0038/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0038
Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 1
2013-11-30 13:40:10,607 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 1.51 sec
MapReduce Total cumulative CPU time: 1 seconds 510 msec
Ended Job = job_1385815888185_0038
Loading data to table default.q8_national_market_share
Table default.q8_national_market_share stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 49, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.03 sec   HDFS Read: 2424 HDFS Write: 186 SUCCESS
Job 1: Map: 12  Reduce: 3   Cumulative CPU: 121.71 sec   HDFS Read: 2463567332 HDFS Write: 63652437 SUCCESS
Job 2: Map: 70  Reduce: 18   Cumulative CPU: 742.08 sec   HDFS Read: 17857328069 HDFS Write: 303326332 SUCCESS
Job 3: Map: 305  Reduce: 80   Cumulative CPU: 6254.71 sec   HDFS Read: 79885531296 HDFS Write: 1917158445 SUCCESS
Job 4: Map: 24  Reduce: 5   Cumulative CPU: 436.9 sec   HDFS Read: 4370488604 HDFS Write: 11836897 SUCCESS
Job 5: Map: 6  Reduce: 1   Cumulative CPU: 27.54 sec   HDFS Read: 154712894 HDFS Write: 11111526 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 7.53 sec   HDFS Read: 11111893 HDFS Write: 152 SUCCESS
Job 7: Map: 1  Reduce: 1   Cumulative CPU: 1.51 sec   HDFS Read: 519 HDFS Write: 49 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 6 minutes 33 seconds 10 msec
OK
Time taken: 1086.101 seconds
Time:1096.73
Running Hive query: tpch/q9_product_type_profit.hive
13/11/30 13:40:12 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 13:40:12 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 13:40:12 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 13:40:12 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 13:40:12 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 13:40:12 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 13:40:12 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.779 seconds
OK
Time taken: 0.122 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.099 seconds
OK
Time taken: 0.184 seconds
OK
Time taken: 0.209 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.073 seconds
OK
Time taken: 0.068 seconds
Total MapReduce jobs = 15
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 13:40:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 13:40:29 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_13-40-21_521_8744672854802036863-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 13:40:29 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_13-40-21_521_8744672854802036863-1/-local-10027/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 13:40:29 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 13:40:29 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 13:40:29 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 13:40:29 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 13:40:29 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 13:40:29 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 13:40:29 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 01:40:30	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 01:40:30	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_13-40-21_521_8744672854802036863-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-11-30 01:40:30	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_13-40-21_521_8744672854802036863-1/-local-10024/HashTable-Stage-25/MapJoin-mapfile80--.hashtable
2013-11-30 01:40:30	End of local task; Time Taken: 0.643 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 15
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0039, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0039/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0039
Hadoop job information for Stage-25: number of mappers: 2; number of reducers: 0
2013-11-30 13:40:50,857 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 8.0 sec
MapReduce Total cumulative CPU time: 8 seconds 0 msec
Ended Job = job_1385815888185_0039
Stage-32 is filtered out by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 15
Number of reduce tasks not specified. Estimated from input data size: 78
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0040, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0040/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0040
Hadoop job information for Stage-8: number of mappers: 298; number of reducers: 78
2013-11-30 13:58:16,836 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 8816.4 sec
MapReduce Total cumulative CPU time: 0 days 2 hours 26 minutes 56 seconds 400 msec
Ended Job = job_1385815888185_0040
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 15
Number of reduce tasks not specified. Estimated from input data size: 49
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0041, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0041/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0041
Hadoop job information for Stage-1: number of mappers: 175; number of reducers: 49
2013-11-30 14:30:32,136 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8893.6 sec
MapReduce Total cumulative CPU time: 0 days 2 hours 28 minutes 13 seconds 600 msec
Ended Job = job_1385815888185_0041
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 15
Number of reduce tasks not specified. Estimated from input data size: 42
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0042, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0042/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0042
Hadoop job information for Stage-2: number of mappers: 158; number of reducers: 42
2013-11-30 14:53:33,844 Stage-2 map = 100%,  reduce = 21%
Ended Job = job_1385815888185_0042 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1385815888185_0042_m_000052 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000006 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000009 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000011 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000055 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000078 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000087 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000095 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000125 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000134 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000155 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000063 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000106 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000156 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_r_000020 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_r_000027 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000085 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000058 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000019 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000135 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_m_000075 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_r_000041 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_r_000033 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_r_000020 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_r_000027 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_r_000012 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_r_000023 (and more) from job job_1385815888185_0042
Examining task ID: task_1385815888185_0042_r_000034 (and more) from job job_1385815888185_0042

Task with the most failures(4): 
-----
Task ID:
  task_1385815888185_0042_r_000028

URL:
  http://hadoop11:8088/taskdetails.jsp?jobid=job_1385815888185_0042&tipid=task_1385815888185_0042_r_000028
-----
Diagnostic Messages for this Task:


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Job 0: Map: 2   Cumulative CPU: 8.0 sec   HDFS Read: 142874307 HDFS Write: 29304980 SUCCESS
Job 1: Map: 298  Reduce: 78   Cumulative CPU: 8816.4 sec   HDFS Read: 79611505377 HDFS Write: 37657255257 SUCCESS
Job 2: Map: 175  Reduce: 49   Cumulative CPU: 8893.6 sec   HDFS Read: 49867471744 HDFS Write: 40120742931 SUCCESS
Job 3: Map: 158  Reduce: 42   FAIL
Total MapReduce CPU Time Spent: 0 days 4 hours 55 minutes 17 seconds 999 msec
Command exited with non-zero status 2
Time:4403.16
Running Hive query: tpch/q10_returned_item.hive
13/11/30 14:53:35 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 14:53:35 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 14:53:35 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 14:53:35 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 14:53:35 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 14:53:35 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 14:53:35 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.761 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.125 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.181 seconds
OK
Time taken: 0.22 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.059 seconds
Total MapReduce jobs = 7
Stage-1 is selected by condition resolver.
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 20
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0043, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0043/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0043
Hadoop job information for Stage-1: number of mappers: 78; number of reducers: 20
2013-11-30 14:59:50,262 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 969.42 sec
MapReduce Total cumulative CPU time: 16 minutes 9 seconds 420 msec
Ended Job = job_1385815888185_0043
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 14:59:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 14:59:52 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_14-53-44_346_487492608194192446-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 14:59:52 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_14-53-44_346_487492608194192446-1/-local-10015/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 14:59:52 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 14:59:52 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 14:59:52 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 14:59:52 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 14:59:52 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 14:59:52 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 14:59:52 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 02:59:53	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 02:59:54	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_14-53-44_346_487492608194192446-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-11-30 02:59:54	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_14-53-44_346_487492608194192446-1/-local-10010/HashTable-Stage-13/MapJoin-mapfile21--.hashtable
2013-11-30 02:59:54	End of local task; Time Taken: 0.689 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 7
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0044, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0044/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0044
Hadoop job information for Stage-13: number of mappers: 6; number of reducers: 0
2013-11-30 15:01:46,295 Stage-13 map = 100%,  reduce = 0%, Cumulative CPU 81.42 sec
MapReduce Total cumulative CPU time: 1 minutes 21 seconds 420 msec
Ended Job = job_1385815888185_0044
Stage-17 is filtered out by condition resolver.
Stage-18 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0045, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0045/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0045
Hadoop job information for Stage-3: number of mappers: 302; number of reducers: 79
2013-11-30 15:11:20,650 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 3232.42 sec
MapReduce Total cumulative CPU time: 53 minutes 52 seconds 420 msec
Ended Job = job_1385815888185_0045
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0046, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0046/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0046
Hadoop job information for Stage-4: number of mappers: 6; number of reducers: 1
2013-11-30 15:12:38,767 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 115.43 sec
MapReduce Total cumulative CPU time: 1 minutes 55 seconds 430 msec
Ended Job = job_1385815888185_0046
Launching Job 5 out of 7
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0047, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0047/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0047
Hadoop job information for Stage-5: number of mappers: 4; number of reducers: 1
2013-11-30 15:13:25,299 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 59.56 sec
MapReduce Total cumulative CPU time: 59 seconds 560 msec
Ended Job = job_1385815888185_0047
Loading data to table default.q10_returned_item
Table default.q10_returned_item stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3591, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 78  Reduce: 20   Cumulative CPU: 969.42 sec   HDFS Read: 20257241310 HDFS Write: 980369893 SUCCESS
Job 1: Map: 6   Cumulative CPU: 81.42 sec   HDFS Read: 980375183 HDFS Write: 1021319236 SUCCESS
Job 2: Map: 302  Reduce: 79   Cumulative CPU: 3232.42 sec   HDFS Read: 80603534126 HDFS Write: 889025471 SUCCESS
Job 3: Map: 6  Reduce: 1   Cumulative CPU: 115.43 sec   HDFS Read: 889043800 HDFS Write: 704132397 SUCCESS
Job 4: Map: 4  Reduce: 1   Cumulative CPU: 59.56 sec   HDFS Read: 704143434 HDFS Write: 3591 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 14 minutes 18 seconds 250 msec
OK
Time taken: 1181.465 seconds
Time:1191.52
Running Hive query: tpch/q11_important_stock.hive
13/11/30 15:13:27 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 15:13:27 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 15:13:27 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 15:13:27 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 15:13:27 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 15:13:27 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 15:13:27 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.658 seconds
OK
Time taken: 0.111 seconds
OK
Time taken: 0.125 seconds
OK
Time taken: 0.182 seconds
OK
Time taken: 0.103 seconds
OK
Time taken: 0.126 seconds
OK
Time taken: 0.24 seconds
OK
Time taken: 0.038 seconds
OK
Time taken: 0.036 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.074 seconds
Total MapReduce jobs = 5
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 15:13:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 15:13:40 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_15-13-36_023_6042078091572577274-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 15:13:40 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_15-13-36_023_6042078091572577274-1/-local-10011/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 15:13:40 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 15:13:40 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 15:13:40 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 15:13:40 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 15:13:40 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 15:13:40 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 15:13:40 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 03:13:41	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 03:13:42	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_15-13-36_023_6042078091572577274-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-11-30 03:13:42	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_15-13-36_023_6042078091572577274-1/-local-10008/HashTable-Stage-10/MapJoin-mapfile20--.hashtable
2013-11-30 03:13:42	End of local task; Time Taken: 0.956 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 5
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0048, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0048/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0048
Hadoop job information for Stage-10: number of mappers: 2; number of reducers: 0
2013-11-30 15:14:00,756 Stage-10 map = 100%,  reduce = 0%, Cumulative CPU 6.09 sec
MapReduce Total cumulative CPU time: 6 seconds 90 msec
Ended Job = job_1385815888185_0048
Stage-11 is filtered out by condition resolver.
Stage-12 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 13
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0049, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0049/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0049
Hadoop job information for Stage-2: number of mappers: 47; number of reducers: 13
2013-11-30 15:15:57,413 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 726.63 sec
MapReduce Total cumulative CPU time: 12 minutes 6 seconds 630 msec
Ended Job = job_1385815888185_0049
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0050, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0050/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0050
Hadoop job information for Stage-3: number of mappers: 6; number of reducers: 1
2013-11-30 15:16:31,549 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 43.86 sec
MapReduce Total cumulative CPU time: 43 seconds 860 msec
Ended Job = job_1385815888185_0050
Loading data to table default.q11_part_tmp
Table default.q11_part_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 63174115, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2   Cumulative CPU: 6.09 sec   HDFS Read: 142874307 HDFS Write: 846792 SUCCESS
Job 1: Map: 47  Reduce: 13   Cumulative CPU: 726.63 sec   HDFS Read: 12210439580 HDFS Write: 94289297 SUCCESS
Job 2: Map: 6  Reduce: 1   Cumulative CPU: 43.86 sec   HDFS Read: 94293053 HDFS Write: 63174115 SUCCESS
Total MapReduce CPU Time Spent: 12 minutes 56 seconds 580 msec
OK
Time taken: 176.005 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0051, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0051/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0051
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-30 15:16:55,298 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.97 sec
MapReduce Total cumulative CPU time: 5 seconds 970 msec
Ended Job = job_1385815888185_0051
Loading data to table default.q11_sum_tmp
Table default.q11_sum_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 5.97 sec   HDFS Read: 63174334 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 970 msec
OK
Time taken: 23.783 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 15:16:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 15:16:58 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_15-16-55_812_7649842873649604644-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 15:16:58 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_15-16-55_812_7649842873649604644-1/-local-10006/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 15:16:58 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 15:16:58 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 15:16:58 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 15:16:58 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 15:16:58 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 15:16:58 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 15:16:58 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 03:16:59	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 03:16:59	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_15-16-55_812_7649842873649604644-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-11-30 03:16:59	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_15-16-55_812_7649842873649604644-1/-local-10003/HashTable-Stage-2/MapJoin-mapfile31--.hashtable
2013-11-30 03:16:59	End of local task; Time Taken: 0.632 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0052, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0052/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0052
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2013-11-30 15:17:25,282 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.11 sec
MapReduce Total cumulative CPU time: 8 seconds 110 msec
Ended Job = job_1385815888185_0052
Loading data to table default.q11_important_stock
Table default.q11_important_stock stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 0, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 8.11 sec   HDFS Read: 63174334 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 110 msec
OK
Time taken: 29.965 seconds
Time:239.96
Running Hive query: tpch/q12_shipping.hive
13/11/30 15:17:27 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 15:17:27 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 15:17:27 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 15:17:27 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 15:17:27 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 15:17:27 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 15:17:27 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.762 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.188 seconds
OK
Time taken: 0.239 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.051 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0053, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0053/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0053
Hadoop job information for Stage-1: number of mappers: 364; number of reducers: 80
2013-11-30 15:27:36,222 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3601.74 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 0 minutes 1 seconds 740 msec
Ended Job = job_1385815888185_0053
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0054, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0054/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0054
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1
2013-11-30 15:27:56,040 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.65 sec
MapReduce Total cumulative CPU time: 5 seconds 650 msec
Ended Job = job_1385815888185_0054
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0055, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0055/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0055
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-30 15:28:14,708 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.82 sec
MapReduce Total cumulative CPU time: 1 seconds 820 msec
Ended Job = job_1385815888185_0055
Loading data to table default.q12_shipping
Table default.q12_shipping stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 46, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 364  Reduce: 80   Cumulative CPU: 3601.74 sec   HDFS Read: 97375874339 HDFS Write: 9920 SUCCESS
Job 1: Map: 6  Reduce: 1   Cumulative CPU: 5.65 sec   HDFS Read: 28550 HDFS Write: 156 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.82 sec   HDFS Read: 523 HDFS Write: 46 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 0 minutes 9 seconds 210 msec
OK
Time taken: 639.747 seconds
Time:649.43
Running Hive query: tpch/q13_customer_distribution.hive
13/11/30 15:28:16 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 15:28:16 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 15:28:16 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 15:28:16 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 15:28:16 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 15:28:16 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 15:28:16 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.731 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.188 seconds
OK
Time taken: 0.238 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.059 seconds
Total MapReduce jobs = 4
Stage-1 is selected by condition resolver.
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 21
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0056, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0056/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0056
Hadoop job information for Stage-1: number of mappers: 77; number of reducers: 21
2013-11-30 15:32:42,676 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1756.01 sec
MapReduce Total cumulative CPU time: 29 minutes 16 seconds 10 msec
Ended Job = job_1385815888185_0056
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0057, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0057/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0057
Hadoop job information for Stage-2: number of mappers: 5; number of reducers: 1
2013-11-30 15:33:55,942 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 109.13 sec
MapReduce Total cumulative CPU time: 1 minutes 49 seconds 130 msec
Ended Job = job_1385815888185_0057
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0058, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0058/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0058
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-30 15:34:14,491 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.52 sec
MapReduce Total cumulative CPU time: 1 seconds 520 msec
Ended Job = job_1385815888185_0058
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0059, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0059/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0059
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-30 15:34:37,524 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 2.07 sec
MapReduce Total cumulative CPU time: 2 seconds 70 msec
Ended Job = job_1385815888185_0059
Loading data to table default.q13_customer_distribution
Table default.q13_customer_distribution stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 392, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 21   Cumulative CPU: 1756.01 sec   HDFS Read: 20257241173 HDFS Write: 333230859 SUCCESS
Job 1: Map: 5  Reduce: 1   Cumulative CPU: 109.13 sec   HDFS Read: 333236246 HDFS Write: 1054 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.52 sec   HDFS Read: 1421 HDFS Write: 1054 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 2.07 sec   HDFS Read: 1421 HDFS Write: 392 SUCCESS
Total MapReduce CPU Time Spent: 31 minutes 8 seconds 730 msec
OK
Time taken: 373.185 seconds
Time:382.81
Running Hive query: tpch/q14_promotion_effect.hive
13/11/30 15:34:39 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 15:34:39 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 15:34:39 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 15:34:39 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 15:34:39 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 15:34:39 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 15:34:39 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.679 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.181 seconds
OK
Time taken: 0.245 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.058 seconds
Total MapReduce jobs = 2
Stage-1 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0060, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0060/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0060
Hadoop job information for Stage-1: number of mappers: 307; number of reducers: 79
2013-11-30 15:42:34,413 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2530.74 sec
MapReduce Total cumulative CPU time: 42 minutes 10 seconds 740 msec
Ended Job = job_1385815888185_0060
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0061, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0061/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0061
Hadoop job information for Stage-2: number of mappers: 7; number of reducers: 1
2013-11-30 15:42:53,252 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.71 sec
MapReduce Total cumulative CPU time: 6 seconds 710 msec
Ended Job = job_1385815888185_0061
Loading data to table default.q14_promotion_effect
Table default.q14_promotion_effect stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 19, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 307  Reduce: 79   Cumulative CPU: 2530.74 sec   HDFS Read: 82035510185 HDFS Write: 10191 SUCCESS
Job 1: Map: 7  Reduce: 1   Cumulative CPU: 6.71 sec   HDFS Read: 28744 HDFS Write: 19 SUCCESS
Total MapReduce CPU Time Spent: 42 minutes 17 seconds 450 msec
OK
Time taken: 486.025 seconds
Time:495.74
Running Hive query: tpch/q15_top_supplier.hive
13/11/30 15:42:55 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 15:42:55 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 15:42:55 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 15:42:55 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 15:42:55 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 15:42:55 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 15:42:55 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.869 seconds
OK
Time taken: 0.11 seconds
OK
Time taken: 0.211 seconds
OK
Time taken: 0.096 seconds
OK
Time taken: 0.13 seconds
OK
Time taken: 0.252 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.053 seconds
OK
Time taken: 0.057 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0062, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0062/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0062
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-11-30 15:50:25,396 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2502.35 sec
MapReduce Total cumulative CPU time: 41 minutes 42 seconds 350 msec
Ended Job = job_1385815888185_0062
Loading data to table default.revenue
Table default.revenue stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 22446302, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 2502.35 sec   HDFS Read: 79582199808 HDFS Write: 22446302 SUCCESS
Total MapReduce CPU Time Spent: 41 minutes 42 seconds 350 msec
OK
Time taken: 442.054 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0063, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0063/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0063
Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 1
2013-11-30 15:50:45,467 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 15.05 sec
MapReduce Total cumulative CPU time: 15 seconds 50 msec
Ended Job = job_1385815888185_0063
Loading data to table default.max_revenue
Table default.max_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 13, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 7  Reduce: 1   Cumulative CPU: 15.05 sec   HDFS Read: 22453421 HDFS Write: 13 SUCCESS
Total MapReduce CPU Time Spent: 15 seconds 50 msec
OK
Time taken: 19.895 seconds
Total MapReduce jobs = 3
Stage-12 is selected by condition resolver.
Stage-1 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 15:50:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 15:50:49 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_15-50-45_944_3309312676620215837-1/-local-10009/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 15:50:49 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_15-50-45_944_3309312676620215837-1/-local-10009/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 15:50:49 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 15:50:49 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 15:50:49 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 15:50:49 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 15:50:49 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 15:50:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 15:50:49 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 03:50:50	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 03:50:52	Processing rows:	200000	Hashtable size:	199999	Memory usage:	139595848	percentage:	0.293
2013-11-30 03:50:53	Processing rows:	300000	Hashtable size:	299999	Memory usage:	140034048	percentage:	0.294
2013-11-30 03:50:53	Processing rows:	400000	Hashtable size:	399999	Memory usage:	149176232	percentage:	0.313
2013-11-30 03:50:54	Processing rows:	500000	Hashtable size:	499999	Memory usage:	174376656	percentage:	0.365
2013-11-30 03:50:54	Processing rows:	600000	Hashtable size:	599999	Memory usage:	196419768	percentage:	0.412
2013-11-30 03:50:55	Processing rows:	700000	Hashtable size:	699999	Memory usage:	221428632	percentage:	0.464
2013-11-30 03:50:57	Processing rows:	800000	Hashtable size:	799999	Memory usage:	244749112	percentage:	0.513
2013-11-30 03:50:57	Processing rows:	900000	Hashtable size:	899999	Memory usage:	278065784	percentage:	0.583
2013-11-30 03:50:58	Processing rows:	1000000	Hashtable size:	999999	Memory usage:	252270984	percentage:	0.529
2013-11-30 03:50:58	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_15-50-45_944_3309312676620215837-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile11--.hashtable
2013-11-30 03:50:58	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_15-50-45_944_3309312676620215837-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile11--.hashtable
2013-11-30 03:50:58	End of local task; Time Taken: 8.63 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0064, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0064/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0064
Hadoop job information for Stage-8: number of mappers: 2; number of reducers: 0
2013-11-30 15:51:31,063 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 26.76 sec
MapReduce Total cumulative CPU time: 26 seconds 760 msec
Ended Job = job_1385815888185_0064
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 15:51:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 15:51:33 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_15-50-45_944_3309312676620215837-1/-local-10013/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 15:51:33 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_15-50-45_944_3309312676620215837-1/-local-10013/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 15:51:33 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 15:51:33 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 15:51:33 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 15:51:33 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 15:51:33 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 15:51:33 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 15:51:33 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 03:51:34	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 03:51:34	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_15-50-45_944_3309312676620215837-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-11-30 03:51:34	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_15-50-45_944_3309312676620215837-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2013-11-30 03:51:34	End of local task; Time Taken: 0.618 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0065, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0065/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0065
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
2013-11-30 15:51:57,673 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 7.19 sec
MapReduce Total cumulative CPU time: 7 seconds 190 msec
Ended Job = job_1385815888185_0065
Loading data to table default.q15_top_supplier
Table default.q15_top_supplier stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 77, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2   Cumulative CPU: 26.76 sec   HDFS Read: 142874307 HDFS Write: 90810975 SUCCESS
Job 1: Map: 2  Reduce: 1   Cumulative CPU: 7.19 sec   HDFS Read: 90811709 HDFS Write: 77 SUCCESS
Total MapReduce CPU Time Spent: 33 seconds 950 msec
OK
Time taken: 72.223 seconds
Time:544.38
Running Hive query: tpch/q16_parts_supplier_relationship.hive
13/11/30 15:51:59 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 15:51:59 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 15:51:59 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 15:51:59 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 15:51:59 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 15:51:59 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 15:51:59 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.755 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.194 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.112 seconds
OK
Time taken: 0.221 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.038 seconds
OK
Time taken: 0.06 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.1 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0066, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0066/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0066
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 0
2013-11-30 15:52:29,662 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 10.29 sec
MapReduce Total cumulative CPU time: 10 seconds 290 msec
Ended Job = job_1385815888185_0066
Stage-4 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Stage-5 is filtered out by condition resolver.
Launching Job 3 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0067, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0067/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0067
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
2013-11-30 15:52:42,968 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 2.23 sec
MapReduce Total cumulative CPU time: 2 seconds 230 msec
Ended Job = job_1385815888185_0067
Loading data to table default.supplier_tmp
Table default.supplier_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6885604, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2   Cumulative CPU: 10.29 sec   HDFS Read: 142874307 HDFS Write: 6885604 SUCCESS
Job 1: Map: 1   Cumulative CPU: 2.23 sec   HDFS Read: 6885993 HDFS Write: 6885604 SUCCESS
Total MapReduce CPU Time Spent: 12 seconds 520 msec
OK
Time taken: 35.061 seconds
Total MapReduce jobs = 2
Stage-3 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 15
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0068, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0068/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0068
Hadoop job information for Stage-3: number of mappers: 56; number of reducers: 15
2013-11-30 15:55:25,332 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 986.06 sec
MapReduce Total cumulative CPU time: 16 minutes 26 seconds 60 msec
Ended Job = job_1385815888185_0068
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 15:55:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 15:55:27 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_15-52-43_543_3344627411550027936-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 15:55:27 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_15-52-43_543_3344627411550027936-1/-local-10008/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 15:55:27 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 15:55:27 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 15:55:27 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 15:55:27 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 15:55:27 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 15:55:27 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 15:55:27 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 03:55:28	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 03:55:29	Processing rows:	200000	Hashtable size:	199999	Memory usage:	38514320	percentage:	0.081
2013-11-30 03:55:29	Processing rows:	300000	Hashtable size:	299999	Memory usage:	49513816	percentage:	0.104
2013-11-30 03:55:29	Processing rows:	400000	Hashtable size:	399999	Memory usage:	58416128	percentage:	0.122
2013-11-30 03:55:29	Processing rows:	500000	Hashtable size:	499999	Memory usage:	67318456	percentage:	0.141
2013-11-30 03:55:29	Processing rows:	600000	Hashtable size:	599999	Memory usage:	80415104	percentage:	0.169
2013-11-30 03:55:29	Processing rows:	700000	Hashtable size:	699999	Memory usage:	89317440	percentage:	0.187
2013-11-30 03:55:29	Processing rows:	800000	Hashtable size:	799999	Memory usage:	96439296	percentage:	0.202
2013-11-30 03:55:29	Processing rows:	900000	Hashtable size:	899999	Memory usage:	105341624	percentage:	0.221
2013-11-30 03:55:29	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_15-52-43_543_3344627411550027936-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-30 03:55:31	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_15-52-43_543_3344627411550027936-1/-local-10003/HashTable-Stage-5/MapJoin-mapfile01--.hashtable
2013-11-30 03:55:31	End of local task; Time Taken: 2.905 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0069, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0069/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0069
Hadoop job information for Stage-5: number of mappers: 15; number of reducers: 0
2013-11-30 15:56:55,387 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 501.49 sec
MapReduce Total cumulative CPU time: 8 minutes 21 seconds 490 msec
Ended Job = job_1385815888185_0069
Loading data to table default.q16_tmp
Table default.q16_tmp stats: [num_partitions: 0, num_files: 15, num_rows: 0, total_size: 2989814109, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 56  Reduce: 15   Cumulative CPU: 986.06 sec   HDFS Read: 14662902576 HDFS Write: 3937268449 SUCCESS
Job 1: Map: 15   Cumulative CPU: 501.49 sec   HDFS Read: 3937307061 HDFS Write: 2989814109 SUCCESS
Total MapReduce CPU Time Spent: 24 minutes 47 seconds 550 msec
OK
Time taken: 252.352 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0070, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0070/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0070
Hadoop job information for Stage-1: number of mappers: 12; number of reducers: 3
2013-11-30 15:57:49,519 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 293.48 sec
MapReduce Total cumulative CPU time: 4 minutes 53 seconds 480 msec
Ended Job = job_1385815888185_0070
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0071, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0071/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0071
Hadoop job information for Stage-2: number of mappers: 3; number of reducers: 1
2013-11-30 15:58:09,201 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.22 sec
MapReduce Total cumulative CPU time: 8 seconds 220 msec
Ended Job = job_1385815888185_0071
Loading data to table default.q16_parts_supplier_relationship
Table default.q16_parts_supplier_relationship stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 1039440, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 12  Reduce: 3   Cumulative CPU: 293.48 sec   HDFS Read: 2989879503 HDFS Write: 1450608 SUCCESS
Job 1: Map: 3  Reduce: 1   Cumulative CPU: 8.22 sec   HDFS Read: 1451709 HDFS Write: 1039440 SUCCESS
Total MapReduce CPU Time Spent: 5 minutes 1 seconds 700 msec
OK
Time taken: 73.814 seconds
Time:371.53
Running Hive query: tpch/q17_small_quantity_order_revenue.hive
13/11/30 15:58:11 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 15:58:11 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 15:58:11 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 15:58:11 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 15:58:11 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 15:58:11 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 15:58:11 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.711 seconds
OK
Time taken: 0.118 seconds
OK
Time taken: 0.171 seconds
OK
Time taken: 0.114 seconds
OK
Time taken: 0.233 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0072, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0072/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0072
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-11-30 16:13:10,316 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6600.98 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 50 minutes 0 seconds 980 msec
Ended Job = job_1385815888185_0072
Loading data to table default.lineitem_tmp
Table default.lineitem_tmp stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 494293140, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 6600.98 sec   HDFS Read: 79582199808 HDFS Write: 494293140 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 50 minutes 0 seconds 980 msec
OK
Time taken: 891.392 seconds
Total MapReduce jobs = 5
Stage-1 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 83
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0073, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0073/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0073
Hadoop job information for Stage-1: number of mappers: 307; number of reducers: 83
2013-11-30 16:28:38,340 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6573.28 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 49 minutes 33 seconds 280 msec
Ended Job = job_1385815888185_0073
Stage-13 is filtered out by condition resolver.
Stage-14 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0074, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0074/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0074
Hadoop job information for Stage-2: number of mappers: 14; number of reducers: 1
2013-11-30 16:30:01,191 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 169.17 sec
MapReduce Total cumulative CPU time: 2 minutes 49 seconds 170 msec
Ended Job = job_1385815888185_0074
Launching Job 3 out of 5
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0075, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0075/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0075
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-30 16:30:20,411 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1.92 sec
MapReduce Total cumulative CPU time: 1 seconds 920 msec
Ended Job = job_1385815888185_0075
Loading data to table default.q17_small_quantity_order_revenue
Table default.q17_small_quantity_order_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 20, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 307  Reduce: 83   Cumulative CPU: 6573.28 sec   HDFS Read: 82035510185 HDFS Write: 22558316 SUCCESS
Job 1: Map: 14  Reduce: 1   Cumulative CPU: 169.17 sec   HDFS Read: 516878416 HDFS Write: 121 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.92 sec   HDFS Read: 488 HDFS Write: 20 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 52 minutes 24 seconds 370 msec
OK
Time taken: 1029.914 seconds
Time:1931.15
Running Hive query: tpch/q18_large_volume_customer.hive
13/11/30 16:30:22 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 16:30:22 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 16:30:22 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 16:30:22 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 16:30:22 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 16:30:22 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 16:30:22 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.586 seconds
OK
Time taken: 0.11 seconds
OK
Time taken: 0.109 seconds
OK
Time taken: 0.171 seconds
OK
Time taken: 0.123 seconds
OK
Time taken: 0.231 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 69
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0076, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0076/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0076
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 69
2013-11-30 16:39:13,003 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3622.12 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 0 minutes 22 seconds 120 msec
Ended Job = job_1385815888185_0076
Loading data to table default.q18_tmp
Table default.q18_tmp stats: [num_partitions: 0, num_files: 69, num_rows: 0, total_size: 2291717657, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 69   Cumulative CPU: 3622.12 sec   HDFS Read: 79582199808 HDFS Write: 2291717657 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 0 minutes 22 seconds 120 msec
OK
Time taken: 522.957 seconds
Total MapReduce jobs = 5
Stage-5 is selected by condition resolver.
Launching Job 1 out of 5
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0077, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0077/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0077
Hadoop job information for Stage-5: number of mappers: 77; number of reducers: 18
2013-11-30 16:56:25,324 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 1974.68 sec
MapReduce Total cumulative CPU time: 32 minutes 54 seconds 680 msec
Ended Job = job_1385815888185_0077
Stage-15 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 5
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0078, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0078/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0078
Hadoop job information for Stage-1: number of mappers: 344; number of reducers: 79
2013-11-30 17:18:24,507 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6971.33 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 56 minutes 11 seconds 330 msec
Ended Job = job_1385815888185_0078
Launching Job 3 out of 5
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0079, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0079/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0079
Hadoop job information for Stage-2: number of mappers: 5; number of reducers: 1
2013-11-30 17:18:49,971 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.72 sec
MapReduce Total cumulative CPU time: 8 seconds 720 msec
Ended Job = job_1385815888185_0079
Launching Job 4 out of 5
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0080, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0080/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0080
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-30 17:19:13,940 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.93 sec
MapReduce Total cumulative CPU time: 2 seconds 930 msec
Ended Job = job_1385815888185_0080
Loading data to table default.q18_large_volume_customer
Table default.q18_large_volume_customer stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 6391, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 77  Reduce: 18   Cumulative CPU: 1974.68 sec   HDFS Read: 20257241173 HDFS Write: 9688881598 SUCCESS
Job 1: Map: 344  Reduce: 79   Cumulative CPU: 6971.33 sec   HDFS Read: 91562986098 HDFS Write: 471842 SUCCESS
Job 2: Map: 5  Reduce: 1   Cumulative CPU: 8.72 sec   HDFS Read: 490105 HDFS Write: 465114 SUCCESS
Job 3: Map: 1  Reduce: 1   Cumulative CPU: 2.93 sec   HDFS Read: 465481 HDFS Write: 6391 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 29 minutes 17 seconds 660 msec
OK
Time taken: 2400.794 seconds
Time:2933.56
Running Hive query: tpch/q19_discounted_revenue.hive
13/11/30 17:19:15 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 17:19:15 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 17:19:15 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 17:19:15 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 17:19:15 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 17:19:15 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 17:19:15 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.707 seconds
OK
Time taken: 0.148 seconds
OK
Time taken: 0.185 seconds
OK
Time taken: 0.217 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.047 seconds
Total MapReduce jobs = 2
Stage-1 is selected by condition resolver.
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 79
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0081, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0081/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0081
Hadoop job information for Stage-1: number of mappers: 307; number of reducers: 79
2013-11-30 17:43:27,621 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9169.46 sec
MapReduce Total cumulative CPU time: 0 days 2 hours 32 minutes 49 seconds 460 msec
Ended Job = job_1385815888185_0081
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0082, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0082/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0082
Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1
2013-11-30 17:44:00,394 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.43 sec
MapReduce Total cumulative CPU time: 5 seconds 430 msec
Ended Job = job_1385815888185_0082
Loading data to table default.q19_discounted_revenue
Table default.q19_discounted_revenue stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 21, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 307  Reduce: 79   Cumulative CPU: 9169.46 sec   HDFS Read: 82035510185 HDFS Write: 9559 SUCCESS
Job 1: Map: 6  Reduce: 1   Cumulative CPU: 5.43 sec   HDFS Read: 27967 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 0 days 2 hours 32 minutes 54 seconds 890 msec
OK
Time taken: 1476.815 seconds
Time:1486.47
Running Hive query: tpch/q20_potential_part_promotion.hive
13/11/30 17:44:02 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 17:44:02 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 17:44:02 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 17:44:02 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 17:44:02 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 17:44:02 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 17:44:02 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.823 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.115 seconds
OK
Time taken: 0.108 seconds
OK
Time taken: 0.161 seconds
OK
Time taken: 0.098 seconds
OK
Time taken: 0.092 seconds
OK
Time taken: 0.094 seconds
OK
Time taken: 0.111 seconds
OK
Time taken: 0.212 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.083 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0083, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0083/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0083
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 3
2013-11-30 17:44:44,376 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 71.93 sec
MapReduce Total cumulative CPU time: 1 minutes 11 seconds 930 msec
Ended Job = job_1385815888185_0083
Loading data to table default.q20_tmp1
Table default.q20_tmp1 stats: [num_partitions: 0, num_files: 3, num_rows: 0, total_size: 1834396, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10  Reduce: 3   Cumulative CPU: 71.93 sec   HDFS Read: 2453310377 HDFS Write: 1834396 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 11 seconds 930 msec
OK
Time taken: 33.37 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0084, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0084/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0084
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-11-30 17:54:34,008 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3133.86 sec
MapReduce Total cumulative CPU time: 52 minutes 13 seconds 860 msec
Ended Job = job_1385815888185_0084
Loading data to table default.q20_tmp2
Table default.q20_tmp2 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 1096808654, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 3133.86 sec   HDFS Read: 79582199808 HDFS Write: 1096808654 SUCCESS
Total MapReduce CPU Time Spent: 52 minutes 13 seconds 860 msec
OK
Time taken: 589.586 seconds
Total MapReduce jobs = 4
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 17:54:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 17:54:37 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_17-54-34_524_7746728134263710515-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 17:54:37 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_17-54-34_524_7746728134263710515-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 17:54:37 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 17:54:37 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 17:54:37 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 17:54:37 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 17:54:37 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 17:54:37 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 17:54:37 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 05:54:38	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 05:54:39	Processing rows:	200000	Hashtable size:	199999	Memory usage:	41489680	percentage:	0.087
2013-11-30 05:54:39	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_17-54-34_524_7746728134263710515-1/-local-10007/HashTable-Stage-8/MapJoin-mapfile21--.hashtable
2013-11-30 05:54:39	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_17-54-34_524_7746728134263710515-1/-local-10007/HashTable-Stage-8/MapJoin-mapfile21--.hashtable
2013-11-30 05:54:39	End of local task; Time Taken: 1.308 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 4
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0085, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0085/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0085
Hadoop job information for Stage-8: number of mappers: 46; number of reducers: 0
2013-11-30 17:55:49,058 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 332.08 sec
MapReduce Total cumulative CPU time: 5 minutes 32 seconds 80 msec
Ended Job = job_1385815888185_0085
Stage-9 is filtered out by condition resolver.
Stage-10 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0086, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0086/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0086
Hadoop job information for Stage-1: number of mappers: 12; number of reducers: 2
2013-11-30 17:58:27,505 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 419.15 sec
MapReduce Total cumulative CPU time: 6 minutes 59 seconds 150 msec
Ended Job = job_1385815888185_0086
Loading data to table default.q20_tmp3
Table default.q20_tmp3 stats: [num_partitions: 0, num_files: 2, num_rows: 0, total_size: 9821178, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 46   Cumulative CPU: 332.08 sec   HDFS Read: 12209592199 HDFS Write: 24620673 SUCCESS
Job 1: Map: 12  Reduce: 2   Cumulative CPU: 419.15 sec   HDFS Read: 1121447463 HDFS Write: 9821178 SUCCESS
Total MapReduce CPU Time Spent: 12 minutes 31 seconds 230 msec
OK
Time taken: 233.455 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0087, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0087/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0087
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-30 17:58:58,344 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.66 sec
MapReduce Total cumulative CPU time: 8 seconds 660 msec
Ended Job = job_1385815888185_0087
Loading data to table default.q20_tmp4
Table default.q20_tmp4 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 3082950, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 8.66 sec   HDFS Read: 9821471 HDFS Write: 3082950 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 660 msec
OK
Time taken: 30.788 seconds
Total MapReduce jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 17:59:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 17:59:01 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_17-58-58_769_5021043665200438709-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 17:59:01 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_17-58-58_769_5021043665200438709-1/-local-10007/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 17:59:01 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 17:59:01 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 17:59:01 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 17:59:01 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 17:59:01 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 17:59:01 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 17:59:01 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 05:59:02	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 05:59:03	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_17-58-58_769_5021043665200438709-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile41--.hashtable
2013-11-30 05:59:03	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_17-58-58_769_5021043665200438709-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile41--.hashtable
2013-11-30 05:59:03	Processing rows:	200000	Hashtable size:	199999	Memory usage:	83190536	percentage:	0.174
2013-11-30 05:59:03	Processing rows:	300000	Hashtable size:	299999	Memory usage:	92167736	percentage:	0.193
2013-11-30 05:59:03	Processing rows:	400000	Hashtable size:	399999	Memory usage:	105339264	percentage:	0.221
2013-11-30 05:59:03	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_17-58-58_769_5021043665200438709-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-11-30 05:59:04	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_17-58-58_769_5021043665200438709-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile31--.hashtable
2013-11-30 05:59:04	End of local task; Time Taken: 2.357 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0088, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0088/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0088
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
2013-11-30 17:59:34,884 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 11.94 sec
MapReduce Total cumulative CPU time: 11 seconds 940 msec
Ended Job = job_1385815888185_0088
Loading data to table default.q20_potential_part_promotion
Table default.q20_potential_part_promotion stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 810032, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 11.94 sec   HDFS Read: 142874307 HDFS Write: 810032 SUCCESS
Total MapReduce CPU Time Spent: 11 seconds 940 msec
OK
Time taken: 36.574 seconds
Time:934.43
Running Hive query: tpch/q21_suppliers_who_kept_orders_waiting.hive
13/11/30 17:59:36 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 17:59:36 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 17:59:36 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 17:59:36 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 17:59:36 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 17:59:36 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 17:59:36 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.767 seconds
OK
Time taken: 0.116 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.11 seconds
OK
Time taken: 0.17 seconds
OK
Time taken: 0.097 seconds
OK
Time taken: 0.12 seconds
OK
Time taken: 0.222 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.083 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0089, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0089/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0089
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-11-30 18:17:53,659 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6451.63 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 47 minutes 31 seconds 630 msec
Ended Job = job_1385815888185_0089
Loading data to table default.q21_tmp1
Table default.q21_tmp1 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 2819600047, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 6451.63 sec   HDFS Read: 79582199808 HDFS Write: 2819600047 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 47 minutes 31 seconds 630 msec
OK
Time taken: 1088.574 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0090, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0090/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0090
Hadoop job information for Stage-1: number of mappers: 297; number of reducers: 80
2013-11-30 18:32:12,877 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4814.79 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 20 minutes 14 seconds 790 msec
Ended Job = job_1385815888185_0090
Loading data to table default.q21_tmp2
Table default.q21_tmp2 stats: [num_partitions: 0, num_files: 80, num_rows: 0, total_size: 2583844107, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 297  Reduce: 80   Cumulative CPU: 4814.79 sec   HDFS Read: 79582199808 HDFS Write: 2583844107 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 20 minutes 14 seconds 790 msec
OK
Time taken: 859.054 seconds
Total MapReduce jobs = 14
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 18:32:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 18:32:19 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_18-32-13_374_1420282713678702367-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 18:32:19 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_18-32-13_374_1420282713678702367-1/-local-10025/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 18:32:19 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 18:32:19 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 18:32:19 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 18:32:19 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 18:32:19 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 18:32:19 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 18:32:19 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 06:32:20	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 06:32:21	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_18-32-13_374_1420282713678702367-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-11-30 06:32:21	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_18-32-13_374_1420282713678702367-1/-local-10022/HashTable-Stage-24/MapJoin-mapfile70--.hashtable
2013-11-30 06:32:21	End of local task; Time Taken: 1.014 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 1 out of 14
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0091, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0091/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0091
Hadoop job information for Stage-24: number of mappers: 2; number of reducers: 0
2013-11-30 18:32:39,194 Stage-24 map = 100%,  reduce = 0%, Cumulative CPU 5.78 sec
MapReduce Total cumulative CPU time: 5 seconds 780 msec
Ended Job = job_1385815888185_0091
Stage-30 is filtered out by condition resolver.
Stage-31 is filtered out by condition resolver.
Stage-8 is selected by condition resolver.
Launching Job 2 out of 14
Number of reduce tasks not specified. Estimated from input data size: 80
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0092, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0092/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0092
Hadoop job information for Stage-8: number of mappers: 299; number of reducers: 80
2013-11-30 18:44:32,060 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 3985.78 sec
MapReduce Total cumulative CPU time: 0 days 1 hours 6 minutes 25 seconds 780 msec
Ended Job = job_1385815888185_0092
Stage-28 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-1 is selected by condition resolver.
Launching Job 3 out of 14
Number of reduce tasks not specified. Estimated from input data size: 19
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0093, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0093/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0093
Hadoop job information for Stage-1: number of mappers: 73; number of reducers: 19
2013-11-30 18:47:27,362 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 909.83 sec
MapReduce Total cumulative CPU time: 15 minutes 9 seconds 830 msec
Ended Job = job_1385815888185_0093
Stage-26 is filtered out by condition resolver.
Stage-27 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 4 out of 14
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0094, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0094/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0094
Hadoop job information for Stage-2: number of mappers: 15; number of reducers: 4
2013-11-30 18:50:54,351 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 983.09 sec
MapReduce Total cumulative CPU time: 16 minutes 23 seconds 90 msec
Ended Job = job_1385815888185_0094
Stage-25 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Launching Job 5 out of 14
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0095, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0095/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0095
Hadoop job information for Stage-3: number of mappers: 14; number of reducers: 3
2013-11-30 18:57:52,712 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 964.84 sec
MapReduce Total cumulative CPU time: 16 minutes 4 seconds 840 msec
Ended Job = job_1385815888185_0095
Launching Job 6 out of 14
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0096, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0096/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0096
Hadoop job information for Stage-4: number of mappers: 3; number of reducers: 1
2013-11-30 18:58:22,342 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 12.07 sec
MapReduce Total cumulative CPU time: 12 seconds 70 msec
Ended Job = job_1385815888185_0096
Launching Job 7 out of 14
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0097, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0097/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0097
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2013-11-30 18:58:48,568 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 4.92 sec
MapReduce Total cumulative CPU time: 4 seconds 920 msec
Ended Job = job_1385815888185_0097
Loading data to table default.q21_suppliers_who_kept_orders_waiting
Table default.q21_suppliers_who_kept_orders_waiting stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 2200, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 2   Cumulative CPU: 5.78 sec   HDFS Read: 142874307 HDFS Write: 1611586 SUCCESS
Job 1: Map: 299  Reduce: 80   Cumulative CPU: 3985.78 sec   HDFS Read: 79583812128 HDFS Write: 686901797 SUCCESS
Job 2: Map: 73  Reduce: 19   Cumulative CPU: 909.83 sec   HDFS Read: 18480594958 HDFS Write: 331675044 SUCCESS
Job 3: Map: 15  Reduce: 4   Cumulative CPU: 983.09 sec   HDFS Read: 3151287636 HDFS Write: 319626840 SUCCESS
Job 4: Map: 14  Reduce: 3   Cumulative CPU: 964.84 sec   HDFS Read: 2903480017 HDFS Write: 4313565 SUCCESS
Job 5: Map: 3  Reduce: 1   Cumulative CPU: 12.07 sec   HDFS Read: 4314666 HDFS Write: 1492766 SUCCESS
Job 6: Map: 1  Reduce: 1   Cumulative CPU: 4.92 sec   HDFS Read: 1493133 HDFS Write: 2200 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 54 minutes 26 seconds 310 msec
OK
Time taken: 1595.645 seconds
Time:3553.66
Running Hive query: tpch/q22_global_sales_opportunity.hive
13/11/30 18:58:50 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 18:58:50 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 18:58:50 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 18:58:50 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 18:58:50 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 18:58:50 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 18:58:50 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative

Logging initialized using configuration in jar:file:/opt/hive-0.12.0/lib/hive-common-0.12.0.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
OK
Time taken: 6.827 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.167 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.101 seconds
OK
Time taken: 0.117 seconds
OK
Time taken: 0.219 seconds
OK
Time taken: 0.037 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.077 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0098, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0098/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0098
Hadoop job information for Stage-1: number of mappers: 10; number of reducers: 0
2013-11-30 18:59:34,236 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 90.02 sec
MapReduce Total cumulative CPU time: 1 minutes 30 seconds 20 msec
Ended Job = job_1385815888185_0098
Stage-4 is filtered out by condition resolver.
Stage-3 is selected by condition resolver.
Stage-5 is filtered out by condition resolver.
Launching Job 3 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1385815888185_0099, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0099/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0099
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
2013-11-30 18:59:55,299 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 10.54 sec
MapReduce Total cumulative CPU time: 10 seconds 540 msec
Ended Job = job_1385815888185_0099
Loading data to table default.q22_customer_tmp
Table default.q22_customer_tmp stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 80028920, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 10   Cumulative CPU: 90.02 sec   HDFS Read: 2463566642 HDFS Write: 80028920 SUCCESS
Job 1: Map: 1   Cumulative CPU: 10.54 sec   HDFS Read: 80030317 HDFS Write: 80028920 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 40 seconds 560 msec
OK
Time taken: 56.723 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0100, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0100/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0100
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-11-30 19:00:19,295 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.08 sec
MapReduce Total cumulative CPU time: 7 seconds 80 msec
Ended Job = job_1385815888185_0100
Loading data to table default.q22_customer_tmp1
Table default.q22_customer_tmp1 stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 7.08 sec   HDFS Read: 80029143 HDFS Write: 16 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 80 msec
OK
Time taken: 23.874 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0101, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0101/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0101
Hadoop job information for Stage-1: number of mappers: 67; number of reducers: 18
2013-11-30 19:03:41,371 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1034.21 sec
MapReduce Total cumulative CPU time: 17 minutes 14 seconds 210 msec
Ended Job = job_1385815888185_0101
Loading data to table default.q22_orders_tmp
Table default.q22_orders_tmp stats: [num_partitions: 0, num_files: 18, num_rows: 0, total_size: 82591212, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 67  Reduce: 18   Cumulative CPU: 1034.21 sec   HDFS Read: 17793674531 HDFS Write: 82591212 SUCCESS
Total MapReduce CPU Time Spent: 17 minutes 14 seconds 210 msec
OK
Time taken: 202.078 seconds
Total MapReduce jobs = 3
Stage-1 is selected by condition resolver.
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0102, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0102/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0102
Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 1
2013-11-30 19:05:01,348 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 109.52 sec
MapReduce Total cumulative CPU time: 1 minutes 49 seconds 520 msec
Ended Job = job_1385815888185_0102
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hive-0.12.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/11/30 19:05:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/11/30 19:05:03 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_19-03-41_861_1244788548855427255-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
13/11/30 19:05:03 WARN conf.Configuration: file:/tmp/hadoop/hive_2013-11-30_19-03-41_861_1244788548855427255-1/-local-10010/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
13/11/30 19:05:03 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
13/11/30 19:05:03 INFO Configuration.deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
13/11/30 19:05:03 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
13/11/30 19:05:03 INFO Configuration.deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
13/11/30 19:05:03 INFO Configuration.deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
13/11/30 19:05:03 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/11/30 19:05:03 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
Execution log at: /tmp/hadoop/.log
2013-11-30 07:05:04	Starting to launch local task to process map join;	maximum memory = 477102080
2013-11-30 07:05:05	Dump the side-table into file: file:/tmp/hadoop/hive_2013-11-30_19-03-41_861_1244788548855427255-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-11-30 07:05:05	Upload 1 File to: file:/tmp/hadoop/hive_2013-11-30_19-03-41_861_1244788548855427255-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable
2013-11-30 07:05:05	End of local task; Time Taken: 0.638 sec.
Execution completed successfully
Mapred Local Task Succeeded . Convert the Join into MapJoin
Mapred Local Task Succeeded . Convert the Join into MapJoin
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0103, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0103/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0103
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2013-11-30 19:05:29,342 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 8.01 sec
MapReduce Total cumulative CPU time: 8 seconds 10 msec
Ended Job = job_1385815888185_0103
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_1385815888185_0104, Tracking URL = http://10.6.40.110/proxy/application_1385815888185_0104/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1385815888185_0104
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2013-11-30 19:05:48,215 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.81 sec
MapReduce Total cumulative CPU time: 1 seconds 810 msec
Ended Job = job_1385815888185_0104
Loading data to table default.q22_global_sales_opportunity
Table default.q22_global_sales_opportunity stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 202, raw_data_size: 0]
MapReduce Jobs Launched: 
Job 0: Map: 8  Reduce: 1   Cumulative CPU: 109.52 sec   HDFS Read: 162622826 HDFS Write: 39621076 SUCCESS
Job 1: Map: 1  Reduce: 1   Cumulative CPU: 8.01 sec   HDFS Read: 39621443 HDFS Write: 320 SUCCESS
Job 2: Map: 1  Reduce: 1   Cumulative CPU: 1.81 sec   HDFS Read: 687 HDFS Write: 202 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 59 seconds 340 msec
OK
Time taken: 127.063 seconds
Time:419.89
